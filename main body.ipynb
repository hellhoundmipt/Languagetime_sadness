{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json_lines as jl\n",
    "import nltk\n",
    "import xmltodict\n",
    "from ispras import texterra as isp\n",
    "import pymorphy2 as pm2\n",
    "import gensim\n",
    "import time as tm\n",
    "from lxml import etree\n",
    "import sys\n",
    "\n",
    "isp_api = isp.API('ba74236a7212a71054ae1408b30b1bdef771d35b')\n",
    "#nltk.download()\n",
    "\n",
    "tokenizer = nltk.TweetTokenizer()\n",
    "morph = pm2.MorphAnalyzer()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лемматизация с помощью Texterra\n",
    "\n",
    "Читаю комментарии из файла по очереди, разбиваю каждый на предложения, лемматизирую с помощью Texterra, составляя при этом словарь FullVocab. Записи в Fullvocab хранятся в следующем виде:\n",
    "\n",
    "__item__:\n",
    "- __id__: id нормальной формы\n",
    "- __normal_form__: нормальная форма\n",
    "- __tags__: часть речи\n",
    "- __word_forms__: список встретившихся словоформ\n",
    "- __cnt__: сколько раз встречалось\n",
    "- __origin__: в каком корпусе текстов слово встретилось (t - треннировочный, a - целевой, e - дополнительный [например, вешний словарь])\n",
    "\n",
    "В lemmatized.txt записываются построчно лемматизированные предложения, но вместо слов в нём \"id нормальной формы\", чтобы для word2vec слова \"стать\" (<i>глагол</i>) и \"стать\" (<i>сущ</i>) не были одним и тем же словом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class FullVocab:\n",
    "    \n",
    "    # Инициализация, либо пустой класс, либо из файла vocab.xml\n",
    "    def __init__(self, from_file = False):\n",
    "        self.id_counter = 0\n",
    "        self.vocab = []\n",
    "        if from_file:\n",
    "            f = open('vocab.xml', 'rb')\n",
    "            root = etree.XML(f.read())\n",
    "            for item in root:\n",
    "                self.add_item(item[3].text.split(' '), item[1].text, item[2].text, item[5].text, cnt = item[4].text, \n",
    "                              word_id = item[0].text)\n",
    "            \n",
    "        \n",
    "    # Добавляем новое слово в словарь\n",
    "    def add_item(self, words, normal_form, tags_normal, origin, cnt = 0, word_id = -1):\n",
    "        \n",
    "        # Дабы было можно и str и list в аргументы добавлять\n",
    "        if type(words) is not list:\n",
    "            words = [words]\n",
    "            \n",
    "        # Проверяем, есть ли нормальная форма слова в словаре. Если есть, то обновляем словоформы, если нет, то \n",
    "        # добавляем новую запись в словарь, присвивая уникальный id\n",
    "        for item in self.vocab:\n",
    "            if item.get('normal_form') == normal_form and item.get('tags') == tags_normal:\n",
    "                item['word_forms'].extend(words)\n",
    "                item['word_forms'] = sorted(set(item['word_forms']))\n",
    "                item['cnt'] += 1\n",
    "                item['origin'] = self.handle_origin(origin, item['origin'])\n",
    "                return item['id']\n",
    "                \n",
    "        #Если такой записи ещё не было\n",
    "        new_item = {'normal_form': normal_form, 'tags': tags_normal, 'word_forms': words}\n",
    "        if normal_form not in new_item['word_forms']:\n",
    "            new_item['word_forms'].append(normal_form)\n",
    "        if int(cnt) > 0:\n",
    "            new_item['cnt'] = int(cnt)\n",
    "        else:\n",
    "            new_item['cnt'] = 1\n",
    "            \n",
    "        new_item['origin'] = self.handle_origin(origin)\n",
    "        \n",
    "        return_id = 0\n",
    "        if word_id == -1:         #если мы не уточняем, какой именно id у нового слова (когда просто добавляем новое слово)\n",
    "            new_item['id'] = self.id_counter\n",
    "            return_id = self.id_counter\n",
    "            self.id_counter = self.id_counter + 1\n",
    "        else:                     #если мы указали id для нового слова (когда читаем из файла сформированный словарь)\n",
    "            return_id = int(word_id)\n",
    "            new_item['id'] = return_id\n",
    "            self.id_counter = return_id + 1\n",
    "            \n",
    "        self.vocab.append(new_item)\n",
    "        return return_id\n",
    "         \n",
    "        \n",
    "    def show_vocab(self):\n",
    "        print(self.vocab)\n",
    "        \n",
    "        \n",
    "    # Сохраняем копию данные класса в vocab.xml\n",
    "    def build_xml(self):\n",
    "        f = open('vocab.xml', 'wb')\n",
    "        root = etree.Element('root')\n",
    "        for item in self.vocab:\n",
    "            child = etree.SubElement(root, 'w' + str(item['id']))\n",
    "            etree.SubElement(child, 'id').text = str(item['id'])\n",
    "            etree.SubElement(child, 'normal_form').text = item['normal_form']\n",
    "            etree.SubElement(child, 'tags').text = item['tags']\n",
    "            etree.SubElement(child, 'word_forms').text = ' '.join(item['word_forms'])\n",
    "            etree.SubElement(child, 'cnt').text = str(item['cnt'])\n",
    "            etree.SubElement(child, 'origin').text = item['origin']\n",
    "            \n",
    "        result = etree.tostring(root, encoding='unicode', method='xml', pretty_print=True)\n",
    "        f.write(result.encode('utf8'))\n",
    "        \n",
    "        \n",
    "    # Ищем нормальную форму или запись в словаре по id\n",
    "    # Если return_val = \"word\", то возвращаем нормальную форму, если \"item\", то запись в словаре\n",
    "    def word_by_id(self, word_id, return_val = \"word\"):\n",
    "        item = next((item for item in self.vocab if item[\"id\"] == word_id))\n",
    "        if item is None:\n",
    "            return -1\n",
    "        elif return_val == 'word':\n",
    "            return item['normal_form']\n",
    "        elif return_val == 'item':\n",
    "            return item\n",
    "        \n",
    "        \n",
    "    def id_by_word(self, word):\n",
    "        item = next((item for item in self.vocab if item['normal_form'] == word))\n",
    "        return item['id']\n",
    "    \n",
    "    \n",
    "    def handle_origin(self, new, old = 'xxx'):\n",
    "        if len(new) == 1:\n",
    "            if new == 't':\n",
    "                return 't' + old[1:3]\n",
    "            elif new == 'a':\n",
    "                return old[0] + 'a' + old[2]\n",
    "            elif new == 'e':\n",
    "                return  old[0:2] + 'e'\n",
    "            \n",
    "        elif len(new) == 3:\n",
    "            return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Understandable, have a nice day\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Maxim\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2889: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "lemmatized_txt = open('lemmatized.txt', 'w', encoding='utf8')\n",
    "\n",
    "with open('learn_scrapy\\pda.jl', 'rb') as f:\n",
    "    \n",
    "    vocab = FullVocab()\n",
    "    num_comments = 30242\n",
    "    counter = 0\n",
    "    prev_counter = 0\n",
    "    anchor_counter = 0\n",
    "    checkpoint = tm.clock()\n",
    "    dt = 100\n",
    "    \n",
    "    for item in jl.reader(f):\n",
    "        sents = nltk.sent_tokenize(item['comment'])\n",
    "        for sent in sents:\n",
    "            toks = tokenizer.tokenize(sent)\n",
    "            toks = [t.lower() for t in toks if t.isalnum()]\n",
    "            sent = ' '.join(toks)\n",
    "            \n",
    "            try:\n",
    "                lemma_annotate = isp_api.lemmatizationAnnotate(text = sent)\n",
    "                pos_annotate = isp_api.posTaggingAnnotate(text = sent)\n",
    "                \n",
    "            except KeyboardInterrupt:\n",
    "                print(\"Understandable, have a nice day\")\n",
    "                vocab.build_xml()\n",
    "                lemmatized_txt.close()\n",
    "                sys.exit()\n",
    "                \n",
    "            except:\n",
    "                print(\"Sentence\", end = \" \\\"\")\n",
    "                print(sent, end = \"\\\" \")\n",
    "                print(\"in commentary #\", end = \"\")\n",
    "                print(counter, end = \" \")\n",
    "                print(\"not processed\")\n",
    "                print(\"\\n\")\n",
    "                continue\n",
    "                \n",
    "            if lemma_annotate.get('annotations').get('lemma') is not None:\n",
    "                lemmatized = []\n",
    "                for i in range(len(lemma_annotate.get('annotations').get('lemma'))):\n",
    "                    l = lemma_annotate.get('annotations').get('lemma')[i].get('value')\n",
    "                    p = pos_annotate['annotations']['pos-token'][i]['value'].get('tag')\n",
    "                    word_id = vocab.add_item(toks[i], l, p, 't')\n",
    "                    lemmatized.append(str(word_id))\n",
    "                \n",
    "                lemmatized = ' '.join(lemmatized)\n",
    "                lemmatized_txt.write(lemmatized + '\\n')\n",
    "        \n",
    "        counter = counter + 1\n",
    "        if(tm.clock() - checkpoint > dt):\n",
    "            print(\"Progress: \", end = '')\n",
    "            print(counter / num_comments * 100, end = '')\n",
    "            print(\"%\")\n",
    "            print(\"Comments processed by turn: \", end= '')\n",
    "            print(counter - prev_counter)\n",
    "            print(\"Comments processed total: \", end= '')\n",
    "            print(counter)\n",
    "            print(\"Time left: \", end = '')\n",
    "            speed = (counter - prev_counter) / dt\n",
    "            print(speed * (num_comments - counter))\n",
    "            print(\"Speed: \", end = '')\n",
    "            print(speed, end = ' ')\n",
    "            print(\"comments/sec\")\n",
    "            print(\"\\n\")\n",
    "            prev_counter = counter\n",
    "            if (counter - anchor_counter) / num_comments * 100 > 33:\n",
    "                print(\"Sleeping for 20 minutes\")\n",
    "                tm.sleep(1200)\n",
    "                anchor_counter = counter\n",
    "            checkpoint = tm.clock()\n",
    "            \n",
    "    vocab.build_xml()\n",
    "    print(\"Finished\")\n",
    "    \n",
    "lemmatized_txt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building model using gensim\n",
    "So basically using word2vec model from gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'normal_form': 'если', 'origin': 'txx', 'tags': 'CONJ', 'id': 0, 'word_forms': ['если'], 'cnt': 1}, {'normal_form': 'она', 'origin': 'txx', 'tags': 'S', 'id': 1, 'word_forms': ['ее', 'она'], 'cnt': 2}, {'normal_form': 'только', 'origin': 'txx', 'tags': 'PART', 'id': 2, 'word_forms': ['только'], 'cnt': 1}, {'normal_form': 'на', 'origin': 'txx', 'tags': 'PR', 'id': 3, 'word_forms': ['на'], 'cnt': 5}, {'normal_form': 'алроид', 'origin': 'txx', 'tags': 'S', 'id': 4, 'word_forms': ['алроиды', 'алроид'], 'cnt': 1}, {'normal_form': 'и', 'origin': 'txx', 'tags': 'CONJ', 'id': 5, 'word_forms': ['и'], 'cnt': 7}, {'normal_form': 'айос', 'origin': 'txx', 'tags': 'S', 'id': 6, 'word_forms': ['айос'], 'cnt': 1}, {'normal_form': 'выходить', 'origin': 'txx', 'tags': 'V', 'id': 7, 'word_forms': ['вышла', 'выходить'], 'cnt': 1}, {'normal_form': 'где', 'origin': 'txx', 'tags': 'ADV', 'id': 8, 'word_forms': ['где'], 'cnt': 2}, {'normal_form': 'быть', 'origin': 'txx', 'tags': 'V', 'id': 9, 'word_forms': ['будете', 'была', 'были', 'быть', 'есть'], 'cnt': 5}, {'normal_form': 'рано', 'origin': 'txx', 'tags': 'ADV', 'id': 10, 'word_forms': ['раньше', 'рано'], 'cnt': 1}, {'normal_form': 'в', 'origin': 'txx', 'tags': 'PR', 'id': 11, 'word_forms': ['в'], 'cnt': 5}, {'normal_form': 'помойка', 'origin': 'txx', 'tags': 'S', 'id': 12, 'word_forms': ['помойке', 'помойка'], 'cnt': 1}, {'normal_form': 'не', 'origin': 'txx', 'tags': 'PART', 'id': 13, 'word_forms': ['не'], 'cnt': 7}, {'normal_form': 'знать', 'origin': 'txx', 'tags': 'V', 'id': 14, 'word_forms': ['знать', 'знаю'], 'cnt': 4}, {'normal_form': 'зачем', 'origin': 'txx', 'tags': 'ADV', 'id': 15, 'word_forms': ['зачем'], 'cnt': 1}, {'normal_form': 'от', 'origin': 'txx', 'tags': 'PR', 'id': 16, 'word_forms': ['от'], 'cnt': 1}, {'normal_form': 'туда', 'origin': 'txx', 'tags': 'ADV', 'id': 17, 'word_forms': ['туда'], 'cnt': 1}, {'normal_form': 'доставать', 'origin': 'txx', 'tags': 'V', 'id': 18, 'word_forms': ['достали', 'доставать'], 'cnt': 1}, {'normal_form': 'рескин', 'origin': 'txx', 'tags': 'S', 'id': 19, 'word_forms': ['рескин'], 'cnt': 1}, {'normal_form': 'что', 'origin': 'txx', 'tags': 'CONJ', 'id': 20, 'word_forms': ['что'], 'cnt': 3}, {'normal_form': 'за', 'origin': 'txx', 'tags': 'PR', 'id': 21, 'word_forms': ['за'], 'cnt': 5}, {'normal_form': 'наркоманский', 'origin': 'txx', 'tags': 'A', 'id': 22, 'word_forms': ['наркоманский'], 'cnt': 1}, {'normal_form': 'сультфильм', 'origin': 'txx', 'tags': 'S', 'id': 23, 'word_forms': ['сультфильм'], 'cnt': 1}, {'normal_form': 'потом', 'origin': 'txx', 'tags': 'ADV', 'id': 24, 'word_forms': ['потом'], 'cnt': 1}, {'normal_form': 'еще', 'origin': 'txx', 'tags': 'PART', 'id': 25, 'word_forms': ['еще'], 'cnt': 2}, {'normal_form': 'говорить', 'origin': 'txx', 'tags': 'V', 'id': 26, 'word_forms': ['говорят', 'говорить'], 'cnt': 1}, {'normal_form': 'аним', 'origin': 'txx', 'tags': 'S', 'id': 27, 'word_forms': ['аниме', 'аним'], 'cnt': 1}, {'normal_form': 'плохо', 'origin': 'txx', 'tags': 'ADV', 'id': 28, 'word_forms': ['плохо'], 'cnt': 1}, {'normal_form': 'хоспад', 'origin': 'txx', 'tags': 'S', 'id': 29, 'word_forms': ['хоспад', 'хоспаде'], 'cnt': 2}, {'normal_form': 'опять', 'origin': 'txx', 'tags': 'ADV', 'id': 30, 'word_forms': ['опять'], 'cnt': 1}, {'normal_form': 'старпер', 'origin': 'txx', 'tags': 'S', 'id': 31, 'word_forms': ['старпер', 'старперы'], 'cnt': 2}, {'normal_form': 'повылезать', 'origin': 'txx', 'tags': 'V', 'id': 32, 'word_forms': ['повылезали', 'повылезать'], 'cnt': 1}, {'normal_form': 'сидеть', 'origin': 'txx', 'tags': 'V', 'id': 33, 'word_forms': ['сидели', 'сидеть', 'сидите'], 'cnt': 2}, {'normal_form': 'там', 'origin': 'txx', 'tags': 'ADV', 'id': 34, 'word_forms': ['там'], 'cnt': 2}, {'normal_form': 'свой', 'origin': 'txx', 'tags': 'A', 'id': 35, 'word_forms': ['свои', 'свой'], 'cnt': 2}, {'normal_form': 'нупогодь', 'origin': 'txx', 'tags': 'S', 'id': 36, 'word_forms': ['нупогоди', 'нупогодь'], 'cnt': 1}, {'normal_form': 'смотреть', 'origin': 'txx', 'tags': 'V', 'id': 37, 'word_forms': ['смотрел', 'смотреть', 'смотрите'], 'cnt': 3}, {'normal_form': 'я', 'origin': 'txx', 'tags': 'S', 'id': 38, 'word_forms': ['я'], 'cnt': 1}, {'normal_form': 'мочь', 'origin': 'txx', 'tags': 'V', 'id': 39, 'word_forms': ['могу', 'мочь'], 'cnt': 1}, {'normal_form': 'ошибаться', 'origin': 'txx', 'tags': 'V', 'id': 40, 'word_forms': ['ошибаться'], 'cnt': 1}, {'normal_form': 'но', 'origin': 'txx', 'tags': 'CONJ', 'id': 41, 'word_forms': ['но'], 'cnt': 2}, {'normal_form': 'как', 'origin': 'txx', 'tags': 'ADV', 'id': 42, 'word_forms': ['как'], 'cnt': 1}, {'normal_form': 'раз', 'origin': 'txx', 'tags': 'S', 'id': 43, 'word_forms': ['раз'], 'cnt': 2}, {'normal_form': 'помнить', 'origin': 'txx', 'tags': 'V', 'id': 44, 'word_forms': ['помнят', 'помнить'], 'cnt': 1}, {'normal_form': 'футурама', 'origin': 'txx', 'tags': 'S', 'id': 45, 'word_forms': ['футураму', 'футурама'], 'cnt': 1}, {'normal_form': '10', 'origin': 'txx', 'tags': 'NUM', 'id': 46, 'word_forms': ['10'], 'cnt': 1}, {'normal_form': 'весь', 'origin': 'txx', 'tags': 'A', 'id': 47, 'word_forms': ['все', 'весь'], 'cnt': 1}, {'normal_form': 'сезон', 'origin': 'txx', 'tags': 'S', 'id': 48, 'word_forms': ['сезоны', 'сезон'], 'cnt': 1}, {'normal_form': 'коллекция', 'origin': 'txx', 'tags': 'S', 'id': 49, 'word_forms': ['коллекция'], 'cnt': 1}, {'normal_form': 'блюрей', 'origin': 'txx', 'tags': 'S', 'id': 50, 'word_forms': ['блюрей'], 'cnt': 1}, {'normal_form': 'глупый', 'origin': 'txx', 'tags': 'A', 'id': 51, 'word_forms': ['глупый'], 'cnt': 1}, {'normal_form': 'жутко', 'origin': 'txx', 'tags': 'ADV', 'id': 52, 'word_forms': ['жутко'], 'cnt': 1}, {'normal_form': 'угарный', 'origin': 'txx', 'tags': 'A', 'id': 53, 'word_forms': ['угарный'], 'cnt': 2}, {'normal_form': 'мультик', 'origin': 'txx', 'tags': 'S', 'id': 54, 'word_forms': ['мультик'], 'cnt': 1}, {'normal_form': 'а', 'origin': 'txx', 'tags': 'CONJ', 'id': 55, 'word_forms': ['а'], 'cnt': 4}, {'normal_form': 'игра', 'origin': 'txx', 'tags': 'S', 'id': 56, 'word_forms': ['игра'], 'cnt': 1}, {'normal_form': 'ггод', 'origin': 'txx', 'tags': 'S', 'id': 57, 'word_forms': ['ггг', 'ггод'], 'cnt': 1}, {'normal_form': 'хороший', 'origin': 'txx', 'tags': 'A', 'id': 58, 'word_forms': ['лучше', 'хороший'], 'cnt': 1}, {'normal_form': 'бы', 'origin': 'txx', 'tags': 'PART', 'id': 59, 'word_forms': ['бы'], 'cnt': 3}, {'normal_form': 'платформирать', 'origin': 'txx', 'tags': 'V', 'id': 60, 'word_forms': ['платформер', 'платформирать'], 'cnt': 1}, {'normal_form': 'какой', 'origin': 'txx', 'tags': 'A', 'id': 61, 'word_forms': ['каким', 'каких', 'какой'], 'cnt': 3}, {'normal_form': 'запилять', 'origin': 'txx', 'tags': 'V', 'id': 62, 'word_forms': ['запилили', 'запилять'], 'cnt': 1}, {'normal_form': 'поддерживать', 'origin': 'txx', 'tags': 'V', 'id': 63, 'word_forms': ['поддерживаю', 'поддерживать'], 'cnt': 1}, {'normal_form': 'классный', 'origin': 'txx', 'tags': 'A', 'id': 64, 'word_forms': ['классный'], 'cnt': 1}, {'normal_form': 'мульт', 'origin': 'txx', 'tags': 'S', 'id': 65, 'word_forms': ['мульт'], 'cnt': 1}, {'normal_form': 'с', 'origin': 'txx', 'tags': 'PR', 'id': 66, 'word_forms': ['с'], 'cnt': 1}, {'normal_form': 'шутки', 'origin': 'txx', 'tags': 'S', 'id': 67, 'word_forms': ['шутками', 'шутки'], 'cnt': 1}, {'normal_form': 'который', 'origin': 'txx', 'tags': 'S', 'id': 68, 'word_forms': ['которые', 'который'], 'cnt': 1}, {'normal_form': 'нужный', 'origin': 'txx', 'tags': 'A', 'id': 69, 'word_forms': ['нужно', 'нужный'], 'cnt': 1}, {'normal_form': 'слушать', 'origin': 'txx', 'tags': 'V', 'id': 70, 'word_forms': ['слушать'], 'cnt': 1}, {'normal_form': 'на', 'origin': 'txx', 'tags': 'S', 'id': 71, 'word_forms': ['ну', 'на'], 'cnt': 1}, {'normal_form': 'понимать', 'origin': 'txx', 'tags': 'V', 'id': 72, 'word_forms': ['понимать'], 'cnt': 1}, {'normal_form': 'разуметься', 'origin': 'txx', 'tags': 'V', 'id': 73, 'word_forms': ['разумеется', 'разуметься'], 'cnt': 1}, {'normal_form': 'good', 'origin': 'txx', 'tags': 'JJ', 'id': 74, 'word_forms': ['good'], 'cnt': 1}, {'normal_form': 'news', 'origin': 'txx', 'tags': 'NN', 'id': 75, 'word_forms': ['news'], 'cnt': 1}, {'normal_form': 'everyone', 'origin': 'txx', 'tags': 'NN', 'id': 76, 'word_forms': ['everyone'], 'cnt': 1}, {'normal_form': 'много', 'origin': 'txx', 'tags': 'ADV', 'id': 77, 'word_forms': ['много'], 'cnt': 1}, {'normal_form': 'уважать', 'origin': 'txx', 'tags': 'V', 'id': 78, 'word_forms': ['уважаемый', 'уважать'], 'cnt': 1}, {'normal_form': 'что', 'origin': 'txx', 'tags': 'S', 'id': 79, 'word_forms': ['чем', 'что'], 'cnt': 4}, {'normal_form': 'изначально', 'origin': 'txx', 'tags': 'ADV', 'id': 80, 'word_forms': ['изначально'], 'cnt': 1}, {'normal_form': 'коверкать', 'origin': 'txx', 'tags': 'V', 'id': 81, 'word_forms': ['коверкать'], 'cnt': 1}, {'normal_form': 'слово', 'origin': 'txx', 'tags': 'S', 'id': 82, 'word_forms': ['слово'], 'cnt': 2}, {'normal_form': 'господь', 'origin': 'txx', 'tags': 'S', 'id': 83, 'word_forms': ['господи', 'господь'], 'cnt': 1}, {'normal_form': 'или', 'origin': 'txx', 'tags': 'CONJ', 'id': 84, 'word_forms': ['или'], 'cnt': 2}, {'normal_form': 'вы', 'origin': 'txx', 'tags': 'S', 'id': 85, 'word_forms': ['вы'], 'cnt': 4}, {'normal_form': 'считать', 'origin': 'txx', 'tags': 'V', 'id': 86, 'word_forms': ['считаете', 'считать'], 'cnt': 1}, {'normal_form': 'так', 'origin': 'txx', 'tags': 'ADV', 'id': 87, 'word_forms': ['так'], 'cnt': 1}, {'normal_form': 'умный', 'origin': 'txx', 'tags': 'A', 'id': 88, 'word_forms': ['умнее', 'умный'], 'cnt': 2}, {'normal_form': 'выглядеть', 'origin': 'txx', 'tags': 'V', 'id': 89, 'word_forms': ['выглядеть'], 'cnt': 2}, {'normal_form': 'вообще', 'origin': 'txx', 'tags': 'ADV', 'id': 90, 'word_forms': ['вообще'], 'cnt': 1}, {'normal_form': 'употребль', 'origin': 'txx', 'tags': 'S', 'id': 91, 'word_forms': ['употребля', 'употребль'], 'cnt': 1}, {'normal_form': 'данный', 'origin': 'txx', 'tags': 'A', 'id': 92, 'word_forms': ['данном', 'данный'], 'cnt': 1}, {'normal_form': 'контекст', 'origin': 'txx', 'tags': 'S', 'id': 93, 'word_forms': ['контексте', 'контекст'], 'cnt': 1}, {'normal_form': 'этот', 'origin': 'txx', 'tags': 'A', 'id': 94, 'word_forms': ['этим', 'это', 'этого', 'этот', 'эту'], 'cnt': 4}, {'normal_form': 'хорошо', 'origin': 'txx', 'tags': 'ADV', 'id': 95, 'word_forms': ['лучше', 'хорошо'], 'cnt': 1}, {'normal_form': '4pda', 'origin': 'txx', 'tags': 'S', 'id': 96, 'word_forms': ['4pda'], 'cnt': 1}, {'normal_form': 'нять', 'origin': 'txx', 'tags': 'V', 'id': 97, 'word_forms': ['нить', 'нять'], 'cnt': 1}, {'normal_form': 'познавательный', 'origin': 'txx', 'tags': 'A', 'id': 98, 'word_forms': ['познавательных', 'познавательный'], 'cnt': 1}, {'normal_form': 'сайт', 'origin': 'txx', 'tags': 'S', 'id': 99, 'word_forms': ['сайтах', 'сайт'], 'cnt': 1}, {'normal_form': 'один', 'origin': 'txx', 'tags': 'NUM', 'id': 100, 'word_forms': ['одно', 'один'], 'cnt': 1}, {'normal_form': 'lvl', 'origin': 'txx', 'tags': 'S', 'id': 101, 'word_forms': ['lvl'], 'cnt': 1}, {'normal_form': 'прокачинать', 'origin': 'txx', 'tags': 'V', 'id': 102, 'word_forms': ['прокачали', 'прокачинать'], 'cnt': 1}, {'normal_form': 'стать', 'origin': 'txx', 'tags': 'V', 'id': 103, 'word_forms': ['стали', 'стать'], 'cnt': 1}, {'normal_form': 'более', 'origin': 'txx', 'tags': 'ADV', 'id': 104, 'word_forms': ['более'], 'cnt': 1}, {'normal_form': 'после', 'origin': 'txx', 'tags': 'PR', 'id': 105, 'word_forms': ['после'], 'cnt': 1}, {'normal_form': 'ваш', 'origin': 'txx', 'tags': 'A', 'id': 106, 'word_forms': ['вашего', 'ваш'], 'cnt': 1}, {'normal_form': 'комментария', 'origin': 'txx', 'tags': 'S', 'id': 107, 'word_forms': ['комментария'], 'cnt': 1}, {'normal_form': 'часто', 'origin': 'txx', 'tags': 'ADV', 'id': 108, 'word_forms': ['часто'], 'cnt': 1}, {'normal_form': 'использовать', 'origin': 'txx', 'tags': 'V', 'id': 109, 'word_forms': ['используется', 'использовать'], 'cnt': 1}, {'normal_form': 'масса', 'origin': 'txx', 'tags': 'S', 'id': 110, 'word_forms': ['масс', 'масса'], 'cnt': 1}, {'normal_form': 'культура', 'origin': 'txx', 'tags': 'S', 'id': 111, 'word_forms': ['культуре', 'культура'], 'cnt': 1}, {'normal_form': 'уже', 'origin': 'txx', 'tags': 'PART', 'id': 112, 'word_forms': ['уже'], 'cnt': 1}, {'normal_form': 'устойчивый', 'origin': 'txx', 'tags': 'A', 'id': 113, 'word_forms': ['устойчивое', 'устойчивый'], 'cnt': 1}, {'normal_form': 'выражение', 'origin': 'txx', 'tags': 'S', 'id': 114, 'word_forms': ['выражение'], 'cnt': 1}, {'normal_form': 'он', 'origin': 'txx', 'tags': 'S', 'id': 115, 'word_forms': ['он'], 'cnt': 1}, {'normal_form': 'ничто', 'origin': 'txx', 'tags': 'S', 'id': 116, 'word_forms': ['ничего', 'ничто'], 'cnt': 1}, {'normal_form': 'хотеть', 'origin': 'txx', 'tags': 'V', 'id': 117, 'word_forms': ['хотел', 'хотеть', 'хочу'], 'cnt': 2}, {'normal_form': 'показывать', 'origin': 'txx', 'tags': 'V', 'id': 118, 'word_forms': ['показать', 'показывать'], 'cnt': 1}, {'normal_form': 'просто', 'origin': 'txx', 'tags': 'PART', 'id': 119, 'word_forms': ['просто'], 'cnt': 1}, {'normal_form': 'реакция', 'origin': 'txx', 'tags': 'S', 'id': 120, 'word_forms': ['реакция'], 'cnt': 1}, {'normal_form': 'критика', 'origin': 'txx', 'tags': 'S', 'id': 121, 'word_forms': ['критику', 'критика'], 'cnt': 1}, {'normal_form': 'без', 'origin': 'txx', 'tags': 'PR', 'id': 122, 'word_forms': ['без'], 'cnt': 1}, {'normal_form': 'аргумент', 'origin': 'txx', 'tags': 'S', 'id': 123, 'word_forms': ['аргументов', 'аргумент'], 'cnt': 1}, {'normal_form': 'это', 'origin': 'txx', 'tags': 'S', 'id': 124, 'word_forms': ['это'], 'cnt': 1}, {'normal_form': 'же', 'origin': 'txx', 'tags': 'PART', 'id': 125, 'word_forms': ['ж', 'же'], 'cnt': 1}, {'normal_form': 'старый', 'origin': 'txx', 'tags': 'S', 'id': 126, 'word_forms': ['старым', 'старый'], 'cnt': 1}, {'normal_form': 'надо', 'origin': 'txx', 'tags': 'ADV', 'id': 127, 'word_forms': ['надо'], 'cnt': 1}, {'normal_form': 'чтобы', 'origin': 'txx', 'tags': 'CONJ', 'id': 128, 'word_forms': ['чтоб', 'чтобы'], 'cnt': 1}, {'normal_form': 'мультфильм', 'origin': 'txx', 'tags': 'S', 'id': 129, 'word_forms': ['мультфильм'], 'cnt': 1}, {'normal_form': 'американский', 'origin': 'txx', 'tags': 'A', 'id': 130, 'word_forms': ['американскую', 'американский'], 'cnt': 1}, {'normal_form': 'наркот', 'origin': 'txx', 'tags': 'S', 'id': 131, 'word_forms': ['наркоту', 'наркот'], 'cnt': 1}, {'normal_form': 'говореть', 'origin': 'txx', 'tags': 'V', 'id': 132, 'word_forms': ['говорящие', 'говореть'], 'cnt': 1}, {'normal_form': 'голова', 'origin': 'txx', 'tags': 'S', 'id': 133, 'word_forms': ['головы', 'голова'], 'cnt': 1}, {'normal_form': 'банк', 'origin': 'txx', 'tags': 'S', 'id': 134, 'word_forms': ['банках', 'банк'], 'cnt': 1}, {'normal_form': 'мода', 'origin': 'txx', 'tags': 'S', 'id': 135, 'word_forms': ['моде', 'мода'], 'cnt': 1}]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "you must first build vocabulary before training the model",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-ee0aff00f3c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnew_vocab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFullVocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfrom_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnew_vocab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mw2v_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword2vec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLineSentence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'lemmatized.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mC:\\Users\\Maxim\\Anaconda3\\lib\\site-packages\\gensim-2.2.0-py3.5-win-amd64.egg\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentences, size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, cbow_mean, hashfxn, iter, null_word, trim_rule, sorted_vocab, batch_words)\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m             self.train(sentences, total_examples=self.corpus_count, epochs=self.iter,\n\u001b[0;32m--> 479\u001b[0;31m                        start_alpha=self.alpha, end_alpha=self.min_alpha)\n\u001b[0m\u001b[1;32m    480\u001b[0m         \u001b[1;32melse\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtrim_rule\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Maxim\\Anaconda3\\lib\\site-packages\\gensim-2.2.0-py3.5-win-amd64.egg\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, sentences, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    801\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 802\u001b[0;31m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"you must first build vocabulary before training the model\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    803\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msyn0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    804\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"you must first finalize vocabulary before training the model\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: you must first build vocabulary before training the model"
     ]
    }
   ],
   "source": [
    "new_vocab = FullVocab(from_file = True)\n",
    "new_vocab.show_vocab()\n",
    "w2v_model = gensim.models.Word2Vec(gensim.models.word2vec.LineSentence('lemmatized.txt'), min_count=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some testing\n",
    "Everythin' below is my lazy draft, don't borther yourself lookig through"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "comments = ''\n",
    "with open('learn_scrapy\\pda.jl', 'rb') as f:\n",
    "    for item in jl.reader(f):\n",
    "        comments = comments + item['comment']       #создаём один string на все комменты\n",
    "                                                    #скорее всего, будет логичнее для N-граммов каждое предложение\n",
    "                                                    #запоминать в один string и хранить как list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kek1\n",
      "Kek2\n",
      "Kek3\n",
      "Kek4\n"
     ]
    }
   ],
   "source": [
    "#тут токенизируем комментарии\n",
    "#есть подозрение, что я создаю слишком много сущностей \n",
    "\n",
    "tknzr = nltk.TweetTokenizer()\n",
    "print('Kek1')\n",
    "tokens = tknzr.tokenize(comments)\n",
    "print('Kek2')\n",
    "text = nltk.Text(tokens)\n",
    "print('Kek3')\n",
    "sents = nltk.sent_tokenize(comments)                         #строить n-граммную модель, возможно, буду \n",
    "print('Kek4')                                                             #анализируя предложения по-отдельности\n",
    "words = sorted([w.lower() for w in tokens if w.isalpha()])   #впоследствии буду отсекать наименее наиболее частотные слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10600020178435018\n"
     ]
    }
   ],
   "source": [
    "def lexical_diversity(text):\n",
    "    return len(set(text)) / len(text)\n",
    "\n",
    "print(lexical_diversity(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sent_tokens = list()                            #здесь храним предложения\n",
    "for i in range(len(sents)):\n",
    "    sent_tokens.append(tknzr.tokenize(sents[i]))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#сох нормальные формы слов\n",
    "morph = pm2.MorphAnalyzer()     \n",
    "vocab_normalized = sorted(set([morph.parse(w)[0].normalized for w in words]), key = lambda w: w.normal_form)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def progress(full, curr, checkpoint):\n",
    "    if curr / full * 100 > checkpoint:\n",
    "        print(curr / full * 100, end = '')\n",
    "        print('% complited')\n",
    "        return True\n",
    "    \n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "i = 0\n",
    "h = 0.1\n",
    "checkpoint = h\n",
    "time = tm.clock()\n",
    "for sent in sents:\n",
    "    if progress(len(sents), i, checkpoint):\n",
    "        time_per_checkpoint = tm.clock() - time\n",
    "        print('Time per checkpoint:', end = ' ')\n",
    "        print(time_per_checkpoint)\n",
    "        print('Time per sentence:', end = ' ')\n",
    "        time_per_sent = time_per_checkpoint / (len(sents) * h)\n",
    "        print(time_per_sent)\n",
    "        print('Estimated time:', end = ' ')\n",
    "        print(time_per_checkpoint * (100 - checkpoint))\n",
    "        time = tm.clock()\n",
    "        checkpoint = checkpoint + h\n",
    "        \n",
    "    toks = tknzr.tokenize(sent)\n",
    "    toks = [t.lower() for t in toks if t.isalnum()]\n",
    "    sentence = ''\n",
    "    for t in toks:\n",
    "        sentence = sentence + t + ' '\n",
    "    \n",
    "    lemma_raw = isp_api.lemmatizationAnnotate(text = sentence)\n",
    "    lemmatized = ''\n",
    "    if lemma_raw.get('annotations').get('lemma') is not None:\n",
    "        for l in lemma_raw.get('annotations').get('lemma'):\n",
    "            lemmatized = lemmatized + l.get('value') + ' '\n",
    "        \n",
    "        sents[i] = lemmatized\n",
    "    \n",
    "    else:\n",
    "        sents[i] = ''\n",
    "        \n",
    "    i = i + 1\n",
    "        \n",
    "    \n",
    "sents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sents' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-e8335ea762b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'lemmatized.txt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf8'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msents\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m5000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sents' is not defined"
     ]
    }
   ],
   "source": [
    "with open('lemmatized.txt', 'w', encoding='utf8') as f:\n",
    "    for s in sents[0:5000]:\n",
    "        f.write(s + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec(gensim.models.word2vec.LineSentence('lemmatized.txt'), min_count=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('а', 0.9998720288276672),\n",
       " ('они', 0.9998717904090881),\n",
       " ('где', 0.9998696446418762),\n",
       " ('этот', 0.9998689293861389),\n",
       " ('мочь', 0.9998672604560852),\n",
       " ('когда', 0.999864935874939),\n",
       " ('это', 0.9998648166656494),\n",
       " ('там', 0.9998632669448853),\n",
       " ('с', 0.9998587965965271),\n",
       " ('ничто', 0.9998581409454346)]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive = ['айфон'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "сейчас\n",
      "{'value': {'characters': [], 'tag': 'ADV', 'type': 'syn-tag-rus'}, 'start': 0, 'text': 'сейчас', 'end': 6, 'annotated-text': 'сейчас я говорю о том человеке который спит отдыхая на работе'}\n",
      "[Parse(word='сейчас', tag=OpencorporaTag('ADVB'), normal_form='сейчас', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'сейчас', 3, 0),))]\n",
      "\n",
      "\n",
      "я\n",
      "{'value': {'characters': [{'tag': 'Masculine', 'type': 'gender'}, {'tag': 'DEICTIC', 'type': 'pronoun'}, {'tag': 'Singular', 'type': 'number'}, {'tag': 'Nominative', 'type': 'case'}, {'tag': 'Animated', 'type': 'animacy'}], 'tag': 'S', 'type': 'syn-tag-rus'}, 'start': 7, 'text': 'я', 'end': 8, 'annotated-text': 'сейчас я говорю о том человеке который спит отдыхая на работе'}\n",
      "[Parse(word='я', tag=OpencorporaTag('NPRO,1per sing,nomn'), normal_form='я', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'я', 3100, 0),))]\n",
      "\n",
      "\n",
      "говорить\n",
      "{'value': {'characters': [{'tag': 'First', 'type': 'person'}, {'tag': 'Singular', 'type': 'number'}, {'tag': 'NotPast', 'type': 'tense'}, {'tag': 'Indicative', 'type': 'mode'}], 'tag': 'V', 'type': 'syn-tag-rus'}, 'start': 9, 'text': 'говорю', 'end': 15, 'annotated-text': 'сейчас я говорю о том человеке который спит отдыхая на работе'}\n",
      "[Parse(word='говорить', tag=OpencorporaTag('INFN,impf,tran'), normal_form='говорить', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'говорить', 395, 0),))]\n",
      "\n",
      "\n",
      "о\n",
      "{'value': {'characters': [], 'tag': 'PR', 'type': 'syn-tag-rus'}, 'start': 16, 'text': 'о', 'end': 17, 'annotated-text': 'сейчас я говорю о том человеке который спит отдыхая на работе'}\n",
      "[Parse(word='о', tag=OpencorporaTag('PREP'), normal_form='о', score=0.990985, methods_stack=((<DictionaryAnalyzer>, 'о', 2133, 0),)), Parse(word='о', tag=OpencorporaTag('INTJ'), normal_form='о', score=0.009014, methods_stack=((<DictionaryAnalyzer>, 'о', 21, 0),))]\n",
      "\n",
      "\n",
      "то\n",
      "{'value': {'characters': [{'tag': 'Neuter', 'type': 'gender'}, {'tag': 'Prepositional', 'type': 'case'}, {'tag': 'Singular', 'type': 'number'}, {'tag': 'Inanimated', 'type': 'animacy'}], 'tag': 'S', 'type': 'syn-tag-rus'}, 'start': 18, 'text': 'том', 'end': 21, 'annotated-text': 'сейчас я говорю о том человеке который спит отдыхая на работе'}\n",
      "[Parse(word='то', tag=OpencorporaTag('CONJ'), normal_form='то', score=0.5, methods_stack=((<DictionaryAnalyzer>, 'то', 20, 0),)), Parse(word='то', tag=OpencorporaTag('PRCL'), normal_form='то', score=0.1, methods_stack=((<DictionaryAnalyzer>, 'то', 22, 0),))]\n",
      "\n",
      "\n",
      "человек\n",
      "{'value': {'characters': [{'tag': 'Masculine', 'type': 'gender'}, {'tag': 'Prepositional', 'type': 'case'}, {'tag': 'Singular', 'type': 'number'}, {'tag': 'Animated', 'type': 'animacy'}], 'tag': 'S', 'type': 'syn-tag-rus'}, 'start': 22, 'text': 'человеке', 'end': 30, 'annotated-text': 'сейчас я говорю о том человеке который спит отдыхая на работе'}\n",
      "[Parse(word='человек', tag=OpencorporaTag('NOUN,anim,masc sing,nomn'), normal_form='человек', score=0.549382, methods_stack=((<DictionaryAnalyzer>, 'человек', 488, 0),)), Parse(word='человек', tag=OpencorporaTag('NOUN,anim,masc sing,nomn'), normal_form='человек', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'человек', 488, 0),))]\n",
      "\n",
      "\n",
      "который\n",
      "{'value': {'characters': [{'tag': 'Masculine', 'type': 'gender'}, {'tag': 'Nominative', 'type': 'case'}, {'tag': 'Singular', 'type': 'number'}], 'tag': 'S', 'type': 'syn-tag-rus'}, 'start': 31, 'text': 'который', 'end': 38, 'annotated-text': 'сейчас я говорю о том человеке который спит отдыхая на работе'}\n",
      "[Parse(word='который', tag=OpencorporaTag('ADJF,Apro,Subx,Anph masc,sing,nomn'), normal_form='который', score=0.823529, methods_stack=((<DictionaryAnalyzer>, 'который', 1788, 0),)), Parse(word='который', tag=OpencorporaTag('ADJF,Apro,Subx,Anph masc,sing,nomn'), normal_form='который', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'который', 1788, 0),))]\n",
      "\n",
      "\n",
      "спать\n",
      "{'value': {'characters': [{'tag': 'Third', 'type': 'person'}, {'tag': 'Singular', 'type': 'number'}, {'tag': 'NotPast', 'type': 'tense'}, {'tag': 'Indicative', 'type': 'mode'}], 'tag': 'V', 'type': 'syn-tag-rus'}, 'start': 39, 'text': 'спит', 'end': 43, 'annotated-text': 'сейчас я говорю о том человеке который спит отдыхая на работе'}\n",
      "[Parse(word='спать', tag=OpencorporaTag('INFN,impf,intr'), normal_form='спать', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'спать', 2808, 0),))]\n",
      "\n",
      "\n",
      "отдыхать\n",
      "{'value': {'characters': [{'tag': 'Gerund', 'type': 'representation'}, {'tag': 'NotPast', 'type': 'tense'}], 'tag': 'V', 'type': 'syn-tag-rus'}, 'start': 44, 'text': 'отдыхая', 'end': 51, 'annotated-text': 'сейчас я говорю о том человеке который спит отдыхая на работе'}\n",
      "[Parse(word='отдыхать', tag=OpencorporaTag('INFN,impf,intr'), normal_form='отдыхать', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'отдыхать', 2247, 0),))]\n",
      "\n",
      "\n",
      "на\n",
      "{'value': {'characters': [], 'tag': 'PR', 'type': 'syn-tag-rus'}, 'start': 52, 'text': 'на', 'end': 54, 'annotated-text': 'сейчас я говорю о том человеке который спит отдыхая на работе'}\n",
      "[Parse(word='на', tag=OpencorporaTag('PREP'), normal_form='на', score=0.99931, methods_stack=((<DictionaryAnalyzer>, 'на', 24, 0),)), Parse(word='на', tag=OpencorporaTag('PRCL'), normal_form='на', score=0.000477, methods_stack=((<DictionaryAnalyzer>, 'на', 22, 0),)), Parse(word='на', tag=OpencorporaTag('INTJ'), normal_form='на', score=0.000212, methods_stack=((<DictionaryAnalyzer>, 'на', 21, 0),))]\n",
      "\n",
      "\n",
      "работа\n",
      "{'value': {'characters': [{'tag': 'Feminine', 'type': 'gender'}, {'tag': 'Prepositional', 'type': 'case'}, {'tag': 'Singular', 'type': 'number'}, {'tag': 'Inanimated', 'type': 'animacy'}], 'tag': 'S', 'type': 'syn-tag-rus'}, 'start': 55, 'text': 'работе', 'end': 61, 'annotated-text': 'сейчас я говорю о том человеке который спит отдыхая на работе'}\n",
      "[Parse(word='работа', tag=OpencorporaTag('NOUN,inan,femn sing,nomn'), normal_form='работа', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'работа', 55, 0),))]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i = 5\n",
    "\n",
    "message = 'сейчас я говорю о том человеке который спит отдыхая на работе'\n",
    "message_tokens = tokenizer.tokenize(message)\n",
    "parse_tokens = [morph.parse(t) for t in message_tokens]\n",
    "#print(parse_tokens[i])\n",
    "isp_POS = isp_api.posTaggingAnnotate(text = message)\n",
    "isp_lemma = isp_api.lemmatizationAnnotate(text = message)\n",
    "\n",
    "#print(isp_POS['annotations']['pos-token'][i])\n",
    "\n",
    "#print(isp_lemma.get('annotations').get('lemma')[i].get('value'))\n",
    "\n",
    "#print(' '.join([l.get('value') for l in isp_lemma.get('annotations').get('lemma')]))\n",
    "\n",
    "for j in range(len(message_tokens)):\n",
    "    isp_lemma_token = isp_lemma.get('annotations').get('lemma')[j].get('value')\n",
    "    morph_parse_token = [w.normalized for w in morph.parse(isp_lemma_token) if w.normal_form == w.word]\n",
    "    print(isp_lemma_token)\n",
    "    print(isp_POS['annotations']['pos-token'][j])\n",
    "    print(morph_parse_token)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "встать\n",
      "{'text': 'встать', 'annotations': {'lemma': [{'text': 'встать', 'value': 'вставать', 'annotated-text': 'встать', 'start': 0, 'end': 6}]}}\n",
      "[Parse(word='встать', tag=OpencorporaTag('INFN,perf,intr'), normal_form='встать', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'встать', 904, 0),))]\n",
      "\n",
      "\n",
      "вставать\n",
      "{'text': 'вставать', 'annotations': {'lemma': [{'text': 'вставать', 'value': 'вставать', 'annotated-text': 'вставать', 'start': 0, 'end': 8}]}}\n",
      "[Parse(word='вставать', tag=OpencorporaTag('INFN,impf,intr'), normal_form='вставать', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'вставать', 903, 0),))]\n",
      "\n",
      "\n",
      "встали\n",
      "{'text': 'встали', 'annotations': {'lemma': [{'text': 'встали', 'value': 'вставать', 'annotated-text': 'встали', 'start': 0, 'end': 6}]}}\n",
      "[Parse(word='встали', tag=OpencorporaTag('VERB,perf,intr plur,past,indc'), normal_form='встать', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'встали', 904, 4),))]\n",
      "\n",
      "\n",
      "вставали\n",
      "{'text': 'вставали', 'annotations': {'lemma': [{'text': 'вставали', 'value': 'вставать', 'annotated-text': 'вставали', 'start': 0, 'end': 8}]}}\n",
      "[Parse(word='вставали', tag=OpencorporaTag('VERB,impf,intr plur,past,indc'), normal_form='вставать', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'вставали', 903, 10),))]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ws = ['встать', 'вставать', 'встали', 'вставали']\n",
    "for w in ws:\n",
    "    print(w)\n",
    "    ns = morph.parse(w)\n",
    "    ts = isp_api.lemmatizationAnnotate(text = w)\n",
    "    print(ts)\n",
    "    print(ns)\n",
    "        \n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

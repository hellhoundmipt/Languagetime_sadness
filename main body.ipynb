{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json_lines as jl\n",
    "import nltk\n",
    "import xmltodict\n",
    "from ispras import texterra as isp\n",
    "import pymorphy2 as pm2\n",
    "import gensim\n",
    "import time as tm\n",
    "\n",
    "isp_api = isp.API('ba74236a7212a71054ae1408b30b1bdef771d35b')\n",
    "#nltk.download()\n",
    "\n",
    "tokenizer = nltk.TweetTokenizer()\n",
    "morph = pm2.MorphAnalyzer()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Нормализация с помощью Texterra <s>и pymorphy2</s> \n",
    "\n",
    "Читаю комментарии из файла по очереди, разбиваю каждый на предложения, лемматизирую с помощью Texterra <s>, параллельно выбираю у pymorpy лучший разбор и составляю словарь нормальных форм (элементы типа pymorphy2.Parse).</s> Сохраняю нормализованные предложения в файл lemmatized.txt <s>, ибо по-другому я не придумал, как заставить word2vec работать:/</s>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FullVocab:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.id_counter = 0\n",
    "        self.vocab = []\n",
    "        \n",
    "    def add_item(self, word, normal_form, tags_normal, tags_word):\n",
    "        for item in self.vocab:\n",
    "            if item.get('normal_form') == normal_form and item.get('tags') == tags_normal:\n",
    "                item.get('word_forms').append({'word': word,'tags': tags_word})\n",
    "                return item['id']\n",
    "                \n",
    "        new_item = {'id': self.id_counter, 'normal_form': normal_form, 'tags': tags_normal}\n",
    "        new_item['word_forms'] = [{'word': word,'tags': tags_word}]\n",
    "        self.vocab.append(new_item)\n",
    "        self.id_counter = self.id_counter + 1\n",
    "        return self.id_counter - 1\n",
    "            \n",
    "    def show_vocab(self):\n",
    "        print(self.vocab)\n",
    "        \n",
    "    def build_xml(self):\n",
    "        f = open('vocab.xml', 'w')\n",
    "        root = etree.Element('root')\n",
    "        for item in self.vocab:\n",
    "            child = etree.SubElement(root, 'w' + str(item['id']))\n",
    "            etree.SubElement(child, 'id').text = str(item['id'])\n",
    "            etree.SubElement(child, 'normal_form').text = item['normal_form']\n",
    "            \n",
    "        #print(etree.tostring(root, encoding='unicode', method='xml', pretty_print=True))\n",
    "        f.write(etree.tostring(root, encoding='unicode', method='xml', pretty_print=True))\n",
    "        \n",
    "    def read_xml(self):\n",
    "        f = open('vocab.xml', 'r')\n",
    "        root = etree.XML(f.read())\n",
    "        #print(etree.tostring(root[0], encoding='unicode', method='xml', pretty_print=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'на и понимать разуметься '"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatized_txt = open('lemmatized.txt', 'w', encoding='utf8')\n",
    "\n",
    "parse_normal_dict = list()\n",
    "\n",
    "with open('learn_scrapy\\pda.jl', 'rb') as f:\n",
    "    for comment in jl.reader(f):\n",
    "        sents = nltk.sent_tokenize(comment)\n",
    "        for sent in sents:\n",
    "            toks = tknzr.tokenize(sent)\n",
    "            toks = [t.lower() for t in toks if t.isalnum()]\n",
    "            sent = ' '.join(toks)\n",
    "            \n",
    "            lemma_raw = isp_api.lemmatizationAnnotate(text = sent)\n",
    "            lemmatized = ''\n",
    "            if lemma_raw.get('annotations').get('lemma') is not None:\n",
    "                for l in lemma_raw.get('annotations').get('lemma'):\n",
    "                    lemmatized = lemmatized + l.get('value') + ' '"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "comments = ''\n",
    "with open('learn_scrapy\\pda.jl', 'rb') as f:\n",
    "    for item in jl.reader(f):\n",
    "        comments = comments + item['comment']       #создаём один string на все комменты\n",
    "                                                    #скорее всего, будет логичнее для N-граммов каждое предложение\n",
    "                                                    #запоминать в один string и хранить как list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kek1\n",
      "Kek2\n",
      "Kek3\n",
      "Kek4\n"
     ]
    }
   ],
   "source": [
    "#тут токенизируем комментарии\n",
    "#есть подозрение, что я создаю слишком много сущностей \n",
    "\n",
    "tknzr = nltk.TweetTokenizer()\n",
    "print('Kek1')\n",
    "tokens = tknzr.tokenize(comments)\n",
    "print('Kek2')\n",
    "text = nltk.Text(tokens)\n",
    "print('Kek3')\n",
    "sents = nltk.sent_tokenize(comments)                         #строить n-граммную модель, возможно, буду \n",
    "print('Kek4')                                                             #анализируя предложения по-отдельности\n",
    "words = sorted([w.lower() for w in tokens if w.isalpha()])   #впоследствии буду отсекать наименее наиболее частотные слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10600020178435018\n"
     ]
    }
   ],
   "source": [
    "def lexical_diversity(text):\n",
    "    return len(set(text)) / len(text)\n",
    "\n",
    "print(lexical_diversity(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sent_tokens = list()                            #здесь храним предложения\n",
    "for i in range(len(sents)):\n",
    "    sent_tokens.append(tknzr.tokenize(sents[i]))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#сох нормальные формы слов\n",
    "morph = pm2.MorphAnalyzer()     \n",
    "vocab_normalized = sorted(set([morph.parse(w)[0].normalized for w in words]), key = lambda w: w.normal_form)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def progress(full, curr, checkpoint):\n",
    "    if curr / full * 100 > checkpoint:\n",
    "        print(curr / full * 100, end = '')\n",
    "        print('% complited')\n",
    "        return True\n",
    "    \n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "i = 0\n",
    "h = 0.1\n",
    "checkpoint = h\n",
    "time = tm.clock()\n",
    "for sent in sents:\n",
    "    if progress(len(sents), i, checkpoint):\n",
    "        time_per_checkpoint = tm.clock() - time\n",
    "        print('Time per checkpoint:', end = ' ')\n",
    "        print(time_per_checkpoint)\n",
    "        print('Time per sentence:', end = ' ')\n",
    "        time_per_sent = time_per_checkpoint / (len(sents) * h)\n",
    "        print(time_per_sent)\n",
    "        print('Estimated time:', end = ' ')\n",
    "        print(time_per_checkpoint * (100 - checkpoint))\n",
    "        time = tm.clock()\n",
    "        checkpoint = checkpoint + h\n",
    "        \n",
    "    toks = tknzr.tokenize(sent)\n",
    "    toks = [t.lower() for t in toks if t.isalnum()]\n",
    "    sentence = ''\n",
    "    for t in toks:\n",
    "        sentence = sentence + t + ' '\n",
    "    \n",
    "    lemma_raw = isp_api.lemmatizationAnnotate(text = sentence)\n",
    "    lemmatized = ''\n",
    "    if lemma_raw.get('annotations').get('lemma') is not None:\n",
    "        for l in lemma_raw.get('annotations').get('lemma'):\n",
    "            lemmatized = lemmatized + l.get('value') + ' '\n",
    "        \n",
    "        sents[i] = lemmatized\n",
    "    \n",
    "    else:\n",
    "        sents[i] = ''\n",
    "        \n",
    "    i = i + 1\n",
    "        \n",
    "    \n",
    "sents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sents' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-e8335ea762b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'lemmatized.txt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf8'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msents\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m5000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sents' is not defined"
     ]
    }
   ],
   "source": [
    "with open('lemmatized.txt', 'w', encoding='utf8') as f:\n",
    "    for s in sents[0:5000]:\n",
    "        f.write(s + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec(gensim.models.word2vec.LineSentence('lemmatized.txt'), min_count=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('а', 0.9998720288276672),\n",
       " ('они', 0.9998717904090881),\n",
       " ('где', 0.9998696446418762),\n",
       " ('этот', 0.9998689293861389),\n",
       " ('мочь', 0.9998672604560852),\n",
       " ('когда', 0.999864935874939),\n",
       " ('это', 0.9998648166656494),\n",
       " ('там', 0.9998632669448853),\n",
       " ('с', 0.9998587965965271),\n",
       " ('ничто', 0.9998581409454346)]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive = ['айфон'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "сейчас\n",
      "{'text': 'сейчас', 'value': {'characters': [], 'tag': 'ADV', 'type': 'syn-tag-rus'}, 'annotated-text': 'сейчас я говорю о тех которые гребут', 'start': 0, 'end': 6}\n",
      "[Parse(word='сейчас', tag=OpencorporaTag('ADVB'), normal_form='сейчас', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'сейчас', 3, 0),))]\n",
      "\n",
      "\n",
      "я\n",
      "{'text': 'я', 'value': {'characters': [{'tag': 'Masculine', 'type': 'gender'}, {'tag': 'DEICTIC', 'type': 'pronoun'}, {'tag': 'Singular', 'type': 'number'}, {'tag': 'Nominative', 'type': 'case'}, {'tag': 'Animated', 'type': 'animacy'}], 'tag': 'S', 'type': 'syn-tag-rus'}, 'annotated-text': 'сейчас я говорю о тех которые гребут', 'start': 7, 'end': 8}\n",
      "[Parse(word='я', tag=OpencorporaTag('NPRO,1per sing,nomn'), normal_form='я', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'я', 3100, 0),))]\n",
      "\n",
      "\n",
      "говорить\n",
      "{'text': 'говорю', 'value': {'characters': [{'tag': 'First', 'type': 'person'}, {'tag': 'Singular', 'type': 'number'}, {'tag': 'NotPast', 'type': 'tense'}, {'tag': 'Indicative', 'type': 'mode'}], 'tag': 'V', 'type': 'syn-tag-rus'}, 'annotated-text': 'сейчас я говорю о тех которые гребут', 'start': 9, 'end': 15}\n",
      "[Parse(word='говорить', tag=OpencorporaTag('INFN,impf,tran'), normal_form='говорить', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'говорить', 395, 0),))]\n",
      "\n",
      "\n",
      "о\n",
      "{'text': 'о', 'value': {'characters': [], 'tag': 'PR', 'type': 'syn-tag-rus'}, 'annotated-text': 'сейчас я говорю о тех которые гребут', 'start': 16, 'end': 17}\n",
      "[Parse(word='о', tag=OpencorporaTag('PREP'), normal_form='о', score=0.990985, methods_stack=((<DictionaryAnalyzer>, 'о', 2133, 0),)), Parse(word='о', tag=OpencorporaTag('INTJ'), normal_form='о', score=0.009014, methods_stack=((<DictionaryAnalyzer>, 'о', 21, 0),))]\n",
      "\n",
      "\n",
      "тот\n",
      "{'text': 'тех', 'value': {'characters': [{'tag': 'Genitive', 'type': 'case'}, {'tag': 'Plural', 'type': 'number'}], 'tag': 'A', 'type': 'syn-tag-rus'}, 'annotated-text': 'сейчас я говорю о тех которые гребут', 'start': 18, 'end': 21}\n",
      "[Parse(word='тот', tag=OpencorporaTag('ADJF,Apro,Subx,Anph masc,sing,nomn'), normal_form='тот', score=0.703703, methods_stack=((<DictionaryAnalyzer>, 'тот', 2900, 0),)), Parse(word='тот', tag=OpencorporaTag('ADJF,Apro,Subx,Anph masc,sing,nomn'), normal_form='тот', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'тот', 2900, 0),))]\n",
      "\n",
      "\n",
      "который\n",
      "{'text': 'которые', 'value': {'characters': [{'tag': 'Nominative', 'type': 'case'}, {'tag': 'Plural', 'type': 'number'}], 'tag': 'S', 'type': 'syn-tag-rus'}, 'annotated-text': 'сейчас я говорю о тех которые гребут', 'start': 22, 'end': 29}\n",
      "[Parse(word='который', tag=OpencorporaTag('ADJF,Apro,Subx,Anph masc,sing,nomn'), normal_form='который', score=0.823529, methods_stack=((<DictionaryAnalyzer>, 'который', 1788, 0),)), Parse(word='который', tag=OpencorporaTag('ADJF,Apro,Subx,Anph masc,sing,nomn'), normal_form='который', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'который', 1788, 0),))]\n",
      "\n",
      "\n",
      "гребти\n",
      "{'text': 'гребут', 'value': {'characters': [{'tag': 'Third', 'type': 'person'}, {'tag': 'Plural', 'type': 'number'}, {'tag': 'NotPast', 'type': 'tense'}, {'tag': 'Indicative', 'type': 'mode'}], 'tag': 'V', 'type': 'syn-tag-rus'}, 'annotated-text': 'сейчас я говорю о тех которые гребут', 'start': 30, 'end': 36}\n",
      "[Parse(word='гребти', tag=OpencorporaTag('NOUN,inan,femn,Sgtm,Fixd sing,nomn'), normal_form='гребти', score=0.1666666666666667, methods_stack=((<FakeDictionary>, 'гребти', 290, 0), (<KnownSuffixAnalyzer>, 'ебти'))), Parse(word='гребти', tag=OpencorporaTag('NOUN,inan,femn,Sgtm,Fixd sing,nomn'), normal_form='гребти', score=1.0, methods_stack=((<FakeDictionary>, 'гребти', 290, 0), (<KnownSuffixAnalyzer>, 'ебти'))), Parse(word='гребти', tag=OpencorporaTag('NOUN,inan,femn,Sgtm,Fixd sing,nomn'), normal_form='гребти', score=1.0, methods_stack=((<FakeDictionary>, 'гребти', 290, 0), (<KnownSuffixAnalyzer>, 'ебти'))), Parse(word='гребти', tag=OpencorporaTag('NOUN,inan,femn,Sgtm,Fixd sing,nomn'), normal_form='гребти', score=1.0, methods_stack=((<FakeDictionary>, 'гребти', 290, 0), (<KnownSuffixAnalyzer>, 'ебти'))), Parse(word='гребти', tag=OpencorporaTag('NOUN,inan,femn,Sgtm,Fixd sing,nomn'), normal_form='гребти', score=1.0, methods_stack=((<FakeDictionary>, 'гребти', 290, 0), (<KnownSuffixAnalyzer>, 'ебти'))), Parse(word='гребти', tag=OpencorporaTag('NOUN,inan,femn,Sgtm,Fixd sing,nomn'), normal_form='гребти', score=1.0, methods_stack=((<FakeDictionary>, 'гребти', 290, 0), (<KnownSuffixAnalyzer>, 'ебти')))]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i = 5\n",
    "\n",
    "message = 'сейчас я говорю о тех которые гребут'\n",
    "message_tokens = tokenizer.tokenize(message)\n",
    "parse_tokens = [morph.parse(t) for t in message_tokens]\n",
    "#print(parse_tokens[i])\n",
    "isp_POS = isp_api.posTaggingAnnotate(text = message)\n",
    "isp_lemma = isp_api.lemmatizationAnnotate(text = message)\n",
    "\n",
    "#print(isp_POS['annotations']['pos-token'][i])\n",
    "\n",
    "#print(isp_lemma.get('annotations').get('lemma')[i].get('value'))\n",
    "\n",
    "#print(' '.join([l.get('value') for l in isp_lemma.get('annotations').get('lemma')]))\n",
    "\n",
    "for j in range(len(message_tokens)):\n",
    "    isp_lemma_token = isp_lemma.get('annotations').get('lemma')[j].get('value')\n",
    "    morph_parse_token = [w.normalized for w in morph.parse(isp_lemma_token) if w.normal_form == w.word]\n",
    "    print(isp_lemma_token)\n",
    "    print(isp_POS['annotations']['pos-token'][j])\n",
    "    print(morph_parse_token)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "встать\n",
      "{'text': 'встать', 'annotations': {'lemma': [{'text': 'встать', 'value': 'вставать', 'annotated-text': 'встать', 'start': 0, 'end': 6}]}}\n",
      "[Parse(word='встать', tag=OpencorporaTag('INFN,perf,intr'), normal_form='встать', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'встать', 904, 0),))]\n",
      "\n",
      "\n",
      "вставать\n",
      "{'text': 'вставать', 'annotations': {'lemma': [{'text': 'вставать', 'value': 'вставать', 'annotated-text': 'вставать', 'start': 0, 'end': 8}]}}\n",
      "[Parse(word='вставать', tag=OpencorporaTag('INFN,impf,intr'), normal_form='вставать', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'вставать', 903, 0),))]\n",
      "\n",
      "\n",
      "встали\n",
      "{'text': 'встали', 'annotations': {'lemma': [{'text': 'встали', 'value': 'вставать', 'annotated-text': 'встали', 'start': 0, 'end': 6}]}}\n",
      "[Parse(word='встали', tag=OpencorporaTag('VERB,perf,intr plur,past,indc'), normal_form='встать', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'встали', 904, 4),))]\n",
      "\n",
      "\n",
      "вставали\n",
      "{'text': 'вставали', 'annotations': {'lemma': [{'text': 'вставали', 'value': 'вставать', 'annotated-text': 'вставали', 'start': 0, 'end': 8}]}}\n",
      "[Parse(word='вставали', tag=OpencorporaTag('VERB,impf,intr plur,past,indc'), normal_form='вставать', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'вставали', 903, 10),))]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ws = ['встать', 'вставать', 'встали', 'вставали']\n",
    "for w in ws:\n",
    "    print(w)\n",
    "    ns = morph.parse(w)\n",
    "    ts = isp_api.lemmatizationAnnotate(text = w)\n",
    "    print(ts)\n",
    "    print(ns)\n",
    "        \n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

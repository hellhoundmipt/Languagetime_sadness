{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Maxim\\Anaconda3\\lib\\site-packages\\gensim-2.2.0-py3.5-win-amd64.egg\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import json_lines as jl\n",
    "import nltk\n",
    "import xmltodict\n",
    "from ispras import texterra as isp\n",
    "import pymorphy2 as pm2\n",
    "import gensim\n",
    "import time as tm\n",
    "from lxml import etree\n",
    "import sys\n",
    "\n",
    "isp_api = isp.API('ba74236a7212a71054ae1408b30b1bdef771d35b')\n",
    "#nltk.download()\n",
    "\n",
    "tokenizer = nltk.TweetTokenizer()\n",
    "morph = pm2.MorphAnalyzer()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лемматизация с помощью Texterra\n",
    "\n",
    "Читаю комментарии и статьи из файла по очереди, разбиваю их на предложения, лемматизирую с помощью Texterra, составляя при этом словарь FullVocab. Записи в Fullvocab хранятся в следующем виде:\n",
    "\n",
    "__item__:\n",
    "- __id__: id нормальной формы\n",
    "- __normal_form__: нормальная форма\n",
    "- __tags__: часть речи\n",
    "- __word_forms__: список встретившихся словоформ\n",
    "- __cnt__: сколько раз встречалось\n",
    "- __origin__: в каком корпусе текстов слово встретилось (t - треннировочный [комментарии], a - целевой [статьи], e - дополнительный [например, вешний словарь])\n",
    "\n",
    "В lemmatized.txt записываются построчно лемматизированные предложения, но вместо слов в нём \"id нормальной формы\", чтобы для word2vec слова \"стать\" (<i>глагол</i>) и \"стать\" (<i>сущ</i>) не были одним и тем же словом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class FullVocab:\n",
    "    \n",
    "    # Инициализация, либо пустой класс, либо из файла vocab.xml\n",
    "    def __init__(self, from_file = False):\n",
    "        self.id_counter = 0\n",
    "        self.vocab = []\n",
    "        if from_file:\n",
    "            f = open('vocab.xml', 'rb')\n",
    "            root = etree.XML(f.read())\n",
    "            for item in root:\n",
    "                self.add_item(item[3].text.split(' '), item[1].text, item[2].text, item[5].text, cnt = item[4].text, \n",
    "                              word_id = item[0].text)\n",
    "            \n",
    "        \n",
    "    # Добавляем новое слово в словарь\n",
    "    def add_item(self, words, normal_form, tags_normal, origin, cnt = 0, word_id = -1):\n",
    "        \n",
    "        # Дабы было можно и str и list в аргументы добавлять\n",
    "        if type(words) is not list:\n",
    "            words = [words]\n",
    "            \n",
    "        # Проверяем, есть ли нормальная форма слова в словаре. Если есть, то обновляем словоформы, если нет, то \n",
    "        # добавляем новую запись в словарь, присвивая уникальный id\n",
    "        for item in self.vocab:\n",
    "            if item.get('normal_form') == normal_form and item.get('tags') == tags_normal:\n",
    "                item['word_forms'].extend(words)\n",
    "                item['word_forms'] = sorted(set(item['word_forms']))\n",
    "                item['cnt'] += 1\n",
    "                item['origin'] = self.handle_origin(origin, item['origin'])\n",
    "                return item['id']\n",
    "                \n",
    "        #Если такой записи ещё не было\n",
    "        new_item = {'normal_form': normal_form, 'tags': tags_normal, 'word_forms': words}\n",
    "        if normal_form not in new_item['word_forms']:\n",
    "            new_item['word_forms'].append(normal_form)\n",
    "        if int(cnt) > 0:\n",
    "            new_item['cnt'] = int(cnt)\n",
    "        else:\n",
    "            new_item['cnt'] = 1\n",
    "            \n",
    "        new_item['origin'] = self.handle_origin(origin)\n",
    "        \n",
    "        return_id = 0\n",
    "        if word_id == -1:         #если мы не уточняем, какой именно id у нового слова (когда просто добавляем новое слово)\n",
    "            new_item['id'] = self.id_counter\n",
    "            return_id = self.id_counter\n",
    "            self.id_counter = self.id_counter + 1\n",
    "        else:                     #если мы указали id для нового слова (когда читаем из файла сформированный словарь)\n",
    "            return_id = int(word_id)\n",
    "            new_item['id'] = return_id\n",
    "            self.id_counter = return_id + 1\n",
    "            \n",
    "        self.vocab.append(new_item)\n",
    "        return return_id\n",
    "         \n",
    "        \n",
    "    def show_vocab(self):\n",
    "        print(self.vocab)\n",
    "        \n",
    "        \n",
    "    # Сохраняем копию данные класса в vocab.xml\n",
    "    def build_xml(self):\n",
    "        f = open('vocab.xml', 'wb')\n",
    "        root = etree.Element('root')\n",
    "        for item in self.vocab:\n",
    "            child = etree.SubElement(root, 'w' + str(item['id']))\n",
    "            etree.SubElement(child, 'id').text = str(item['id'])\n",
    "            etree.SubElement(child, 'normal_form').text = item['normal_form']\n",
    "            etree.SubElement(child, 'tags').text = item['tags']\n",
    "            etree.SubElement(child, 'word_forms').text = ' '.join(item['word_forms'])\n",
    "            etree.SubElement(child, 'cnt').text = str(item['cnt'])\n",
    "            etree.SubElement(child, 'origin').text = item['origin']\n",
    "            \n",
    "        result = etree.tostring(root, encoding='unicode', method='xml', pretty_print=True)\n",
    "        f.write(result.encode('utf8'))\n",
    "        \n",
    "        \n",
    "    # Ищем нормальную форму или запись в словаре по id\n",
    "    # Если return_val = \"word\", то возвращаем нормальную форму, если \"item\", то запись в словаре\n",
    "    def word_by_id(self, word_id, return_val = \"word\"):\n",
    "        item = next((item for item in self.vocab if item[\"id\"] == word_id), None)\n",
    "        if item is None:\n",
    "            return None\n",
    "        elif return_val == 'word':\n",
    "            return item['normal_form']\n",
    "        elif return_val == 'item':\n",
    "            return item\n",
    "        \n",
    "        \n",
    "    def id_by_word(self, word, tag = None):\n",
    "        if tag == None:\n",
    "            item = next((item for item in self.vocab if item['normal_form'] == word), None)\n",
    "        else:\n",
    "            item = next((item for item in self.vocab if item['normal_form'] == word and item['tags'] == tag), None)\n",
    "        if item is None:\n",
    "            return None\n",
    "        else:\n",
    "            return item['id']\n",
    "    \n",
    "    \n",
    "    def handle_origin(self, new, old = 'xxx'):\n",
    "        if len(new) == 1:\n",
    "            if new == 't':\n",
    "                return 't' + old[1:3]\n",
    "            elif new == 'a':\n",
    "                return old[0] + 'a' + old[2]\n",
    "            elif new == 'e':\n",
    "                return  old[0:2] + 'e'\n",
    "            \n",
    "        elif len(new) == 3:\n",
    "            return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nlemmatized_txt = open(\\'lemmatized.txt\\', \\'w\\', encoding=\\'utf8\\')\\n\\nwith open(\\'learn_scrapy\\\\pda.jl\\', \\'rb\\') as f:\\n    \\n    vocab = FullVocab()\\n    num_comments = 100617\\n    counter = 0\\n    prev_counter = 0\\n    anchor_counter = 0\\n    checkpoint = tm.clock()\\n    dt = 900\\n    \\n    for item in jl.reader(f):\\n        item_origin = None    #Сюда пишем \\'t\\', если comment, \\'a\\', если article\\n        sents = None\\n        \\n        if item.get(\\'comment\\') is not None:\\n            sents = nltk.sent_tokenize(item[\\'comment\\'])\\n            item_origin = \\'t\\'\\n        elif item.get(\\'article\\') is not None:   \\n            sents = nltk.sent_tokenize(item[\\'article\\'])\\n            item_origin = \\'a\\'\\n            \\n        for sent in sents:\\n            toks = tokenizer.tokenize(sent)\\n            toks = [t.lower() for t in toks if t.isalnum()]\\n            sent = \\' \\'.join(toks)\\n            \\n            try:\\n                lemma_annotate = isp_api.lemmatizationAnnotate(text = sent)\\n                pos_annotate = isp_api.posTaggingAnnotate(text = sent)\\n                \\n            except KeyboardInterrupt:\\n                print(\"Understandable, have a nice day\")\\n                vocab.build_xml()\\n                lemmatized_txt.close()\\n                sys.exit()\\n                \\n            except:\\n                print(\"Sentence\", end = \" \"\")\\n                print(sent, end = \"\" \")\\n                print(\"in commentary #\", end = \"\")\\n                print(counter, end = \" \")\\n                print(\"not processed\")\\n                print(\"\\n\")\\n                continue\\n                \\n            if lemma_annotate.get(\\'annotations\\').get(\\'lemma\\') is not None:\\n                lemmatized = []\\n                for i in range(len(lemma_annotate.get(\\'annotations\\').get(\\'lemma\\'))):\\n                    l = lemma_annotate.get(\\'annotations\\').get(\\'lemma\\')[i].get(\\'value\\')\\n                    p = pos_annotate[\\'annotations\\'][\\'pos-token\\'][i][\\'value\\'].get(\\'tag\\')\\n                    word_id = vocab.add_item(toks[i], l, p, item_origin)\\n                    lemmatized.append(str(word_id))\\n                \\n                lemmatized = \\' \\'.join(lemmatized)\\n                lemmatized_txt.write(lemmatized + \\'\\n\\')\\n        \\n        counter = counter + 1\\n        if(tm.clock() - checkpoint > dt):\\n            print(\"Progress: \", end = \\'\\')\\n            print(counter / num_comments * 100, end = \\'\\')\\n            print(\"%\")\\n            print(\"Comments processed by turn: \", end= \\'\\')\\n            print(counter - prev_counter)\\n            print(\"Comments processed total: \", end= \\'\\')\\n            print(counter)\\n            print(\"Time left: \", end = \\'\\')\\n            speed = (counter - prev_counter) / dt\\n            print(speed * (num_comments - counter))\\n            print(\"Speed: \", end = \\'\\')\\n            print(speed, end = \\' \\')\\n            print(\"comments/sec\")\\n            print(\"\\n\")\\n            prev_counter = counter\\n            vocab.build_xml()\\n            if (counter - anchor_counter) / num_comments * 100 > 25:\\n                print(\"Sleeping for 20 minutes\")\\n                tm.sleep(1200)\\n                anchor_counter = counter\\n            checkpoint = tm.clock()\\n            \\n    vocab.build_xml()\\n    print(\"Finished\")\\n    \\nlemmatized_txt.close()\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "lemmatized_txt = open('lemmatized.txt', 'w', encoding='utf8')\n",
    "\n",
    "with open('learn_scrapy\\pda.jl', 'rb') as f:\n",
    "    \n",
    "    vocab = FullVocab()\n",
    "    num_comments = 100617\n",
    "    counter = 0\n",
    "    prev_counter = 0\n",
    "    anchor_counter = 0\n",
    "    checkpoint = tm.clock()\n",
    "    dt = 900\n",
    "    \n",
    "    for item in jl.reader(f):\n",
    "        item_origin = None    #Сюда пишем 't', если comment, 'a', если article\n",
    "        sents = None\n",
    "        \n",
    "        if item.get('comment') is not None:\n",
    "            sents = nltk.sent_tokenize(item['comment'])\n",
    "            item_origin = 't'\n",
    "        elif item.get('article') is not None:   \n",
    "            sents = nltk.sent_tokenize(item['article'])\n",
    "            item_origin = 'a'\n",
    "            \n",
    "        for sent in sents:\n",
    "            toks = tokenizer.tokenize(sent)\n",
    "            toks = [t.lower() for t in toks if t.isalnum()]\n",
    "            sent = ' '.join(toks)\n",
    "            \n",
    "            try:\n",
    "                lemma_annotate = isp_api.lemmatizationAnnotate(text = sent)\n",
    "                pos_annotate = isp_api.posTaggingAnnotate(text = sent)\n",
    "                \n",
    "            except KeyboardInterrupt:\n",
    "                print(\"Understandable, have a nice day\")\n",
    "                vocab.build_xml()\n",
    "                lemmatized_txt.close()\n",
    "                sys.exit()\n",
    "                \n",
    "            except:\n",
    "                print(\"Sentence\", end = \" \\\"\")\n",
    "                print(sent, end = \"\\\" \")\n",
    "                print(\"in commentary #\", end = \"\")\n",
    "                print(counter, end = \" \")\n",
    "                print(\"not processed\")\n",
    "                print(\"\\n\")\n",
    "                continue\n",
    "                \n",
    "            if lemma_annotate.get('annotations').get('lemma') is not None:\n",
    "                lemmatized = []\n",
    "                for i in range(len(lemma_annotate.get('annotations').get('lemma'))):\n",
    "                    l = lemma_annotate.get('annotations').get('lemma')[i].get('value')\n",
    "                    p = pos_annotate['annotations']['pos-token'][i]['value'].get('tag')\n",
    "                    word_id = vocab.add_item(toks[i], l, p, item_origin)\n",
    "                    lemmatized.append(str(word_id))\n",
    "                \n",
    "                lemmatized = ' '.join(lemmatized)\n",
    "                lemmatized_txt.write(lemmatized + '\\n')\n",
    "        \n",
    "        counter = counter + 1\n",
    "        if(tm.clock() - checkpoint > dt):\n",
    "            print(\"Progress: \", end = '')\n",
    "            print(counter / num_comments * 100, end = '')\n",
    "            print(\"%\")\n",
    "            print(\"Comments processed by turn: \", end= '')\n",
    "            print(counter - prev_counter)\n",
    "            print(\"Comments processed total: \", end= '')\n",
    "            print(counter)\n",
    "            print(\"Time left: \", end = '')\n",
    "            speed = (counter - prev_counter) / dt\n",
    "            print(speed * (num_comments - counter))\n",
    "            print(\"Speed: \", end = '')\n",
    "            print(speed, end = ' ')\n",
    "            print(\"comments/sec\")\n",
    "            print(\"\\n\")\n",
    "            prev_counter = counter\n",
    "            vocab.build_xml()\n",
    "            if (counter - anchor_counter) / num_comments * 100 > 25:\n",
    "                print(\"Sleeping for 20 minutes\")\n",
    "                tm.sleep(1200)\n",
    "                anchor_counter = counter\n",
    "            checkpoint = tm.clock()\n",
    "            \n",
    "    vocab.build_xml()\n",
    "    print(\"Finished\")\n",
    "    \n",
    "lemmatized_txt.close()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2vec модель из gensim\n",
    "Ищу синонимы с помощью метода most_similar() и подставляю в изначальный текст"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab finished\n"
     ]
    }
   ],
   "source": [
    "new_vocab = FullVocab(from_file = True)\n",
    "print('Vocab finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building models completed\n"
     ]
    }
   ],
   "source": [
    "w2v_model = gensim.models.Word2Vec(gensim.models.word2vec.LineSentence('lemmatized.txt'), min_count=10, sg = 1)\n",
    "#w2v_model1 = gensim.models.Word2Vec(gensim.models.word2vec.LineSentence('lemmatized.txt'), min_count=10)\n",
    "print('Building models completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Вы думаете, что в смартфонах уже давно не появляется никаких новых технологий?\n",
      "Форумчане ноете , что в смартах уже давненько не запилит никиёв новых технологий ?\n",
      "\n",
      "\n",
      "Как бы не так!\n",
      "Радужно бы не вправду !\n",
      "\n",
      "\n",
      "Мы на пороге существенных изменений как конструкции, так и внешнего вида гаджетов.\n",
      "Живившие на пороге существенных изменений как конструкции , вправду и внешнего вида гаджетов .\n",
      "\n",
      "\n",
      "Связано это с дисплеями, которые во флагманах уже поглотили почти всю переднюю поверхность.\n",
      "Связано это с дисплеями , бб во безрамочниках уже поглотили почти всю переднюю поверхность .\n",
      "\n",
      "\n",
      "В такие переломные моменты производители ещё не знают, какая из технологий сработает, и наряду с интересными решениями порой предлагают и сомнительные.\n",
      "В голимые переломные моменты брэнды ещё не регати , какая из технологий сработает , и наряду с зачётными решениями нугой предлагают и сомнительные .\n",
      "\n",
      "\n",
      "Действительно ли полезны нововведения или на нас тестируют новые функции и технологии?\n",
      "Сомнительно ли полезны нововведения или на жививших допиливают новые функции и технологии ?\n",
      "\n",
      "\n",
      "Лучше всего эволюцию экранов в мобильных устройствах демонстрирует простая закономерность: с каждым годом увеличивается соотношение площади, занимаемой дисплеем, и площади всей фронтальной панели гаджетов.\n",
      "Качественнее всего эволюцию экранов в мобильных устройствах демонстрирует простая закономерность : с каждым годиком увеличивается соотношение площади , занимаемой дисплеем , и площади всей фронтальной панели гаджетов .\n",
      "\n",
      "\n",
      "Ещё в прошлом году среднее соотношение среди флагманов составляло около 70%.\n",
      "Ещё в прошлом годике среднее соотношение среди безрамочников составляло около 70% .\n",
      "\n",
      "\n",
      "Теперь же большинство представленных на выставках новинок имеет соотношение около 80%.\n",
      "Теперь же большинство представленных на выставках новинок имеет соотношение около 80% .\n",
      "\n",
      "\n",
      "У того же LG G6 экран занимает 78% площади фронтальной панели, а у Samsung Galaxy S8 Plus этот показатель и вовсе 84%.\n",
      "У дебила же g2 G6 экран занимает 78% площади фронтальной панели , а у Samsung Galaxy S8 Plus никчёмный показатель и вовсе 84% .\n",
      "\n",
      "\n",
      "Когда больше нет возможности увеличить дисплеи, производители предлагают новые решения в этой области, иногда интересные, а иногда — странные.\n",
      "Когда больше неесть кругозора увеличить дисплеи , брэнды предлагают новые решения в никчёмной области , иногда зачётные , а иногда — странные .\n",
      "\n",
      "\n",
      "Попробуем разобраться, какие из дисплейных инноваций действительно полезны и какие экраны мы увидим в ближайшем будущем.\n",
      "Щупаем вбивать , какие из дисплейных иноваций сомнительно полезны и какие экраны живившие небудем в ближайшем иновации .\n",
      "\n",
      "\n",
      "Всё идёт к тому, что в скором времени дисплеи станут по-настоящему безрамочными и будут занимать всю поверхность смартфонов — по крайней мере, фронтальную.\n",
      "Пень идёт к дебилу , что в скором времени дисплеи станут по-настоящему безрамочными и будут занимать всю поверхность смартов — по крайней барабану , фронтальную .\n",
      "\n",
      "\n",
      "Воображение уже рисует потрясающие картины девайсов ближайшего будущего, но, увы, всё не так радужно.\n",
      "Воображение уже рисует потрясающие картины брэндов ближайшего иновации , но , нич , пень не вправду радужно .\n",
      "\n",
      "\n",
      "Компаниям приходится идти на различные ухищрения ради увеличения занимаемой дисплеем площади.\n",
      "Компаниям переставляет идти на различные ухищрения ради увеличения занимаемой дисплеем площади .\n",
      "\n",
      "\n",
      "Та же Samsung отказалась от логотипа на фронтальной панели и занялась оптимизацией свободного места на корпусе смартфона.\n",
      "Упоротая же Samsung отказалась от логотипа на фронтальной панели и променяла нвидиа свободного лайтрума на корпусе смарта .\n",
      "\n",
      "\n",
      "Первыми под нож пошли физические клавиши.\n",
      "Первыми под нож скатывались физические клавиши .\n",
      "\n",
      "\n",
      "Подавляющее большинство производителей уже давно перестали их использовать, сделав кнопки виртуальными.\n",
      "Подавляющее большинство брэндов уже давненько перестали европейцев использовать , запатентовывав кнопки виртуальными .\n",
      "\n",
      "\n",
      "Так поступили даже в Samsung, компании, долгое время остававшейся приверженцем механической кнопки «Домой».\n",
      "Вправду поступили даже в Samsung , компании , долгое время остававшейся приверженцем механической кнопки « Домой » .\n",
      "\n",
      "\n",
      "Правда, тактильный отклик у теперь уже сенсорной кнопки остался — эта фишка была впервые представлена Apple под названием 3D Touch.\n",
      "Но , тактильный отклик у теперь уже сенсорной кнопки остался — никчёмная фишка была впервые представлена майкрософт под автогаджетом 3D Touch .\n",
      "\n",
      "\n",
      "Свои аналоги данной технологии используют некоторые китайские компании, так что в будущем нам стоит ожидать большее распространение сенсорных кнопок с обратной связью.\n",
      "Свои расходники данной технологии используют некоторые испанские компании , вправду скок в иновации живившим стоит ожидать большее распространение сенсорных кнопок с обратной опсосом .\n",
      "\n",
      "\n",
      "Другое, несколько неожиданное решение по дальнейшей оптимизации свободного пространства предложили сразу несколько производителей смартфонов — отказаться от традиционного разговорного динамика.\n",
      "Другое , несколько неожиданное решение по дальнейшей нвидиа свободного пространства предложили сразу несколько брэндов смартов — отказаться от традиционного разговорного динамика .\n",
      "\n",
      "\n",
      "Звучит странно, но технологии, позволяющие это сделать, существуют уже давно.\n",
      "Моторет лолы , но технологии , позволяющие это запатентовывать , вымирают уже давненько .\n",
      "\n",
      "\n",
      "Речь идёт о костной проводимости звука: если в обычных динамиках звуковая волна формируется мембраной и затем поступает в ухо, то в динамиках костной проводимости вибрация передаётся напрямую в височные кости.\n",
      "Целесообразность идёт о костной проводимости звука : если в обычных олеофобках звуковая волна формируется мембраной и затем поступает в пауэрбанк , то в олеофобках костной проводимости вибрация передаётся напрямую в височные кости .\n",
      "\n",
      "\n",
      "Кому-то это покажется небезопасным, но на деле наушники с костной проводимостью активно используются людьми с осложнениями слуха, а также спортсменами, поскольку оставляют уши открытыми и не мешают слышать, что происходит вокруг.\n",
      "Кому-то это покажется небезопасным , но на деле блютузы с костной проводимостью активно используются юзерами с осложнениями слуха , а также спортсменами , поскольку загоняют пауэрбанки открытыми и не убедятся регать , скок тупеет вокруг .\n",
      "\n",
      "\n",
      "У динамика с костной проводимостью есть одно существенное преимущество — вашего собеседника не услышит никто посторонний, даже если он находится очень близко от вас.\n",
      "У динамика с костной проводимостью есть одно существенное преимущество — вашего собеседника не регать никто посторонний , даже если объективность находится ооочень близко от форумчан .\n",
      "\n",
      "\n",
      "Эта технология уже использовалась в Xiaomi Mi Mix и, по слухам, именно такой динамик получит Essential Phone от создателя Android Энди Рубина и Apple iPhone 8.\n",
      "Никчёмная технология уже использовалась в автогаджет Mi Mix и , по слухам , именно голимой олеофобок получит Essential Phone от создателя Android Энди Рубина и Apple sgs 8 .\n",
      "\n",
      "\n",
      "Нерешённым остаётся вопрос с датчиками приближения и освещённости, которые стали неизменным атрибутом любого современного смартфона.\n",
      "Нерешённым остаётся вопрос с датчиками приближения и освещённости , бб добивали неизменным атрибутом любого специфичного смарта .\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def chiki_briki(filename):\n",
    "    \n",
    "    target_article_txt = open(filename, 'r', encoding='utf-8')\n",
    "    target_article = target_article_txt.read()\n",
    "    ta_sents = nltk.sent_tokenize(target_article)\n",
    "    new_article = ''\n",
    "    for sent in ta_sents:\n",
    "        la = isp_api.lemmatizationAnnotate(text = sent)\n",
    "        lp = isp_api.posTaggingAnnotate(text = sent)\n",
    "        \n",
    "        comparer = []\n",
    "        new_sent = []       \n",
    "        for i in range(len(la['annotations']['lemma'])):\n",
    "            l = la['annotations']['lemma'][i]\n",
    "            p = lp['annotations']['pos-token'][i]\n",
    "            word_id = str(new_vocab.id_by_word(l['value'], tag = p['value']['tag']))\n",
    "            comparer.append({'text': l['text'], 'norma': l['value'], 'tag': p['value']['tag'], 'word_id': word_id})\n",
    "        \n",
    "        for i, c in enumerate(comparer):\n",
    "            if c['tag'] in ['PR', 'PART', 'CONJ'] or not c['text'].isalnum():\n",
    "                new_sent.append(c['text'])\n",
    "                continue\n",
    "                \n",
    "            word_id = new_vocab.id_by_word(c['norma'], tag = c['tag'])\n",
    "            word_item = new_vocab.word_by_id(word_id, return_val = 'item')\n",
    "            word_norm = new_vocab.word_by_id(word_id)\n",
    "            if word_id is None or int(word_item['cnt']) < 10:\n",
    "                new_sent.append(c['text'])\n",
    "                continue\n",
    "                \n",
    "            #print(word_item['normal_form'] + ' - ' + word_item['origin'])\n",
    "            ''' \n",
    "            if word_item['origin'] is not 'xax':\n",
    "                new_sent.append(c['text'])\n",
    "                continue\n",
    "            '''\n",
    "            #new_sent.append(c['norma'])\n",
    "            synonyms = w2v_model.most_similar_cosmul(positive = [str(word_id)], topn = 11)\n",
    "            '''\n",
    "            print(word_item['normal_form'])\n",
    "            for s in synonyms:\n",
    "                print(new_vocab.word_by_id(int(s[0])), end = \" - \")\n",
    "                print(s[1])\n",
    "            '''\n",
    "            \n",
    "            suitable_syns = [s for s in synonyms if new_vocab.word_by_id(int(s[0]), return_val = 'item')['origin'] in\n",
    "                           ['txx'] and \n",
    "                           new_vocab.word_by_id(int(s[0]), return_val = 'item')['tags'] == c['tag'] and \n",
    "                           not (new_vocab.word_by_id(int(s[0])) == 'не' + word_norm or\n",
    "                           'не' + new_vocab.word_by_id(int(s[0])) == word_norm or\n",
    "                           new_vocab.word_by_id(int(s[0])) == 'без' + word_norm or\n",
    "                           'без' + new_vocab.word_by_id(int(s[0])) == word_norm)]\n",
    "            lemma = [w['word_id'] for w in comparer]\n",
    "            '''\n",
    "            ranked_syns = []\n",
    "            for s in suitable_syns:\n",
    "                lemma[i] = s[0]\n",
    "                rank = (w2v_model.score([lemma])) / 100 \n",
    "                ranked_syns.append({'syn': s[0], 'rank': rank[0]})\n",
    "            \n",
    "            ranked_syns = sorted(ranked_syns, key=lambda s: s['rank'], reverse=True)\n",
    "            '''\n",
    "            \n",
    "            #print(c['norma'])\n",
    "            #print(suitable_syns)\n",
    "            #print(ranked_syns)\n",
    "            \n",
    "            if len(suitable_syns) == 0:  \n",
    "                new_sent.append(c['text'])\n",
    "                continue\n",
    "            \n",
    "            \n",
    "            #Принимаем решение, пропускаем ли синоним\n",
    "            def make_decision(suitable_syns, word_item, cnt):\n",
    "                decision = False\n",
    "                freq = int(word_item['cnt']) / cnt\n",
    "                first_proximity = 0.79\n",
    "                second_proximity = 0.75\n",
    "                allowed_freq_min = 100 / cnt\n",
    "                allowed_freq_max = 20000 / cnt\n",
    "                if freq < allowed_freq_max and freq > allowed_freq_min and suitable_syns[0][1] > first_proximity:\n",
    "                    decision = True\n",
    "                \n",
    "                return decision\n",
    "                \n",
    "            \n",
    "            if  make_decision(suitable_syns, word_item, new_vocab.id_counter):\n",
    "                synonym = suitable_syns[0][0]\n",
    "                synonym_item = new_vocab.word_by_id(int(suitable_syns[0][0]), return_val = 'item')\n",
    "                \n",
    "                \n",
    "                #Получаю наилучший вариант из pymorphy2.parse\n",
    "                def get_propper_parse(word, word_forms):\n",
    "                    word_parse = morph.parse(word)\n",
    "                    ranks = [0] * len(word_parse)\n",
    "                    for i, p in enumerate(word_parse):\n",
    "                        lexemas = [w.word for w in p.lexeme]\n",
    "                        for w in word_forms:\n",
    "                            if w in lexemas:\n",
    "                                ranks[i] += 1\n",
    "                                \n",
    "                    max_index = ranks.index(max(ranks))\n",
    "                    return word_parse[max_index]\n",
    "                \n",
    "                def inflect_by_template(word, template):\n",
    "                    grammemes = sorted(template.tag.grammemes)\n",
    "                    for g in grammemes:\n",
    "                        w = word.inflect({g})\n",
    "                        if w is not None:\n",
    "                            word = w\n",
    "\n",
    "                    return word\n",
    "                \n",
    "                \n",
    "                synonym_parse = get_propper_parse(synonym_item['normal_form'], synonym_item['word_forms'])\n",
    "                word_parse = get_propper_parse(c['text'], word_item['word_forms'])\n",
    "                synonym_parse = inflect_by_template(synonym_parse, word_parse)\n",
    "                \n",
    "                word_to_substitute = synonym_parse.word\n",
    "                #Первое слово в предожении с заглавной буквы\n",
    "                if i == 0:\n",
    "                    word_to_substitute = word_to_substitute.title()\n",
    "                    \n",
    "                new_sent.append(word_to_substitute)\n",
    "            #elif suitable_syns[0][1] > second_proximity: \n",
    "                #optionnal_syn = '(' + new_vocab.word_by_id(int(ranked_syns[0]['syn'])) + ')'\n",
    "                #new_sent.append(optionnal_syn)\n",
    "            else:  \n",
    "                new_sent.append(c['text'])\n",
    "                continue\n",
    "                \n",
    "        \n",
    "        print(sent)\n",
    "        print(' '.join(new_sent))\n",
    "        print('\\n')\n",
    "            \n",
    "        '''\n",
    "            t_id = new_vocab.id_by_word(t)\n",
    "            \n",
    "            \n",
    "            if t_id is not None and int(new_vocab.word_by_id(t_id, return_val = 'item')['cnt']) > 10:\n",
    "                lst = w2v_model.most_similar(positive = [str(t_id)])\n",
    "                new_t = new_vocab.word_by_id(int(lst[0][0]), return_val = 'item')\n",
    "                if new_t['origin'] == 'tax' or new_t['origin'] == 'txx':\n",
    "                    print(new_t['normal_form'], end = ' ')\n",
    "                else:\n",
    "                    print(t, end = ' ')\n",
    "        \n",
    "            else:\n",
    "                print(t, end = ' ')\n",
    "        '''\n",
    "        \n",
    "        \n",
    "chiki_briki('target_article.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some testing\n",
    "Everythin' below is my lazy draft, don't borther yourself lookig through"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s8 - 0.7711077332496643\n",
      "active - 0.7300979495048523\n",
      "s6 - 0.7240570187568665\n",
      "s3 - 0.7103427052497864\n",
      "s9 - 0.7008739709854126\n",
      "s7 - 0.6820350289344788\n",
      "samsung - 0.677720844745636\n",
      "galaxy - 0.6628284454345703\n",
      "c - 0.6439862251281738\n",
      "флагманский - 0.6419556736946106\n"
     ]
    }
   ],
   "source": [
    "lst = w2v_model.most_similar(positive = [str(new_vocab.id_by_word('samsung'))])\n",
    "for l in lst:\n",
    "    print(new_vocab.word_by_id(int(l[0])), end = \" - \")\n",
    "    print(l[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2034\n",
      "2365\n",
      "0.915373687744\n",
      "[('2151', 0.8232387900352478), ('2367', 0.7768270969390869), ('2380', 0.7505201101303101), ('1583', 0.7477496862411499), ('1344', 0.7444450855255127), ('26640', 0.7400096654891968), ('453', 0.7385299801826477), ('2021', 0.7321900129318237), ('3844', 0.7259445190429688), ('7477', 0.724348783493042)]\n",
      "87557\n",
      "0.011421131377274232\n",
      "0.11421131377274232\n"
     ]
    }
   ],
   "source": [
    "print(new_vocab.id_by_word('samsung'))\n",
    "print(new_vocab.id_by_word('galaxy'))\n",
    "print(1 + w2v_model.score([[str(new_vocab.id_by_word('samsung')), str(new_vocab.id_by_word('mass'))]])[0] / 100)\n",
    "print(w2v_model.most_similar_cosmul(positive = ['256']))\n",
    "\n",
    "print(new_vocab.id_counter)\n",
    "allowed_freq_min = 1000 / new_vocab.id_counter\n",
    "allowed_freq_max = 10000 / new_vocab.id_counter\n",
    "print(allowed_freq_min)\n",
    "print(allowed_freq_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "comments = ''\n",
    "with open('learn_scrapy\\pda.jl', 'rb') as f:\n",
    "    for item in jl.reader(f):\n",
    "        comments = comments + item['comment']       #создаём один string на все комменты\n",
    "                                                    #скорее всего, будет логичнее для N-граммов каждое предложение\n",
    "                                                    #запоминать в один string и хранить как list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kek1\n",
      "Kek2\n",
      "Kek3\n",
      "Kek4\n"
     ]
    }
   ],
   "source": [
    "#тут токенизируем комментарии\n",
    "#есть подозрение, что я создаю слишком много сущностей \n",
    "\n",
    "tknzr = nltk.TweetTokenizer()\n",
    "print('Kek1')\n",
    "tokens = tknzr.tokenize(comments)\n",
    "print('Kek2')\n",
    "text = nltk.Text(tokens)\n",
    "print('Kek3')\n",
    "sents = nltk.sent_tokenize(comments)                         #строить n-граммную модель, возможно, буду \n",
    "print('Kek4')                                                             #анализируя предложения по-отдельности\n",
    "words = sorted([w.lower() for w in tokens if w.isalpha()])   #впоследствии буду отсекать наименее наиболее частотные слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10600020178435018\n"
     ]
    }
   ],
   "source": [
    "def lexical_diversity(text):\n",
    "    return len(set(text)) / len(text)\n",
    "\n",
    "print(lexical_diversity(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sent_tokens = list()                            #здесь храним предложения\n",
    "for i in range(len(sents)):\n",
    "    sent_tokens.append(tknzr.tokenize(sents[i]))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#сох нормальные формы слов\n",
    "morph = pm2.MorphAnalyzer()     \n",
    "vocab_normalized = sorted(set([morph.parse(w)[0].normalized for w in words]), key = lambda w: w.normal_form)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def progress(full, curr, checkpoint):\n",
    "    if curr / full * 100 > checkpoint:\n",
    "        print(curr / full * 100, end = '')\n",
    "        print('% complited')\n",
    "        return True\n",
    "    \n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "i = 0\n",
    "h = 0.1\n",
    "checkpoint = h\n",
    "time = tm.clock()\n",
    "for sent in sents:\n",
    "    if progress(len(sents), i, checkpoint):\n",
    "        time_per_checkpoint = tm.clock() - time\n",
    "        print('Time per checkpoint:', end = ' ')\n",
    "        print(time_per_checkpoint)\n",
    "        print('Time per sentence:', end = ' ')\n",
    "        time_per_sent = time_per_checkpoint / (len(sents) * h)\n",
    "        print(time_per_sent)\n",
    "        print('Estimated time:', end = ' ')\n",
    "        print(time_per_checkpoint * (100 - checkpoint))\n",
    "        time = tm.clock()\n",
    "        checkpoint = checkpoint + h\n",
    "        \n",
    "    toks = tknzr.tokenize(sent)\n",
    "    toks = [t.lower() for t in toks if t.isalnum()]\n",
    "    sentence = ''\n",
    "    for t in toks:\n",
    "        sentence = sentence + t + ' '\n",
    "    \n",
    "    lemma_raw = isp_api.lemmatizationAnnotate(text = sentence)\n",
    "    lemmatized = ''\n",
    "    if lemma_raw.get('annotations').get('lemma') is not None:\n",
    "        for l in lemma_raw.get('annotations').get('lemma'):\n",
    "            lemmatized = lemmatized + l.get('value') + ' '\n",
    "        \n",
    "        sents[i] = lemmatized\n",
    "    \n",
    "    else:\n",
    "        sents[i] = ''\n",
    "        \n",
    "    i = i + 1\n",
    "        \n",
    "    \n",
    "sents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sents' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-e8335ea762b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'lemmatized.txt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf8'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msents\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m5000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sents' is not defined"
     ]
    }
   ],
   "source": [
    "with open('lemmatized.txt', 'w', encoding='utf8') as f:\n",
    "    for s in sents[0:5000]:\n",
    "        f.write(s + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec(gensim.models.word2vec.LineSentence('lemmatized.txt'), min_count=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('а', 0.9998720288276672),\n",
       " ('они', 0.9998717904090881),\n",
       " ('где', 0.9998696446418762),\n",
       " ('этот', 0.9998689293861389),\n",
       " ('мочь', 0.9998672604560852),\n",
       " ('когда', 0.999864935874939),\n",
       " ('это', 0.9998648166656494),\n",
       " ('там', 0.9998632669448853),\n",
       " ('с', 0.9998587965965271),\n",
       " ('ничто', 0.9998581409454346)]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive = ['айфон'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "сейчас\n",
      "{'value': {'characters': [], 'tag': 'ADV', 'type': 'syn-tag-rus'}, 'start': 0, 'text': 'сейчас', 'end': 6, 'annotated-text': 'сейчас я говорю о том человеке который спит отдыхая на работе'}\n",
      "[Parse(word='сейчас', tag=OpencorporaTag('ADVB'), normal_form='сейчас', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'сейчас', 3, 0),))]\n",
      "\n",
      "\n",
      "я\n",
      "{'value': {'characters': [{'tag': 'Masculine', 'type': 'gender'}, {'tag': 'DEICTIC', 'type': 'pronoun'}, {'tag': 'Singular', 'type': 'number'}, {'tag': 'Nominative', 'type': 'case'}, {'tag': 'Animated', 'type': 'animacy'}], 'tag': 'S', 'type': 'syn-tag-rus'}, 'start': 7, 'text': 'я', 'end': 8, 'annotated-text': 'сейчас я говорю о том человеке который спит отдыхая на работе'}\n",
      "[Parse(word='я', tag=OpencorporaTag('NPRO,1per sing,nomn'), normal_form='я', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'я', 3100, 0),))]\n",
      "\n",
      "\n",
      "говорить\n",
      "{'value': {'characters': [{'tag': 'First', 'type': 'person'}, {'tag': 'Singular', 'type': 'number'}, {'tag': 'NotPast', 'type': 'tense'}, {'tag': 'Indicative', 'type': 'mode'}], 'tag': 'V', 'type': 'syn-tag-rus'}, 'start': 9, 'text': 'говорю', 'end': 15, 'annotated-text': 'сейчас я говорю о том человеке который спит отдыхая на работе'}\n",
      "[Parse(word='говорить', tag=OpencorporaTag('INFN,impf,tran'), normal_form='говорить', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'говорить', 395, 0),))]\n",
      "\n",
      "\n",
      "о\n",
      "{'value': {'characters': [], 'tag': 'PR', 'type': 'syn-tag-rus'}, 'start': 16, 'text': 'о', 'end': 17, 'annotated-text': 'сейчас я говорю о том человеке который спит отдыхая на работе'}\n",
      "[Parse(word='о', tag=OpencorporaTag('PREP'), normal_form='о', score=0.990985, methods_stack=((<DictionaryAnalyzer>, 'о', 2133, 0),)), Parse(word='о', tag=OpencorporaTag('INTJ'), normal_form='о', score=0.009014, methods_stack=((<DictionaryAnalyzer>, 'о', 21, 0),))]\n",
      "\n",
      "\n",
      "то\n",
      "{'value': {'characters': [{'tag': 'Neuter', 'type': 'gender'}, {'tag': 'Prepositional', 'type': 'case'}, {'tag': 'Singular', 'type': 'number'}, {'tag': 'Inanimated', 'type': 'animacy'}], 'tag': 'S', 'type': 'syn-tag-rus'}, 'start': 18, 'text': 'том', 'end': 21, 'annotated-text': 'сейчас я говорю о том человеке который спит отдыхая на работе'}\n",
      "[Parse(word='то', tag=OpencorporaTag('CONJ'), normal_form='то', score=0.5, methods_stack=((<DictionaryAnalyzer>, 'то', 20, 0),)), Parse(word='то', tag=OpencorporaTag('PRCL'), normal_form='то', score=0.1, methods_stack=((<DictionaryAnalyzer>, 'то', 22, 0),))]\n",
      "\n",
      "\n",
      "человек\n",
      "{'value': {'characters': [{'tag': 'Masculine', 'type': 'gender'}, {'tag': 'Prepositional', 'type': 'case'}, {'tag': 'Singular', 'type': 'number'}, {'tag': 'Animated', 'type': 'animacy'}], 'tag': 'S', 'type': 'syn-tag-rus'}, 'start': 22, 'text': 'человеке', 'end': 30, 'annotated-text': 'сейчас я говорю о том человеке который спит отдыхая на работе'}\n",
      "[Parse(word='человек', tag=OpencorporaTag('NOUN,anim,masc sing,nomn'), normal_form='человек', score=0.549382, methods_stack=((<DictionaryAnalyzer>, 'человек', 488, 0),)), Parse(word='человек', tag=OpencorporaTag('NOUN,anim,masc sing,nomn'), normal_form='человек', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'человек', 488, 0),))]\n",
      "\n",
      "\n",
      "который\n",
      "{'value': {'characters': [{'tag': 'Masculine', 'type': 'gender'}, {'tag': 'Nominative', 'type': 'case'}, {'tag': 'Singular', 'type': 'number'}], 'tag': 'S', 'type': 'syn-tag-rus'}, 'start': 31, 'text': 'который', 'end': 38, 'annotated-text': 'сейчас я говорю о том человеке который спит отдыхая на работе'}\n",
      "[Parse(word='который', tag=OpencorporaTag('ADJF,Apro,Subx,Anph masc,sing,nomn'), normal_form='который', score=0.823529, methods_stack=((<DictionaryAnalyzer>, 'который', 1788, 0),)), Parse(word='который', tag=OpencorporaTag('ADJF,Apro,Subx,Anph masc,sing,nomn'), normal_form='который', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'который', 1788, 0),))]\n",
      "\n",
      "\n",
      "спать\n",
      "{'value': {'characters': [{'tag': 'Third', 'type': 'person'}, {'tag': 'Singular', 'type': 'number'}, {'tag': 'NotPast', 'type': 'tense'}, {'tag': 'Indicative', 'type': 'mode'}], 'tag': 'V', 'type': 'syn-tag-rus'}, 'start': 39, 'text': 'спит', 'end': 43, 'annotated-text': 'сейчас я говорю о том человеке который спит отдыхая на работе'}\n",
      "[Parse(word='спать', tag=OpencorporaTag('INFN,impf,intr'), normal_form='спать', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'спать', 2808, 0),))]\n",
      "\n",
      "\n",
      "отдыхать\n",
      "{'value': {'characters': [{'tag': 'Gerund', 'type': 'representation'}, {'tag': 'NotPast', 'type': 'tense'}], 'tag': 'V', 'type': 'syn-tag-rus'}, 'start': 44, 'text': 'отдыхая', 'end': 51, 'annotated-text': 'сейчас я говорю о том человеке который спит отдыхая на работе'}\n",
      "[Parse(word='отдыхать', tag=OpencorporaTag('INFN,impf,intr'), normal_form='отдыхать', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'отдыхать', 2247, 0),))]\n",
      "\n",
      "\n",
      "на\n",
      "{'value': {'characters': [], 'tag': 'PR', 'type': 'syn-tag-rus'}, 'start': 52, 'text': 'на', 'end': 54, 'annotated-text': 'сейчас я говорю о том человеке который спит отдыхая на работе'}\n",
      "[Parse(word='на', tag=OpencorporaTag('PREP'), normal_form='на', score=0.99931, methods_stack=((<DictionaryAnalyzer>, 'на', 24, 0),)), Parse(word='на', tag=OpencorporaTag('PRCL'), normal_form='на', score=0.000477, methods_stack=((<DictionaryAnalyzer>, 'на', 22, 0),)), Parse(word='на', tag=OpencorporaTag('INTJ'), normal_form='на', score=0.000212, methods_stack=((<DictionaryAnalyzer>, 'на', 21, 0),))]\n",
      "\n",
      "\n",
      "работа\n",
      "{'value': {'characters': [{'tag': 'Feminine', 'type': 'gender'}, {'tag': 'Prepositional', 'type': 'case'}, {'tag': 'Singular', 'type': 'number'}, {'tag': 'Inanimated', 'type': 'animacy'}], 'tag': 'S', 'type': 'syn-tag-rus'}, 'start': 55, 'text': 'работе', 'end': 61, 'annotated-text': 'сейчас я говорю о том человеке который спит отдыхая на работе'}\n",
      "[Parse(word='работа', tag=OpencorporaTag('NOUN,inan,femn sing,nomn'), normal_form='работа', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'работа', 55, 0),))]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i = 5\n",
    "\n",
    "message = 'сейчас я говорю о том человеке который спит отдыхая на работе'\n",
    "message_tokens = tokenizer.tokenize(message)\n",
    "parse_tokens = [morph.parse(t) for t in message_tokens]\n",
    "#print(parse_tokens[i])\n",
    "isp_POS = isp_api.posTaggingAnnotate(text = message)\n",
    "isp_lemma = isp_api.lemmatizationAnnotate(text = message)\n",
    "\n",
    "#print(isp_POS['annotations']['pos-token'][i])\n",
    "\n",
    "#print(isp_lemma.get('annotations').get('lemma')[i].get('value'))\n",
    "\n",
    "#print(' '.join([l.get('value') for l in isp_lemma.get('annotations').get('lemma')]))\n",
    "\n",
    "for j in range(len(message_tokens)):\n",
    "    isp_lemma_token = isp_lemma.get('annotations').get('lemma')[j].get('value')\n",
    "    morph_parse_token = [w.normalized for w in morph.parse(isp_lemma_token) if w.normal_form == w.word]\n",
    "    print(isp_lemma_token)\n",
    "    print(isp_POS['annotations']['pos-token'][j])\n",
    "    print(morph_parse_token)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "встать\n",
      "{'text': 'встать', 'annotations': {'lemma': [{'text': 'встать', 'value': 'вставать', 'annotated-text': 'встать', 'start': 0, 'end': 6}]}}\n",
      "[Parse(word='встать', tag=OpencorporaTag('INFN,perf,intr'), normal_form='встать', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'встать', 904, 0),))]\n",
      "\n",
      "\n",
      "вставать\n",
      "{'text': 'вставать', 'annotations': {'lemma': [{'text': 'вставать', 'value': 'вставать', 'annotated-text': 'вставать', 'start': 0, 'end': 8}]}}\n",
      "[Parse(word='вставать', tag=OpencorporaTag('INFN,impf,intr'), normal_form='вставать', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'вставать', 903, 0),))]\n",
      "\n",
      "\n",
      "встали\n",
      "{'text': 'встали', 'annotations': {'lemma': [{'text': 'встали', 'value': 'вставать', 'annotated-text': 'встали', 'start': 0, 'end': 6}]}}\n",
      "[Parse(word='встали', tag=OpencorporaTag('VERB,perf,intr plur,past,indc'), normal_form='встать', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'встали', 904, 4),))]\n",
      "\n",
      "\n",
      "вставали\n",
      "{'text': 'вставали', 'annotations': {'lemma': [{'text': 'вставали', 'value': 'вставать', 'annotated-text': 'вставали', 'start': 0, 'end': 8}]}}\n",
      "[Parse(word='вставали', tag=OpencorporaTag('VERB,impf,intr plur,past,indc'), normal_form='вставать', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'вставали', 903, 10),))]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ws = ['встать', 'вставать', 'встали', 'вставали']\n",
    "for w in ws:\n",
    "    print(w)\n",
    "    ns = morph.parse(w)\n",
    "    ts = isp_api.lemmatizationAnnotate(text = w)\n",
    "    print(ts)\n",
    "    print(ns)\n",
    "        \n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

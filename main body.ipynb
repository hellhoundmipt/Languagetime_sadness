{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json_lines as jl\n",
    "import nltk\n",
    "import xmltodict\n",
    "from ispras import texterra as isp\n",
    "import pymorphy2 as pm2\n",
    "import gensim\n",
    "import time as tm\n",
    "from lxml import etree\n",
    "import sys\n",
    "\n",
    "isp_api = isp.API('ba74236a7212a71054ae1408b30b1bdef771d35b')\n",
    "#nltk.download()\n",
    "\n",
    "tokenizer = nltk.TweetTokenizer()\n",
    "morph = pm2.MorphAnalyzer()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лемматизация с помощью Texterra\n",
    "\n",
    "Читаю комментарии и статьи из файла по очереди, разбиваю их на предложения, лемматизирую с помощью Texterra, составляя при этом словарь FullVocab. Записи в Fullvocab хранятся в следующем виде:\n",
    "\n",
    "__item__:\n",
    "- __id__: id нормальной формы\n",
    "- __normal_form__: нормальная форма\n",
    "- __tags__: часть речи\n",
    "- __word_forms__: список встретившихся словоформ\n",
    "- __cnt__: сколько раз встречалось\n",
    "- __origin__: в каком корпусе текстов слово встретилось (t - треннировочный [комментарии], a - целевой [статьи], e - дополнительный [например, вешний словарь])\n",
    "\n",
    "В lemmatized.txt записываются построчно лемматизированные предложения, но вместо слов в нём \"id нормальной формы\", чтобы для word2vec слова \"стать\" (<i>глагол</i>) и \"стать\" (<i>сущ</i>) не были одним и тем же словом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class FullVocab:\n",
    "    \n",
    "    # Инициализация, либо пустой класс, либо из файла vocab.xml\n",
    "    def __init__(self, from_file = False):\n",
    "        self.id_counter = 0\n",
    "        self.vocab = []\n",
    "        if from_file:\n",
    "            f = open('vocab.xml', 'rb')\n",
    "            root = etree.XML(f.read())\n",
    "            for item in root:\n",
    "                self.add_item(item[3].text.split(' '), item[1].text, item[2].text, item[5].text, cnt = item[4].text, \n",
    "                              word_id = item[0].text)\n",
    "            \n",
    "        \n",
    "    # Добавляем новое слово в словарь\n",
    "    def add_item(self, words, normal_form, tags_normal, origin, cnt = 0, word_id = -1):\n",
    "        \n",
    "        # Дабы было можно и str и list в аргументы добавлять\n",
    "        if type(words) is not list:\n",
    "            words = [words]\n",
    "            \n",
    "        # Проверяем, есть ли нормальная форма слова в словаре. Если есть, то обновляем словоформы, если нет, то \n",
    "        # добавляем новую запись в словарь, присвивая уникальный id\n",
    "        for item in self.vocab:\n",
    "            if item.get('normal_form') == normal_form and item.get('tags') == tags_normal:\n",
    "                item['word_forms'].extend(words)\n",
    "                item['word_forms'] = sorted(set(item['word_forms']))\n",
    "                item['cnt'] += 1\n",
    "                item['origin'] = self.handle_origin(origin, item['origin'])\n",
    "                return item['id']\n",
    "                \n",
    "        #Если такой записи ещё не было\n",
    "        new_item = {'normal_form': normal_form, 'tags': tags_normal, 'word_forms': words}\n",
    "        if normal_form not in new_item['word_forms']:\n",
    "            new_item['word_forms'].append(normal_form)\n",
    "        if int(cnt) > 0:\n",
    "            new_item['cnt'] = int(cnt)\n",
    "        else:\n",
    "            new_item['cnt'] = 1\n",
    "            \n",
    "        new_item['origin'] = self.handle_origin(origin)\n",
    "        \n",
    "        return_id = 0\n",
    "        if word_id == -1:         #если мы не уточняем, какой именно id у нового слова (когда просто добавляем новое слово)\n",
    "            new_item['id'] = self.id_counter\n",
    "            return_id = self.id_counter\n",
    "            self.id_counter = self.id_counter + 1\n",
    "        else:                     #если мы указали id для нового слова (когда читаем из файла сформированный словарь)\n",
    "            return_id = int(word_id)\n",
    "            new_item['id'] = return_id\n",
    "            self.id_counter = return_id + 1\n",
    "            \n",
    "        self.vocab.append(new_item)\n",
    "        return return_id\n",
    "         \n",
    "        \n",
    "    def show_vocab(self):\n",
    "        print(self.vocab)\n",
    "        \n",
    "        \n",
    "    # Сохраняем копию данные класса в vocab.xml\n",
    "    def build_xml(self):\n",
    "        f = open('vocab.xml', 'wb')\n",
    "        root = etree.Element('root')\n",
    "        for item in self.vocab:\n",
    "            child = etree.SubElement(root, 'w' + str(item['id']))\n",
    "            etree.SubElement(child, 'id').text = str(item['id'])\n",
    "            etree.SubElement(child, 'normal_form').text = item['normal_form']\n",
    "            etree.SubElement(child, 'tags').text = item['tags']\n",
    "            etree.SubElement(child, 'word_forms').text = ' '.join(item['word_forms'])\n",
    "            etree.SubElement(child, 'cnt').text = str(item['cnt'])\n",
    "            etree.SubElement(child, 'origin').text = item['origin']\n",
    "            \n",
    "        result = etree.tostring(root, encoding='unicode', method='xml', pretty_print=True)\n",
    "        f.write(result.encode('utf8'))\n",
    "        \n",
    "        \n",
    "    # Ищем нормальную форму или запись в словаре по id\n",
    "    # Если return_val = \"word\", то возвращаем нормальную форму, если \"item\", то запись в словаре\n",
    "    def word_by_id(self, word_id, return_val = \"word\"):\n",
    "        item = next((item for item in self.vocab if item[\"id\"] == word_id), None)\n",
    "        if item is None:\n",
    "            return None\n",
    "        elif return_val == 'word':\n",
    "            return item['normal_form']\n",
    "        elif return_val == 'item':\n",
    "            return item\n",
    "        \n",
    "        \n",
    "    def id_by_word(self, word, tag = None):\n",
    "        if tag == None:\n",
    "            item = next((item for item in self.vocab if item['normal_form'] == word), None)\n",
    "        else:\n",
    "            item = next((item for item in self.vocab if item['normal_form'] == word and item['tags'] == tag), None)\n",
    "        if item is None:\n",
    "            return None\n",
    "        else:\n",
    "            return item['id']\n",
    "    \n",
    "    \n",
    "    def handle_origin(self, new, old = 'xxx'):\n",
    "        if len(new) == 1:\n",
    "            if new == 't':\n",
    "                return 't' + old[1:3]\n",
    "            elif new == 'a':\n",
    "                return old[0] + 'a' + old[2]\n",
    "            elif new == 'e':\n",
    "                return  old[0:2] + 'e'\n",
    "            \n",
    "        elif len(new) == 3:\n",
    "            return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 0.8288857747696712%\n",
      "Comments processed by turn: 834\n",
      "Comments processed total: 834\n",
      "Time left: 92465.58\n",
      "Speed: 0.9266666666666666 comments/sec\n",
      "\n",
      "\n",
      "Progress: 1.8813918125167715%\n",
      "Comments processed by turn: 1059\n",
      "Comments processed total: 1893\n",
      "Time left: 116165.24\n",
      "Speed: 1.1766666666666667 comments/sec\n",
      "\n",
      "\n",
      "Progress: 2.996511523897552%\n",
      "Comments processed by turn: 1122\n",
      "Comments processed total: 3015\n",
      "Time left: 121677.15999999999\n",
      "Speed: 1.2466666666666666 comments/sec\n",
      "\n",
      "\n",
      "Progress: 3.99733643420098%\n",
      "Comments processed by turn: 1007\n",
      "Comments processed total: 4022\n",
      "Time left: 108079.07222222221\n",
      "Speed: 1.1188888888888888 comments/sec\n",
      "\n",
      "\n",
      "Progress: 5.0886033175308345%\n",
      "Comments processed by turn: 1098\n",
      "Comments processed total: 5120\n",
      "Time left: 116506.34\n",
      "Speed: 1.22 comments/sec\n",
      "\n",
      "\n",
      "Progress: 6.192790482721608%\n",
      "Comments processed by turn: 1111\n",
      "Comments processed total: 6231\n",
      "Time left: 116514.27333333333\n",
      "Speed: 1.2344444444444445 comments/sec\n",
      "\n",
      "\n",
      "Progress: 7.2313823707723355%\n",
      "Comments processed by turn: 1045\n",
      "Comments processed total: 7276\n",
      "Time left: 108379.27222222224\n",
      "Speed: 1.1611111111111112 comments/sec\n",
      "\n",
      "\n",
      "Progress: 8.30177802955763%\n",
      "Comments processed by turn: 1077\n",
      "Comments processed total: 8353\n",
      "Time left: 110409.25333333334\n",
      "Speed: 1.1966666666666668 comments/sec\n",
      "\n",
      "\n",
      "Progress: 9.3105538825447%\n",
      "Comments processed by turn: 1015\n",
      "Comments processed total: 9368\n",
      "Time left: 102908.59444444445\n",
      "Speed: 1.1277777777777778 comments/sec\n",
      "\n",
      "\n",
      "Progress: 10.21696134847988%\n",
      "Comments processed by turn: 912\n",
      "Comments processed total: 10280\n",
      "Time left: 91541.49333333335\n",
      "Speed: 1.0133333333333334 comments/sec\n",
      "\n",
      "\n",
      "Progress: 11.137282964111431%\n",
      "Comments processed by turn: 926\n",
      "Comments processed total: 11206\n",
      "Time left: 91993.98444444445\n",
      "Speed: 1.028888888888889 comments/sec\n",
      "\n",
      "\n",
      "Progress: 12.01089279147659%\n",
      "Comments processed by turn: 879\n",
      "Comments processed total: 12085\n",
      "Time left: 86466.25333333334\n",
      "Speed: 0.9766666666666667 comments/sec\n",
      "\n",
      "\n",
      "Progress: 13.00476062693183%\n",
      "Comments processed by turn: 1000\n",
      "Comments processed total: 13085\n",
      "Time left: 97257.77777777778\n",
      "Speed: 1.1111111111111112 comments/sec\n",
      "\n",
      "\n",
      "Progress: 13.912161960702466%\n",
      "Comments processed by turn: 913\n",
      "Comments processed total: 13998\n",
      "Time left: 87870.16333333333\n",
      "Speed: 1.0144444444444445 comments/sec\n",
      "\n",
      "\n",
      "Progress: 14.813600087460369%\n",
      "Comments processed by turn: 907\n",
      "Comments processed total: 14905\n",
      "Time left: 86378.6488888889\n",
      "Speed: 1.0077777777777779 comments/sec\n",
      "\n",
      "\n",
      "Progress: 15.726964628243737%\n",
      "Comments processed by turn: 919\n",
      "Comments processed total: 15824\n",
      "Time left: 86583.07444444444\n",
      "Speed: 1.021111111111111 comments/sec\n",
      "\n",
      "\n",
      "Progress: 16.433604659252413%\n",
      "Comments processed by turn: 711\n",
      "Comments processed total: 16535\n",
      "Time left: 66424.78\n",
      "Speed: 0.79 comments/sec\n",
      "\n",
      "\n",
      "Progress: 17.247582416490257%\n",
      "Comments processed by turn: 819\n",
      "Comments processed total: 17354\n",
      "Time left: 75769.33\n",
      "Speed: 0.91 comments/sec\n",
      "\n",
      "\n",
      "Progress: 18.262321476490058%\n",
      "Comments processed by turn: 1021\n",
      "Comments processed total: 18375\n",
      "Time left: 93298.98\n",
      "Speed: 1.1344444444444444 comments/sec\n",
      "\n",
      "\n",
      "Progress: 19.14885158571613%\n",
      "Comments processed by turn: 892\n",
      "Comments processed total: 19267\n",
      "Time left: 80626.88888888889\n",
      "Speed: 0.9911111111111112 comments/sec\n",
      "\n",
      "\n",
      "Progress: 20.102964707753163%\n",
      "Comments processed by turn: 960\n",
      "Comments processed total: 20227\n",
      "Time left: 85749.33333333333\n",
      "Speed: 1.0666666666666667 comments/sec\n",
      "\n",
      "\n",
      "Sentence \"小米科技\" in commentary #20474 not processed\n",
      "\n",
      "\n",
      "Progress: 21.108758957233867%\n",
      "Comments processed by turn: 1012\n",
      "Comments processed total: 21239\n",
      "Time left: 89256.1511111111\n",
      "Speed: 1.1244444444444444 comments/sec\n",
      "\n",
      "\n",
      "Progress: 22.080761700309093%\n",
      "Comments processed by turn: 978\n",
      "Comments processed total: 22217\n",
      "Time left: 85194.66666666667\n",
      "Speed: 1.0866666666666667 comments/sec\n",
      "\n",
      "\n",
      "Progress: 23.023942276156117%\n",
      "Comments processed by turn: 949\n",
      "Comments processed total: 23166\n",
      "Time left: 81667.77666666667\n",
      "Speed: 1.0544444444444445 comments/sec\n",
      "\n",
      "\n",
      "Progress: 24.011846904598627%\n",
      "Comments processed by turn: 994\n",
      "Comments processed total: 24160\n",
      "Time left: 84442.50888888889\n",
      "Speed: 1.1044444444444443 comments/sec\n",
      "\n",
      "\n",
      "Progress: 24.94409493425564%\n",
      "Comments processed by turn: 938\n",
      "Comments processed total: 25098\n",
      "Time left: 78707.58\n",
      "Speed: 1.0422222222222222 comments/sec\n",
      "\n",
      "\n",
      "Progress: 25.95585239074908%\n",
      "Comments processed by turn: 1018\n",
      "Comments processed total: 26116\n",
      "Time left: 84268.9088888889\n",
      "Speed: 1.1311111111111112 comments/sec\n",
      "\n",
      "\n",
      "Sleeping for 20 minutes\n",
      "Progress: 27.012333899837998%\n",
      "Comments processed by turn: 1063\n",
      "Comments processed total: 27179\n",
      "Time left: 86738.43777777777\n",
      "Speed: 1.181111111111111 comments/sec\n",
      "\n",
      "\n",
      "Progress: 27.89985787689953%\n",
      "Comments processed by turn: 893\n",
      "Comments processed total: 28072\n",
      "Time left: 71980.76111111112\n",
      "Speed: 0.9922222222222222 comments/sec\n",
      "\n",
      "\n",
      "Progress: 28.894719580190227%\n",
      "Comments processed by turn: 1001\n",
      "Comments processed total: 29073\n",
      "Time left: 79572.82666666666\n",
      "Speed: 1.1122222222222222 comments/sec\n",
      "\n",
      "\n",
      "Progress: 29.851814305733626%\n",
      "Comments processed by turn: 963\n",
      "Comments processed total: 30036\n",
      "Time left: 75521.67\n",
      "Speed: 1.07 comments/sec\n",
      "\n",
      "\n",
      "Progress: 30.857608555214327%\n",
      "Comments processed by turn: 1012\n",
      "Comments processed total: 31048\n",
      "Time left: 78226.47555555555\n",
      "Speed: 1.1244444444444444 comments/sec\n",
      "\n",
      "\n",
      "Progress: 31.858433465517756%\n",
      "Comments processed by turn: 1007\n",
      "Comments processed total: 32055\n",
      "Time left: 76713.26\n",
      "Speed: 1.1188888888888888 comments/sec\n",
      "\n",
      "\n",
      "Progress: 32.90696403192304%\n",
      "Comments processed by turn: 1055\n",
      "Comments processed total: 33110\n",
      "Time left: 79133.20555555556\n",
      "Speed: 1.1722222222222223 comments/sec\n",
      "\n",
      "\n",
      "Progress: 33.911764413568285%\n",
      "Comments processed by turn: 1011\n",
      "Comments processed total: 34121\n",
      "Time left: 74697.17333333332\n",
      "Speed: 1.1233333333333333 comments/sec\n",
      "\n",
      "\n",
      "Progress: 34.844012443225296%\n",
      "Comments processed by turn: 938\n",
      "Comments processed total: 35059\n",
      "Time left: 68326.00444444444\n",
      "Speed: 1.0422222222222222 comments/sec\n",
      "\n",
      "\n",
      "Progress: 35.715634534919545%\n",
      "Comments processed by turn: 877\n",
      "Comments processed total: 35936\n",
      "Time left: 63028.04111111111\n",
      "Speed: 0.9744444444444444 comments/sec\n",
      "\n",
      "\n",
      "Progress: 36.709502370374786%\n",
      "Comments processed by turn: 1000\n",
      "Comments processed total: 36936\n",
      "Time left: 70756.66666666667\n",
      "Speed: 1.1111111111111112 comments/sec\n",
      "\n",
      "\n",
      "Progress: 37.69740699881729%\n",
      "Comments processed by turn: 994\n",
      "Comments processed total: 37930\n",
      "Time left: 69234.30888888889\n",
      "Speed: 1.1044444444444443 comments/sec\n",
      "\n",
      "\n",
      "Progress: 38.7230786050071%\n",
      "Comments processed by turn: 1032\n",
      "Comments processed total: 38962\n",
      "Time left: 70697.73333333334\n",
      "Speed: 1.1466666666666667 comments/sec\n",
      "\n",
      "\n",
      "Progress: 39.80639454565332%\n",
      "Comments processed by turn: 1090\n",
      "Comments processed total: 40052\n",
      "Time left: 73350.94444444444\n",
      "Speed: 1.211111111111111 comments/sec\n",
      "\n",
      "\n",
      "Progress: 40.742618046652154%\n",
      "Comments processed by turn: 942\n",
      "Comments processed total: 40994\n",
      "Time left: 62405.40666666666\n",
      "Speed: 1.0466666666666666 comments/sec\n",
      "\n",
      "\n",
      "Progress: 41.65697645527098%\n",
      "Comments processed by turn: 920\n",
      "Comments processed total: 41914\n",
      "Time left: 60007.511111111104\n",
      "Speed: 1.0222222222222221 comments/sec\n",
      "\n",
      "\n",
      "Progress: 42.4958009083952%\n",
      "Comments processed by turn: 844\n",
      "Comments processed total: 42758\n",
      "Time left: 54258.88444444445\n",
      "Speed: 0.9377777777777778 comments/sec\n",
      "\n",
      "\n",
      "Progress: 43.426061202381305%\n",
      "Comments processed by turn: 936\n",
      "Comments processed total: 43694\n",
      "Time left: 59199.920000000006\n",
      "Speed: 1.04 comments/sec\n",
      "\n",
      "\n",
      "Progress: 44.495462993331145%\n",
      "Comments processed by turn: 1076\n",
      "Comments processed total: 44770\n",
      "Time left: 66768.19111111111\n",
      "Speed: 1.1955555555555555 comments/sec\n",
      "\n",
      "\n",
      "Progress: 45.45056998320363%\n",
      "Comments processed by turn: 961\n",
      "Comments processed total: 45731\n",
      "Time left: 58606.051111111105\n",
      "Speed: 1.0677777777777777 comments/sec\n",
      "\n",
      "\n",
      "Progress: 46.416609519266125%\n",
      "Comments processed by turn: 972\n",
      "Comments processed total: 46703\n",
      "Time left: 58227.12\n",
      "Speed: 1.08 comments/sec\n",
      "\n",
      "\n",
      "Progress: 47.24151982269398%\n",
      "Comments processed by turn: 830\n",
      "Comments processed total: 47533\n",
      "Time left: 48955.24444444445\n",
      "Speed: 0.9222222222222223 comments/sec\n",
      "\n",
      "\n",
      "Progress: 48.26719142888379%\n",
      "Comments processed by turn: 1032\n",
      "Comments processed total: 48565\n",
      "Time left: 59686.293333333335\n",
      "Speed: 1.1466666666666667 comments/sec\n",
      "\n",
      "\n",
      "Progress: 49.28292435671904%\n",
      "Comments processed by turn: 1022\n",
      "Comments processed total: 49587\n",
      "Time left: 57947.4\n",
      "Speed: 1.1355555555555557 comments/sec\n",
      "\n",
      "\n",
      "Progress: 50.29269407754157%\n",
      "Comments processed by turn: 1016\n",
      "Comments processed total: 50603\n",
      "Time left: 56460.248888888884\n",
      "Speed: 1.1288888888888888 comments/sec\n",
      "\n",
      "\n",
      "Progress: 51.33724917260503%\n",
      "Comments processed by turn: 1051\n",
      "Comments processed total: 51654\n",
      "Time left: 57177.903333333335\n",
      "Speed: 1.1677777777777778 comments/sec\n",
      "\n",
      "\n",
      "Sleeping for 20 minutes\n",
      "Progress: 52.22676088533746%\n",
      "Comments processed by turn: 895\n",
      "Comments processed total: 52549\n",
      "Time left: 47800.955555555556\n",
      "Speed: 0.9944444444444445 comments/sec\n",
      "\n",
      "\n",
      "Progress: 53.268334376894565%\n",
      "Comments processed by turn: 1048\n",
      "Comments processed total: 53597\n",
      "Time left: 54752.177777777775\n",
      "Speed: 1.1644444444444444 comments/sec\n",
      "\n",
      "\n",
      "Progress: 54.13200552590517%\n",
      "Comments processed by turn: 869\n",
      "Comments processed total: 54466\n",
      "Time left: 44561.35444444444\n",
      "Speed: 0.9655555555555555 comments/sec\n",
      "\n",
      "\n",
      "Progress: 54.796903107824726%\n",
      "Comments processed by turn: 669\n",
      "Comments processed total: 55135\n",
      "Time left: 33808.28666666667\n",
      "Speed: 0.7433333333333333 comments/sec\n",
      "\n",
      "\n",
      "Progress: 55.59398511185982%\n",
      "Comments processed by turn: 802\n",
      "Comments processed total: 55937\n",
      "Time left: 39814.84444444444\n",
      "Speed: 0.8911111111111111 comments/sec\n",
      "\n",
      "\n",
      "Progress: 56.592822286492336%\n",
      "Comments processed by turn: 1005\n",
      "Comments processed total: 56942\n",
      "Time left: 48770.41666666667\n",
      "Speed: 1.1166666666666667 comments/sec\n",
      "\n",
      "\n",
      "Progress: 57.49028494190842%\n",
      "Comments processed by turn: 903\n",
      "Comments processed total: 57845\n",
      "Time left: 42914.573333333334\n",
      "Speed: 1.0033333333333334 comments/sec\n",
      "\n",
      "\n",
      "Progress: 58.45036127095818%\n",
      "Comments processed by turn: 966\n",
      "Comments processed total: 58811\n",
      "Time left: 44871.77333333333\n",
      "Speed: 1.0733333333333333 comments/sec\n",
      "\n",
      "\n",
      "Progress: 59.371676754425195%\n",
      "Comments processed by turn: 927\n",
      "Comments processed total: 59738\n",
      "Time left: 42105.37\n",
      "Speed: 1.03 comments/sec\n",
      "\n",
      "\n",
      "Progress: 60.206525736207595%\n",
      "Comments processed by turn: 840\n",
      "Comments processed total: 60578\n",
      "Time left: 37369.73333333334\n",
      "Speed: 0.9333333333333333 comments/sec\n",
      "\n",
      "\n",
      "Progress: 61.06224594253457%\n",
      "Comments processed by turn: 861\n",
      "Comments processed total: 61439\n",
      "Time left: 37480.28666666667\n",
      "Speed: 0.9566666666666667 comments/sec\n",
      "\n",
      "\n",
      "Progress: 61.96567180496337%\n",
      "Comments processed by turn: 909\n",
      "Comments processed total: 62348\n",
      "Time left: 38651.69\n",
      "Speed: 1.01 comments/sec\n",
      "\n",
      "\n",
      "Progress: 62.89593209894948%\n",
      "Comments processed by turn: 936\n",
      "Comments processed total: 63284\n",
      "Time left: 38826.32\n",
      "Speed: 1.04 comments/sec\n",
      "\n",
      "\n",
      "Progress: 63.78444994384647%\n",
      "Comments processed by turn: 894\n",
      "Comments processed total: 64178\n",
      "Time left: 36196.073333333334\n",
      "Speed: 0.9933333333333333 comments/sec\n",
      "\n",
      "\n",
      "Progress: 64.73061212319986%\n",
      "Comments processed by turn: 952\n",
      "Comments processed total: 65130\n",
      "Time left: 37537.36\n",
      "Speed: 1.0577777777777777 comments/sec\n",
      "\n",
      "\n",
      "Progress: 65.69963326276871%\n",
      "Comments processed by turn: 975\n",
      "Comments processed total: 66105\n",
      "Time left: 37388.0\n",
      "Speed: 1.0833333333333333 comments/sec\n",
      "\n",
      "\n",
      "Progress: 66.5215619626902%\n",
      "Comments processed by turn: 827\n",
      "Comments processed total: 66932\n",
      "Time left: 30952.772222222222\n",
      "Speed: 0.9188888888888889 comments/sec\n",
      "\n",
      "\n",
      "Sentence \"і5\" in commentary #67682 not processed\n",
      "\n",
      "\n",
      "Progress: 67.42200622161265%\n",
      "Comments processed by turn: 906\n",
      "Comments processed total: 67838\n",
      "Time left: 32997.526666666665\n",
      "Speed: 1.0066666666666666 comments/sec\n",
      "\n",
      "\n",
      "Progress: 68.3602174582824%\n",
      "Comments processed by turn: 944\n",
      "Comments processed total: 68782\n",
      "Time left: 33391.37777777778\n",
      "Speed: 1.048888888888889 comments/sec\n",
      "\n",
      "\n",
      "Progress: 69.28948388443304%\n",
      "Comments processed by turn: 935\n",
      "Comments processed total: 69717\n",
      "Time left: 32101.666666666668\n",
      "Speed: 1.038888888888889 comments/sec\n",
      "\n",
      "\n",
      "Progress: 70.32608803681286%\n",
      "Comments processed by turn: 1043\n",
      "Comments processed total: 70760\n",
      "Time left: 34600.945555555554\n",
      "Speed: 1.1588888888888889 comments/sec\n",
      "\n",
      "\n",
      "Sentence \"хотя лет назад были попытки делать интересный новостной контент\" in commentary #71472 not processed\n",
      "\n",
      "\n",
      "Sentence \"тем более что смарт представили 27 июня\" in commentary #71473 not processed\n",
      "\n",
      "\n",
      "Sentence \"гиков\" in commentary #71474 not processed\n",
      "\n",
      "\n",
      "Sentence \"лол после чтения и беседы с некоторыми складывается впечатление что тут регистрируются только чтобы игры скачать на халяву\" in commentary #71474 not processed\n",
      "\n",
      "\n",
      "Sentence \"да тут уже и игр собственно нет школоты много набежало вот и нормальным людям надоело это все\" in commentary #71475 not processed\n",
      "\n",
      "\n",
      "Sentence \"по моему кто платит тот и заказывает музыку можете заплатить за vivo и будут про неё новости а так в основном проплаченые новости типа что купить для скидки на распродажа у и т д\" in commentary #71476 not processed\n",
      "\n",
      "\n",
      "Sentence \"специально для вас\" in commentary #71477 not processed\n",
      "\n",
      "\n",
      "Sentence \"\" in commentary #71477 not processed\n",
      "\n",
      "\n",
      "Progress: 71.11720683383524%\n",
      "Comments processed by turn: 796\n",
      "Comments processed total: 71556\n",
      "Time left: 25702.84\n",
      "Speed: 0.8844444444444445 comments/sec\n",
      "\n",
      "\n",
      "Progress: 72.07628929504955%\n",
      "Comments processed by turn: 965\n",
      "Comments processed total: 72521\n",
      "Time left: 30125.155555555553\n",
      "Speed: 1.0722222222222222 comments/sec\n",
      "\n",
      "\n",
      "Progress: 73.17153164972122%\n",
      "Comments processed by turn: 1102\n",
      "Comments processed total: 73623\n",
      "Time left: 33052.653333333335\n",
      "Speed: 1.2244444444444444 comments/sec\n",
      "\n",
      "\n",
      "Progress: 74.1733504278601%\n",
      "Comments processed by turn: 1008\n",
      "Comments processed total: 74631\n",
      "Time left: 29104.320000000003\n",
      "Speed: 1.12 comments/sec\n",
      "\n",
      "\n",
      "Progress: 75.09367204349165%\n",
      "Comments processed by turn: 926\n",
      "Comments processed total: 75557\n",
      "Time left: 25783.955555555556\n",
      "Speed: 1.028888888888889 comments/sec\n",
      "\n",
      "\n",
      "Progress: 76.08654601111144%\n",
      "Comments processed by turn: 999\n",
      "Comments processed total: 76556\n",
      "Time left: 26707.710000000003\n",
      "Speed: 1.11 comments/sec\n",
      "\n",
      "\n",
      "Progress: 77.03370205830029%\n",
      "Comments processed by turn: 953\n",
      "Comments processed total: 77509\n",
      "Time left: 24468.804444444446\n",
      "Speed: 1.058888888888889 comments/sec\n",
      "\n",
      "\n",
      "Sleeping for 20 minutes\n",
      "Progress: 78.07229394635101%\n",
      "Comments processed by turn: 1045\n",
      "Comments processed total: 78554\n",
      "Time left: 25617.594444444447\n",
      "Speed: 1.1611111111111112 comments/sec\n",
      "\n",
      "\n",
      "Progress: 79.01249291869166%\n",
      "Comments processed by turn: 946\n",
      "Comments processed total: 79500\n",
      "Time left: 22196.31333333333\n",
      "Speed: 1.051111111111111 comments/sec\n",
      "\n",
      "\n",
      "Progress: 80.12363715873063%\n",
      "Comments processed by turn: 1118\n",
      "Comments processed total: 80618\n",
      "Time left: 24843.20222222222\n",
      "Speed: 1.2422222222222221 comments/sec\n",
      "\n",
      "\n",
      "Progress: 81.15527197193317%\n",
      "Comments processed by turn: 1038\n",
      "Comments processed total: 81656\n",
      "Time left: 21868.353333333333\n",
      "Speed: 1.1533333333333333 comments/sec\n",
      "\n",
      "\n",
      "Progress: 82.13224405418568%\n",
      "Comments processed by turn: 983\n",
      "Comments processed total: 82639\n",
      "Time left: 19635.97111111111\n",
      "Speed: 1.0922222222222222 comments/sec\n",
      "\n",
      "\n",
      "Progress: 83.19965810946461%\n",
      "Comments processed by turn: 1074\n",
      "Comments processed total: 83713\n",
      "Time left: 20172.106666666667\n",
      "Speed: 1.1933333333333334 comments/sec\n",
      "\n",
      "\n",
      "Progress: 84.12296132860251%\n",
      "Comments processed by turn: 929\n",
      "Comments processed total: 84642\n",
      "Time left: 16489.75\n",
      "Speed: 1.0322222222222222 comments/sec\n",
      "\n",
      "\n",
      "Progress: 85.18441217686872%\n",
      "Comments processed by turn: 1068\n",
      "Comments processed total: 85710\n",
      "Time left: 17689.640000000003\n",
      "Speed: 1.1866666666666668 comments/sec\n",
      "\n",
      "\n",
      "Progress: 86.1951757655267%\n",
      "Comments processed by turn: 1017\n",
      "Comments processed total: 86727\n",
      "Time left: 15695.699999999999\n",
      "Speed: 1.13 comments/sec\n",
      "\n",
      "\n",
      "Progress: 87.27153463132473%\n",
      "Comments processed by turn: 1083\n",
      "Comments processed total: 87810\n",
      "Time left: 15411.09\n",
      "Speed: 1.2033333333333334 comments/sec\n",
      "\n",
      "\n",
      "Progress: 88.27037180595725%\n",
      "Comments processed by turn: 1005\n",
      "Comments processed total: 88815\n",
      "Time left: 13178.9\n",
      "Speed: 1.1166666666666667 comments/sec\n",
      "\n",
      "\n",
      "Progress: 89.18969955375334%\n",
      "Comments processed by turn: 925\n",
      "Comments processed total: 89740\n",
      "Time left: 11179.138888888889\n",
      "Speed: 1.0277777777777777 comments/sec\n",
      "\n",
      "\n",
      "Progress: 90.21139568860133%\n",
      "Comments processed by turn: 1028\n",
      "Comments processed total: 90768\n",
      "Time left: 11249.746666666668\n",
      "Speed: 1.1422222222222222 comments/sec\n",
      "\n",
      "\n",
      "Progress: 91.19333711003111%\n",
      "Comments processed by turn: 988\n",
      "Comments processed total: 91756\n",
      "Time left: 9727.40888888889\n",
      "Speed: 1.0977777777777777 comments/sec\n",
      "\n",
      "\n",
      "Progress: 92.12061580051085%\n",
      "Comments processed by turn: 933\n",
      "Comments processed total: 92689\n",
      "Time left: 8218.693333333333\n",
      "Speed: 1.0366666666666666 comments/sec\n",
      "\n",
      "\n",
      "Progress: 93.10852042895336%\n",
      "Comments processed by turn: 994\n",
      "Comments processed total: 93683\n",
      "Time left: 7658.217777777777\n",
      "Speed: 1.1044444444444443 comments/sec\n",
      "\n",
      "\n",
      "Progress: 94.11033920709224%\n",
      "Comments processed by turn: 1008\n",
      "Comments processed total: 94691\n",
      "Time left: 6637.120000000001\n",
      "Speed: 1.12 comments/sec\n",
      "\n",
      "\n",
      "Progress: 95.23241599332121%\n",
      "Comments processed by turn: 1129\n",
      "Comments processed total: 95820\n",
      "Time left: 6017.57\n",
      "Speed: 1.2544444444444445 comments/sec\n",
      "\n",
      "\n",
      "Progress: 96.25113052466283%\n",
      "Comments processed by turn: 1025\n",
      "Comments processed total: 96845\n",
      "Time left: 4295.888888888889\n",
      "Speed: 1.1388888888888888 comments/sec\n",
      "\n",
      "\n",
      "Progress: 97.25195543496625%\n",
      "Comments processed by turn: 1007\n",
      "Comments processed total: 97852\n",
      "Time left: 3093.7277777777776\n",
      "Speed: 1.1188888888888888 comments/sec\n",
      "\n",
      "\n",
      "Progress: 98.3104246797261%\n",
      "Comments processed by turn: 1065\n",
      "Comments processed total: 98917\n",
      "Time left: 2011.6666666666667\n",
      "Speed: 1.1833333333333333 comments/sec\n",
      "\n",
      "\n",
      "Progress: 99.32516373972588%\n",
      "Comments processed by turn: 1021\n",
      "Comments processed total: 99938\n",
      "Time left: 770.2877777777777\n",
      "Speed: 1.1344444444444444 comments/sec\n",
      "\n",
      "\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "lemmatized_txt = open('lemmatized.txt', 'w', encoding='utf8')\n",
    "\n",
    "with open('learn_scrapy\\pda.jl', 'rb') as f:\n",
    "    \n",
    "    vocab = FullVocab()\n",
    "    num_comments = 100617\n",
    "    counter = 0\n",
    "    prev_counter = 0\n",
    "    anchor_counter = 0\n",
    "    checkpoint = tm.clock()\n",
    "    dt = 900\n",
    "    \n",
    "    for item in jl.reader(f):\n",
    "        item_origin = None    #Сюда пишем 't', если comment, 'a', если article\n",
    "        sents = None\n",
    "        \n",
    "        if item.get('comment') is not None:\n",
    "            sents = nltk.sent_tokenize(item['comment'])\n",
    "            item_origin = 't'\n",
    "        elif item.get('article') is not None:   \n",
    "            sents = nltk.sent_tokenize(item['article'])\n",
    "            item_origin = 'a'\n",
    "            \n",
    "        for sent in sents:\n",
    "            toks = tokenizer.tokenize(sent)\n",
    "            toks = [t.lower() for t in toks if t.isalnum()]\n",
    "            sent = ' '.join(toks)\n",
    "            \n",
    "            try:\n",
    "                lemma_annotate = isp_api.lemmatizationAnnotate(text = sent)\n",
    "                pos_annotate = isp_api.posTaggingAnnotate(text = sent)\n",
    "                \n",
    "            except KeyboardInterrupt:\n",
    "                print(\"Understandable, have a nice day\")\n",
    "                vocab.build_xml()\n",
    "                lemmatized_txt.close()\n",
    "                sys.exit()\n",
    "                \n",
    "            except:\n",
    "                print(\"Sentence\", end = \" \\\"\")\n",
    "                print(sent, end = \"\\\" \")\n",
    "                print(\"in commentary #\", end = \"\")\n",
    "                print(counter, end = \" \")\n",
    "                print(\"not processed\")\n",
    "                print(\"\\n\")\n",
    "                continue\n",
    "                \n",
    "            if lemma_annotate.get('annotations').get('lemma') is not None:\n",
    "                lemmatized = []\n",
    "                for i in range(len(lemma_annotate.get('annotations').get('lemma'))):\n",
    "                    l = lemma_annotate.get('annotations').get('lemma')[i].get('value')\n",
    "                    p = pos_annotate['annotations']['pos-token'][i]['value'].get('tag')\n",
    "                    word_id = vocab.add_item(toks[i], l, p, item_origin)\n",
    "                    lemmatized.append(str(word_id))\n",
    "                \n",
    "                lemmatized = ' '.join(lemmatized)\n",
    "                lemmatized_txt.write(lemmatized + '\\n')\n",
    "        \n",
    "        counter = counter + 1\n",
    "        if(tm.clock() - checkpoint > dt):\n",
    "            print(\"Progress: \", end = '')\n",
    "            print(counter / num_comments * 100, end = '')\n",
    "            print(\"%\")\n",
    "            print(\"Comments processed by turn: \", end= '')\n",
    "            print(counter - prev_counter)\n",
    "            print(\"Comments processed total: \", end= '')\n",
    "            print(counter)\n",
    "            print(\"Time left: \", end = '')\n",
    "            speed = (counter - prev_counter) / dt\n",
    "            print(speed * (num_comments - counter))\n",
    "            print(\"Speed: \", end = '')\n",
    "            print(speed, end = ' ')\n",
    "            print(\"comments/sec\")\n",
    "            print(\"\\n\")\n",
    "            prev_counter = counter\n",
    "            vocab.build_xml()\n",
    "            if (counter - anchor_counter) / num_comments * 100 > 25:\n",
    "                print(\"Sleeping for 20 minutes\")\n",
    "                tm.sleep(1200)\n",
    "                anchor_counter = counter\n",
    "            checkpoint = tm.clock()\n",
    "            \n",
    "    vocab.build_xml()\n",
    "    print(\"Finished\")\n",
    "    \n",
    "lemmatized_txt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2vec модель из gensim\n",
    "Ищу синонимы с помощью метода most_similar() и подставляю в изначальный текст"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab finished\n"
     ]
    }
   ],
   "source": [
    "new_vocab = FullVocab(from_file = True)\n",
    "print('Vocab finished')\n",
    "w2v_model = gensim.models.Word2Vec(gensim.models.word2vec.LineSentence('lemmatized.txt'), min_count=10, hs=1, negative=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "сталкер - 0.7671462297439575\n",
      "моб - 0.7632809281349182\n",
      "персонаж - 0.7628835439682007\n",
      "близард - 0.7590043544769287\n",
      "спиннер - 0.7520766258239746\n",
      "мода - 0.7513926029205322\n",
      "мультиплеер - 0.7513076066970825\n",
      "парочка - 0.7484524846076965\n",
      "дополнение - 0.747899055480957\n",
      "длс - 0.7453457117080688\n"
     ]
    }
   ],
   "source": [
    "lst = w2v_model.most_similar_cosmul(positive = [str(new_vocab.id_by_word('длц'))])\n",
    "for l in lst:\n",
    "    print(new_vocab.word_by_id(int(l[0])), end = \" - \")\n",
    "    print(l[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Продолжение легендарной научно-фантастической RPG-серии Mass Effect: Andromeda стало одним из главных разочарований года.\n",
      "Продолжение легендарной научно-фантастической RPG-серии Mass Effect : Andromeda стало одним из ключевой разочарований года .\n",
      "\n",
      "\n",
      "Сиквел вышел настолько слабым, что в Сети начали активно циркулировать слухи об отмене всех сюжетных DLC для Andromeda.\n",
      "Сиквел вышел очевидно мощный , что в интернет начали активно циркулировать подробность об отмене всех сюжетных DLC для Andromeda .\n",
      "\n",
      "\n",
      "Масла в огонь подлила студия Sinclair Networks.\n",
      "Масла в огонь подлила студия Sinclair Networks .\n",
      "\n",
      "\n",
      "На своей странице в Facebook эта никому не известная контора опубликовала информацию о том, что она была нанята Bioware для работы над тремя сюжетными DLC к Mass Effect: Andromeda, и что позже планы на сюжетные дополнения было решено отменить.\n",
      "На своей странице в Facebook эта никому не любопытный контора опубликовала подробность о том , что она была нанята Bioware для работы над несколько сюжетными DLC к Mass Effect : Andromeda , и что скоро порядок на сюжетные дополнения было решаться отменить .\n",
      "\n",
      "\n",
      "EA и Bioware, в свою очередь, сказали, что и слыхом ни о какой Sinclair Networks не слыхивали и что разработку сюжетных DLC Bioware не доверяет сторонним организациям.\n",
      "EA и Bioware , в свою партия , сказали , что и слыхом ни о какой Sinclair Networks не слыхивали и что производство сюжетных DLC Bioware не доверяет стандартный организациям .\n",
      "\n",
      "\n",
      "Более того, как выяснилось несколько позже, компании под названием Sinclair Networks и вовсе не существует.\n",
      "Более того , как выяснилось несколько скоро , компании под копирка Sinclair Networks и отсюда не являться .\n",
      "\n",
      "\n",
      "Значит ли, что новость об отмене сюжетных DLC — фейк?\n",
      "полноценно ли , что новость об отмене сюжетных DLC — фейк ?\n",
      "\n",
      "\n",
      "Не совсем.\n",
      "Не совсем .\n",
      "\n",
      "\n",
      "Как это ни иронично, но обманщики с Facebook оказались правы.\n",
      "Как это ни иронично , но обманщики с Facebook выясняться правы .\n",
      "\n",
      "\n",
      "Журналисты ресурса Kotaku связались с несколькими сотрудниками Bioware, и те подтвердили: сюжетных DLC действительно не выйдет, а команда Bioware Montreal разделится.\n",
      "Журналисты пользователь Kotaku связались с три сотрудниками Bioware , и те подтвердили : сюжетных DLC реально не выйдет , а деятельность Bioware Montreal разделится .\n",
      "\n",
      "\n",
      "Большинство сотрудников станут работать над другими проектами, а оставшиеся займутся выпуском патчей и отладкой мультиплеера.\n",
      "некоторый сотрудников станут работать над другими студия , а оставшиеся развиваться выход патчей и отладкой мультиплеера .\n",
      "\n",
      "\n",
      "Кажется, планы на сиквел не оправдавшей ожидания Mass Effect: Andromeda тоже теперь под большим вопросом.\n",
      "\n",
      "понимать , порядок на сиквел не оправдавшей ожидания Mass Effect : Andromeda тоже теперь под большим вопросом .\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def chiki_briki(filename):\n",
    "    first_proximity = 0.79\n",
    "    second_proximity = 0.75\n",
    "    allowed_freq_min = 200 / new_vocab.id_counter\n",
    "    allowed_freq_max = 1000 / new_vocab.id_counter\n",
    "    \n",
    "    target_article_txt = open(filename, 'r', encoding='utf-8')\n",
    "    target_article = target_article_txt.read()\n",
    "    #print(target_article)\n",
    "    #print('\\n')\n",
    "    ta_sents = nltk.sent_tokenize(target_article)\n",
    "    new_article = ''\n",
    "    for sent in ta_sents:\n",
    "        comparer = []\n",
    "        new_sent = []\n",
    "        \n",
    "        la = isp_api.lemmatizationAnnotate(text = sent)\n",
    "        lp = isp_api.posTaggingAnnotate(text = sent)\n",
    "        for i in range(len(la['annotations']['lemma'])):\n",
    "            l = la['annotations']['lemma'][i]\n",
    "            p = lp['annotations']['pos-token'][i]\n",
    "            word_id = str(new_vocab.id_by_word(l['value'], tag = p['value']['tag']))\n",
    "            comparer.append({'text': l['text'], 'norma': l['value'], 'tag': p['value']['tag'], 'word_id': word_id})\n",
    "            \n",
    "        #lemma = [l['value'] for l in la['annotations']['lemma']]\n",
    "        #print(comparer)\n",
    "        \n",
    "        for i, c in enumerate(comparer):\n",
    "            if c['tag'] in ['PR', 'PART', 'CONJ'] or not c['text'].isalnum():\n",
    "                new_sent.append(c['text'])\n",
    "                continue\n",
    "                \n",
    "            word_id = new_vocab.id_by_word(c['norma'], tag = c['tag'])\n",
    "            word_item = new_vocab.word_by_id(word_id, return_val = 'item')\n",
    "            if word_id is None or int(word_item['cnt']) < 10:\n",
    "                new_sent.append(c['text'])\n",
    "                continue\n",
    "                \n",
    "            #print(word_item['normal_form'] + ' - ' + word_item['origin'])\n",
    "            '''    \n",
    "            if word_item['origin'] is not 'xax':\n",
    "                new_sent.append(c['text'])\n",
    "                continue\n",
    "            '''    \n",
    "            #new_sent.append(c['norma'])\n",
    "            synonyms = w2v_model.most_similar_cosmul(positive = [str(word_id)])\n",
    "            '''\n",
    "            print(word_item['normal_form'])\n",
    "            for s in synonyms:\n",
    "                print(new_vocab.word_by_id(int(s[0])), end = \" - \")\n",
    "                print(s[1])\n",
    "            '''\n",
    "            \n",
    "            suitable_syns = [s for s in synonyms if new_vocab.word_by_id(int(s[0]), return_val = 'item')['origin'] in\n",
    "                           ['txx', 'tax'] and \n",
    "                           new_vocab.word_by_id(int(s[0]), return_val = 'item')['tags'] == c['tag']]\n",
    "            lemma = [w['word_id'] for w in comparer]\n",
    "            ranked_syns = []\n",
    "            for s in suitable_syns:\n",
    "                lemma[i] = s[0]\n",
    "                rank = (w2v_model.score([lemma])) / 100 \n",
    "                ranked_syns.append({'syn': s[0], 'rank': rank[0]})\n",
    "            \n",
    "            ranked_syns = sorted(ranked_syns, key=lambda s: s['rank'], reverse=True)\n",
    "            \n",
    "            #print(c['norma'])\n",
    "            #print(suitable_syns)\n",
    "            #print(ranked_syns)\n",
    "            \n",
    "            if len(suitable_syns) == 0:  \n",
    "                new_sent.append(c['text'])\n",
    "                continue\n",
    "            \n",
    "            freq = int(word_item['cnt']) / new_vocab.id_counter\n",
    "            \n",
    "            if  freq < allowed_freq_max and freq > allowed_freq_min:     #suitable_syns[0][1] > first_proximity:\n",
    "                new_sent.append(new_vocab.word_by_id(int(suitable_syns[0][0])))\n",
    "            #elif suitable_syns[0][1] > second_proximity: \n",
    "                #optionnal_syn = '(' + new_vocab.word_by_id(int(ranked_syns[0]['syn'])) + ')'\n",
    "                #new_sent.append(optionnal_syn)\n",
    "            else:  \n",
    "                new_sent.append(c['text'])\n",
    "                continue\n",
    "                \n",
    "        \n",
    "        print(sent)\n",
    "        print(' '.join(new_sent))\n",
    "        print('\\n')\n",
    "            \n",
    "        '''\n",
    "            t_id = new_vocab.id_by_word(t)\n",
    "            \n",
    "            \n",
    "            if t_id is not None and int(new_vocab.word_by_id(t_id, return_val = 'item')['cnt']) > 10:\n",
    "                lst = w2v_model.most_similar(positive = [str(t_id)])\n",
    "                new_t = new_vocab.word_by_id(int(lst[0][0]), return_val = 'item')\n",
    "                if new_t['origin'] == 'tax' or new_t['origin'] == 'txx':\n",
    "                    print(new_t['normal_form'], end = ' ')\n",
    "                else:\n",
    "                    print(t, end = ' ')\n",
    "        \n",
    "            else:\n",
    "                print(t, end = ' ')\n",
    "        '''\n",
    "        \n",
    "        \n",
    "chiki_briki('target_article.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some testing\n",
    "Everythin' below is my lazy draft, don't borther yourself lookig through"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2034\n",
      "2365\n",
      "0.915373687744\n",
      "[('2151', 0.8232387900352478), ('2367', 0.7768270969390869), ('2380', 0.7505201101303101), ('1583', 0.7477496862411499), ('1344', 0.7444450855255127), ('26640', 0.7400096654891968), ('453', 0.7385299801826477), ('2021', 0.7321900129318237), ('3844', 0.7259445190429688), ('7477', 0.724348783493042)]\n",
      "87557\n",
      "0.011421131377274232\n",
      "0.11421131377274232\n"
     ]
    }
   ],
   "source": [
    "print(new_vocab.id_by_word('samsung'))\n",
    "print(new_vocab.id_by_word('galaxy'))\n",
    "print(1 + w2v_model.score([[str(new_vocab.id_by_word('samsung')), str(new_vocab.id_by_word('mass'))]])[0] / 100)\n",
    "print(w2v_model.most_similar_cosmul(positive = ['256']))\n",
    "\n",
    "print(new_vocab.id_counter)\n",
    "allowed_freq_min = 1000 / new_vocab.id_counter\n",
    "allowed_freq_max = 10000 / new_vocab.id_counter\n",
    "print(allowed_freq_min)\n",
    "print(allowed_freq_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "comments = ''\n",
    "with open('learn_scrapy\\pda.jl', 'rb') as f:\n",
    "    for item in jl.reader(f):\n",
    "        comments = comments + item['comment']       #создаём один string на все комменты\n",
    "                                                    #скорее всего, будет логичнее для N-граммов каждое предложение\n",
    "                                                    #запоминать в один string и хранить как list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kek1\n",
      "Kek2\n",
      "Kek3\n",
      "Kek4\n"
     ]
    }
   ],
   "source": [
    "#тут токенизируем комментарии\n",
    "#есть подозрение, что я создаю слишком много сущностей \n",
    "\n",
    "tknzr = nltk.TweetTokenizer()\n",
    "print('Kek1')\n",
    "tokens = tknzr.tokenize(comments)\n",
    "print('Kek2')\n",
    "text = nltk.Text(tokens)\n",
    "print('Kek3')\n",
    "sents = nltk.sent_tokenize(comments)                         #строить n-граммную модель, возможно, буду \n",
    "print('Kek4')                                                             #анализируя предложения по-отдельности\n",
    "words = sorted([w.lower() for w in tokens if w.isalpha()])   #впоследствии буду отсекать наименее наиболее частотные слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10600020178435018\n"
     ]
    }
   ],
   "source": [
    "def lexical_diversity(text):\n",
    "    return len(set(text)) / len(text)\n",
    "\n",
    "print(lexical_diversity(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sent_tokens = list()                            #здесь храним предложения\n",
    "for i in range(len(sents)):\n",
    "    sent_tokens.append(tknzr.tokenize(sents[i]))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#сох нормальные формы слов\n",
    "morph = pm2.MorphAnalyzer()     \n",
    "vocab_normalized = sorted(set([morph.parse(w)[0].normalized for w in words]), key = lambda w: w.normal_form)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def progress(full, curr, checkpoint):\n",
    "    if curr / full * 100 > checkpoint:\n",
    "        print(curr / full * 100, end = '')\n",
    "        print('% complited')\n",
    "        return True\n",
    "    \n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "i = 0\n",
    "h = 0.1\n",
    "checkpoint = h\n",
    "time = tm.clock()\n",
    "for sent in sents:\n",
    "    if progress(len(sents), i, checkpoint):\n",
    "        time_per_checkpoint = tm.clock() - time\n",
    "        print('Time per checkpoint:', end = ' ')\n",
    "        print(time_per_checkpoint)\n",
    "        print('Time per sentence:', end = ' ')\n",
    "        time_per_sent = time_per_checkpoint / (len(sents) * h)\n",
    "        print(time_per_sent)\n",
    "        print('Estimated time:', end = ' ')\n",
    "        print(time_per_checkpoint * (100 - checkpoint))\n",
    "        time = tm.clock()\n",
    "        checkpoint = checkpoint + h\n",
    "        \n",
    "    toks = tknzr.tokenize(sent)\n",
    "    toks = [t.lower() for t in toks if t.isalnum()]\n",
    "    sentence = ''\n",
    "    for t in toks:\n",
    "        sentence = sentence + t + ' '\n",
    "    \n",
    "    lemma_raw = isp_api.lemmatizationAnnotate(text = sentence)\n",
    "    lemmatized = ''\n",
    "    if lemma_raw.get('annotations').get('lemma') is not None:\n",
    "        for l in lemma_raw.get('annotations').get('lemma'):\n",
    "            lemmatized = lemmatized + l.get('value') + ' '\n",
    "        \n",
    "        sents[i] = lemmatized\n",
    "    \n",
    "    else:\n",
    "        sents[i] = ''\n",
    "        \n",
    "    i = i + 1\n",
    "        \n",
    "    \n",
    "sents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sents' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-e8335ea762b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'lemmatized.txt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf8'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msents\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m5000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sents' is not defined"
     ]
    }
   ],
   "source": [
    "with open('lemmatized.txt', 'w', encoding='utf8') as f:\n",
    "    for s in sents[0:5000]:\n",
    "        f.write(s + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec(gensim.models.word2vec.LineSentence('lemmatized.txt'), min_count=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('а', 0.9998720288276672),\n",
       " ('они', 0.9998717904090881),\n",
       " ('где', 0.9998696446418762),\n",
       " ('этот', 0.9998689293861389),\n",
       " ('мочь', 0.9998672604560852),\n",
       " ('когда', 0.999864935874939),\n",
       " ('это', 0.9998648166656494),\n",
       " ('там', 0.9998632669448853),\n",
       " ('с', 0.9998587965965271),\n",
       " ('ничто', 0.9998581409454346)]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive = ['айфон'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "сейчас\n",
      "{'value': {'characters': [], 'tag': 'ADV', 'type': 'syn-tag-rus'}, 'start': 0, 'text': 'сейчас', 'end': 6, 'annotated-text': 'сейчас я говорю о том человеке который спит отдыхая на работе'}\n",
      "[Parse(word='сейчас', tag=OpencorporaTag('ADVB'), normal_form='сейчас', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'сейчас', 3, 0),))]\n",
      "\n",
      "\n",
      "я\n",
      "{'value': {'characters': [{'tag': 'Masculine', 'type': 'gender'}, {'tag': 'DEICTIC', 'type': 'pronoun'}, {'tag': 'Singular', 'type': 'number'}, {'tag': 'Nominative', 'type': 'case'}, {'tag': 'Animated', 'type': 'animacy'}], 'tag': 'S', 'type': 'syn-tag-rus'}, 'start': 7, 'text': 'я', 'end': 8, 'annotated-text': 'сейчас я говорю о том человеке который спит отдыхая на работе'}\n",
      "[Parse(word='я', tag=OpencorporaTag('NPRO,1per sing,nomn'), normal_form='я', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'я', 3100, 0),))]\n",
      "\n",
      "\n",
      "говорить\n",
      "{'value': {'characters': [{'tag': 'First', 'type': 'person'}, {'tag': 'Singular', 'type': 'number'}, {'tag': 'NotPast', 'type': 'tense'}, {'tag': 'Indicative', 'type': 'mode'}], 'tag': 'V', 'type': 'syn-tag-rus'}, 'start': 9, 'text': 'говорю', 'end': 15, 'annotated-text': 'сейчас я говорю о том человеке который спит отдыхая на работе'}\n",
      "[Parse(word='говорить', tag=OpencorporaTag('INFN,impf,tran'), normal_form='говорить', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'говорить', 395, 0),))]\n",
      "\n",
      "\n",
      "о\n",
      "{'value': {'characters': [], 'tag': 'PR', 'type': 'syn-tag-rus'}, 'start': 16, 'text': 'о', 'end': 17, 'annotated-text': 'сейчас я говорю о том человеке который спит отдыхая на работе'}\n",
      "[Parse(word='о', tag=OpencorporaTag('PREP'), normal_form='о', score=0.990985, methods_stack=((<DictionaryAnalyzer>, 'о', 2133, 0),)), Parse(word='о', tag=OpencorporaTag('INTJ'), normal_form='о', score=0.009014, methods_stack=((<DictionaryAnalyzer>, 'о', 21, 0),))]\n",
      "\n",
      "\n",
      "то\n",
      "{'value': {'characters': [{'tag': 'Neuter', 'type': 'gender'}, {'tag': 'Prepositional', 'type': 'case'}, {'tag': 'Singular', 'type': 'number'}, {'tag': 'Inanimated', 'type': 'animacy'}], 'tag': 'S', 'type': 'syn-tag-rus'}, 'start': 18, 'text': 'том', 'end': 21, 'annotated-text': 'сейчас я говорю о том человеке который спит отдыхая на работе'}\n",
      "[Parse(word='то', tag=OpencorporaTag('CONJ'), normal_form='то', score=0.5, methods_stack=((<DictionaryAnalyzer>, 'то', 20, 0),)), Parse(word='то', tag=OpencorporaTag('PRCL'), normal_form='то', score=0.1, methods_stack=((<DictionaryAnalyzer>, 'то', 22, 0),))]\n",
      "\n",
      "\n",
      "человек\n",
      "{'value': {'characters': [{'tag': 'Masculine', 'type': 'gender'}, {'tag': 'Prepositional', 'type': 'case'}, {'tag': 'Singular', 'type': 'number'}, {'tag': 'Animated', 'type': 'animacy'}], 'tag': 'S', 'type': 'syn-tag-rus'}, 'start': 22, 'text': 'человеке', 'end': 30, 'annotated-text': 'сейчас я говорю о том человеке который спит отдыхая на работе'}\n",
      "[Parse(word='человек', tag=OpencorporaTag('NOUN,anim,masc sing,nomn'), normal_form='человек', score=0.549382, methods_stack=((<DictionaryAnalyzer>, 'человек', 488, 0),)), Parse(word='человек', tag=OpencorporaTag('NOUN,anim,masc sing,nomn'), normal_form='человек', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'человек', 488, 0),))]\n",
      "\n",
      "\n",
      "который\n",
      "{'value': {'characters': [{'tag': 'Masculine', 'type': 'gender'}, {'tag': 'Nominative', 'type': 'case'}, {'tag': 'Singular', 'type': 'number'}], 'tag': 'S', 'type': 'syn-tag-rus'}, 'start': 31, 'text': 'который', 'end': 38, 'annotated-text': 'сейчас я говорю о том человеке который спит отдыхая на работе'}\n",
      "[Parse(word='который', tag=OpencorporaTag('ADJF,Apro,Subx,Anph masc,sing,nomn'), normal_form='который', score=0.823529, methods_stack=((<DictionaryAnalyzer>, 'который', 1788, 0),)), Parse(word='который', tag=OpencorporaTag('ADJF,Apro,Subx,Anph masc,sing,nomn'), normal_form='который', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'который', 1788, 0),))]\n",
      "\n",
      "\n",
      "спать\n",
      "{'value': {'characters': [{'tag': 'Third', 'type': 'person'}, {'tag': 'Singular', 'type': 'number'}, {'tag': 'NotPast', 'type': 'tense'}, {'tag': 'Indicative', 'type': 'mode'}], 'tag': 'V', 'type': 'syn-tag-rus'}, 'start': 39, 'text': 'спит', 'end': 43, 'annotated-text': 'сейчас я говорю о том человеке который спит отдыхая на работе'}\n",
      "[Parse(word='спать', tag=OpencorporaTag('INFN,impf,intr'), normal_form='спать', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'спать', 2808, 0),))]\n",
      "\n",
      "\n",
      "отдыхать\n",
      "{'value': {'characters': [{'tag': 'Gerund', 'type': 'representation'}, {'tag': 'NotPast', 'type': 'tense'}], 'tag': 'V', 'type': 'syn-tag-rus'}, 'start': 44, 'text': 'отдыхая', 'end': 51, 'annotated-text': 'сейчас я говорю о том человеке который спит отдыхая на работе'}\n",
      "[Parse(word='отдыхать', tag=OpencorporaTag('INFN,impf,intr'), normal_form='отдыхать', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'отдыхать', 2247, 0),))]\n",
      "\n",
      "\n",
      "на\n",
      "{'value': {'characters': [], 'tag': 'PR', 'type': 'syn-tag-rus'}, 'start': 52, 'text': 'на', 'end': 54, 'annotated-text': 'сейчас я говорю о том человеке который спит отдыхая на работе'}\n",
      "[Parse(word='на', tag=OpencorporaTag('PREP'), normal_form='на', score=0.99931, methods_stack=((<DictionaryAnalyzer>, 'на', 24, 0),)), Parse(word='на', tag=OpencorporaTag('PRCL'), normal_form='на', score=0.000477, methods_stack=((<DictionaryAnalyzer>, 'на', 22, 0),)), Parse(word='на', tag=OpencorporaTag('INTJ'), normal_form='на', score=0.000212, methods_stack=((<DictionaryAnalyzer>, 'на', 21, 0),))]\n",
      "\n",
      "\n",
      "работа\n",
      "{'value': {'characters': [{'tag': 'Feminine', 'type': 'gender'}, {'tag': 'Prepositional', 'type': 'case'}, {'tag': 'Singular', 'type': 'number'}, {'tag': 'Inanimated', 'type': 'animacy'}], 'tag': 'S', 'type': 'syn-tag-rus'}, 'start': 55, 'text': 'работе', 'end': 61, 'annotated-text': 'сейчас я говорю о том человеке который спит отдыхая на работе'}\n",
      "[Parse(word='работа', tag=OpencorporaTag('NOUN,inan,femn sing,nomn'), normal_form='работа', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'работа', 55, 0),))]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i = 5\n",
    "\n",
    "message = 'сейчас я говорю о том человеке который спит отдыхая на работе'\n",
    "message_tokens = tokenizer.tokenize(message)\n",
    "parse_tokens = [morph.parse(t) for t in message_tokens]\n",
    "#print(parse_tokens[i])\n",
    "isp_POS = isp_api.posTaggingAnnotate(text = message)\n",
    "isp_lemma = isp_api.lemmatizationAnnotate(text = message)\n",
    "\n",
    "#print(isp_POS['annotations']['pos-token'][i])\n",
    "\n",
    "#print(isp_lemma.get('annotations').get('lemma')[i].get('value'))\n",
    "\n",
    "#print(' '.join([l.get('value') for l in isp_lemma.get('annotations').get('lemma')]))\n",
    "\n",
    "for j in range(len(message_tokens)):\n",
    "    isp_lemma_token = isp_lemma.get('annotations').get('lemma')[j].get('value')\n",
    "    morph_parse_token = [w.normalized for w in morph.parse(isp_lemma_token) if w.normal_form == w.word]\n",
    "    print(isp_lemma_token)\n",
    "    print(isp_POS['annotations']['pos-token'][j])\n",
    "    print(morph_parse_token)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "встать\n",
      "{'text': 'встать', 'annotations': {'lemma': [{'text': 'встать', 'value': 'вставать', 'annotated-text': 'встать', 'start': 0, 'end': 6}]}}\n",
      "[Parse(word='встать', tag=OpencorporaTag('INFN,perf,intr'), normal_form='встать', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'встать', 904, 0),))]\n",
      "\n",
      "\n",
      "вставать\n",
      "{'text': 'вставать', 'annotations': {'lemma': [{'text': 'вставать', 'value': 'вставать', 'annotated-text': 'вставать', 'start': 0, 'end': 8}]}}\n",
      "[Parse(word='вставать', tag=OpencorporaTag('INFN,impf,intr'), normal_form='вставать', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'вставать', 903, 0),))]\n",
      "\n",
      "\n",
      "встали\n",
      "{'text': 'встали', 'annotations': {'lemma': [{'text': 'встали', 'value': 'вставать', 'annotated-text': 'встали', 'start': 0, 'end': 6}]}}\n",
      "[Parse(word='встали', tag=OpencorporaTag('VERB,perf,intr plur,past,indc'), normal_form='встать', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'встали', 904, 4),))]\n",
      "\n",
      "\n",
      "вставали\n",
      "{'text': 'вставали', 'annotations': {'lemma': [{'text': 'вставали', 'value': 'вставать', 'annotated-text': 'вставали', 'start': 0, 'end': 8}]}}\n",
      "[Parse(word='вставали', tag=OpencorporaTag('VERB,impf,intr plur,past,indc'), normal_form='вставать', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'вставали', 903, 10),))]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ws = ['встать', 'вставать', 'встали', 'вставали']\n",
    "for w in ws:\n",
    "    print(w)\n",
    "    ns = morph.parse(w)\n",
    "    ts = isp_api.lemmatizationAnnotate(text = w)\n",
    "    print(ts)\n",
    "    print(ns)\n",
    "        \n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

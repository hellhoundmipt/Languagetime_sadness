{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Maxim\\Anaconda3\\lib\\site-packages\\gensim-2.2.0-py3.5-win-amd64.egg\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import jsonlines as jl\n",
    "import nltk\n",
    "import xmltodict\n",
    "from ispras import texterra as isp\n",
    "import pymorphy2 as pm2\n",
    "import gensim\n",
    "import time as tm\n",
    "from lxml import etree\n",
    "import sys\n",
    "\n",
    "isp_api = isp.API('ba74236a7212a71054ae1408b30b1bdef771d35b')\n",
    "#nltk.download()\n",
    "\n",
    "tokenizer = nltk.TweetTokenizer()\n",
    "morph = pm2.MorphAnalyzer()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лемматизация с помощью Texterra\n",
    "\n",
    "Читаю комментарии и статьи из файла по очереди, разбиваю их на предложения, лемматизирую с помощью Texterra, составляя при этом словарь FullVocab. Записи в Fullvocab хранятся в следующем виде:\n",
    "\n",
    "__item__:\n",
    "- __id__: id нормальной формы\n",
    "- __normal_form__: нормальная форма\n",
    "- __tags__: часть речи\n",
    "- __word_forms__: список встретившихся словоформ\n",
    "- __cnt__: сколько раз встречалось\n",
    "- __origin__: в каком корпусе текстов слово встретилось (t - треннировочный [комментарии], a - целевой [статьи], e - дополнительный [например, вешний словарь])\n",
    "\n",
    "В lemmatized.txt записываются построчно лемматизированные предложения, но вместо слов в нём \"id нормальной формы\", чтобы для word2vec слова \"стать\" (<i>глагол</i>) и \"стать\" (<i>сущ</i>) не были одним и тем же словом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class FullVocab:\n",
    "    \n",
    "    # Инициализация, либо пустой класс, либо из файла vocab.xml\n",
    "    def __init__(self, from_file = False):\n",
    "        self.id_counter = 0\n",
    "        self.vocab = []\n",
    "        if from_file:\n",
    "            f = open('vocab.xml', 'rb')\n",
    "            root = etree.XML(f.read())\n",
    "            for item in root:\n",
    "                self.add_item(item[3].text.split(' '), item[1].text, item[2].text, item[5].text, cnt = item[4].text, \n",
    "                              word_id = item[0].text)\n",
    "            \n",
    "        \n",
    "    # Добавляем новое слово в словарь\n",
    "    def add_item(self, words, normal_form, tags_normal, origin, cnt = 0, word_id = -1):\n",
    "        \n",
    "        # Дабы было можно и str и list в аргументы добавлять\n",
    "        if type(words) is not list:\n",
    "            words = [words]\n",
    "            \n",
    "        # Проверяем, есть ли нормальная форма слова в словаре. Если есть, то обновляем словоформы, если нет, то \n",
    "        # добавляем новую запись в словарь, присвивая уникальный id\n",
    "        for item in self.vocab:\n",
    "            if item.get('normal_form') == normal_form and item.get('tags') == tags_normal:\n",
    "                item['word_forms'].extend(words)\n",
    "                item['word_forms'] = sorted(set(item['word_forms']))\n",
    "                item['cnt'] += 1\n",
    "                item['origin'] = self.handle_origin(origin, item['origin'])\n",
    "                return item['id']\n",
    "                \n",
    "        #Если такой записи ещё не было\n",
    "        new_item = {'normal_form': normal_form, 'tags': tags_normal, 'word_forms': words}\n",
    "        if normal_form not in new_item['word_forms']:\n",
    "            new_item['word_forms'].append(normal_form)\n",
    "        if int(cnt) > 0:\n",
    "            new_item['cnt'] = int(cnt)\n",
    "        else:\n",
    "            new_item['cnt'] = 1\n",
    "            \n",
    "        new_item['origin'] = self.handle_origin(origin)\n",
    "        \n",
    "        return_id = 0\n",
    "        if word_id == -1:         #если мы не уточняем, какой именно id у нового слова (когда просто добавляем новое слово)\n",
    "            new_item['id'] = self.id_counter\n",
    "            return_id = self.id_counter\n",
    "            self.id_counter = self.id_counter + 1\n",
    "        else:                     #если мы указали id для нового слова (когда читаем из файла сформированный словарь)\n",
    "            return_id = int(word_id)\n",
    "            new_item['id'] = return_id\n",
    "            self.id_counter = return_id + 1\n",
    "            \n",
    "        self.vocab.append(new_item)\n",
    "        return return_id\n",
    "         \n",
    "        \n",
    "    def show_vocab(self):\n",
    "        print(self.vocab)\n",
    "        \n",
    "        \n",
    "    # Сохраняем копию данные класса в vocab.xml\n",
    "    def build_xml(self):\n",
    "        f = open('vocab.xml', 'wb')\n",
    "        root = etree.Element('root')\n",
    "        for item in self.vocab:\n",
    "            child = etree.SubElement(root, 'w' + str(item['id']))\n",
    "            etree.SubElement(child, 'id').text = str(item['id'])\n",
    "            etree.SubElement(child, 'normal_form').text = item['normal_form']\n",
    "            etree.SubElement(child, 'tags').text = item['tags']\n",
    "            etree.SubElement(child, 'word_forms').text = ' '.join(item['word_forms'])\n",
    "            etree.SubElement(child, 'cnt').text = str(item['cnt'])\n",
    "            etree.SubElement(child, 'origin').text = item['origin']\n",
    "            \n",
    "        result = etree.tostring(root, encoding='unicode', method='xml', pretty_print=True)\n",
    "        f.write(result.encode('utf8'))\n",
    "        \n",
    "        \n",
    "    # Ищем нормальную форму или запись в словаре по id\n",
    "    # Если return_val = \"word\", то возвращаем нормальную форму, если \"item\", то запись в словаре\n",
    "    def word_by_id(self, word_id, return_val = \"word\"):\n",
    "        item = next((item for item in self.vocab if item[\"id\"] == word_id), None)\n",
    "        if item is None:\n",
    "            return None\n",
    "        elif return_val == 'word':\n",
    "            return item['normal_form']\n",
    "        elif return_val == 'item':\n",
    "            return item\n",
    "        \n",
    "        \n",
    "    def id_by_word(self, word, tag = None):\n",
    "        if tag == None:\n",
    "            item = next((item for item in self.vocab if item['normal_form'] == word), None)\n",
    "        else:\n",
    "            item = next((item for item in self.vocab if item['normal_form'] == word and item['tags'] == tag), None)\n",
    "        if item is None:\n",
    "            return None\n",
    "        else:\n",
    "            return item['id']\n",
    "    \n",
    "    \n",
    "    def handle_origin(self, new, old = 'xxx'):\n",
    "        if len(new) == 1:\n",
    "            if new == 't':\n",
    "                return 't' + old[1:3]\n",
    "            elif new == 'a':\n",
    "                return old[0] + 'a' + old[2]\n",
    "            elif new == 'e':\n",
    "                return  old[0:2] + 'e'\n",
    "            \n",
    "        elif len(new) == 3:\n",
    "            return new\n",
    "        \n",
    "    \n",
    "    #Почему-то не работает, использовать cell, который будет ниже\n",
    "    def print_slang_vocab(self, model):\n",
    "        synonyms_cnt = 30\n",
    "        with jl.open('slang_vocab.txt', 'w') as fd:\n",
    "            for item in self.vocab:\n",
    "                if item['origin'] in ['tax', 'xax'] and item['cnt'] > 10:\n",
    "                    synonyms = model.most_similar_cosmul(positive = [str(item['id'])], topn = synonyms_cnt)\n",
    "                    suitable_syns = [s for s in synonyms if self.word_by_id(int(s[0]), return_val = 'item')['origin'] in\n",
    "                                   ['txx'] and \n",
    "                                   self.word_by_id(int(s[0]), return_val = 'item')['tags'] == item['tags'] and \n",
    "                                   not (self.word_by_id(int(s[0])) == 'не' + item['normal_form'] or\n",
    "                                   'не' + self.word_by_id(int(s[0])) == item['normal_form'] or\n",
    "                                   self.word_by_id(int(s[0])) == 'без' + item['normal_form'] or\n",
    "                                   'без' + self.word_by_id(int(s[0])) == item['normal_form'])]\n",
    "                    if(len(suitable_syns) > 0):\n",
    "                        line = item['normal_form'] + ' - '\n",
    "                        syns = [self.word_by_id(int(s[0])) for s in suitable_syns]\n",
    "                        line += ', '.join(syns)\n",
    "                        #print(line)\n",
    "                        fd.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nlemmatized_txt = open(\\'lemmatized.txt\\', \\'w\\', encoding=\\'utf8\\')\\n\\nwith open(\\'learn_scrapy\\\\pda.jl\\', \\'rb\\') as f:\\n    \\n    vocab = FullVocab()\\n    num_comments = 100617\\n    counter = 0\\n    prev_counter = 0\\n    anchor_counter = 0\\n    checkpoint = tm.clock()\\n    dt = 900\\n    \\n    for item in jl.reader(f):\\n        item_origin = None    #Сюда пишем \\'t\\', если comment, \\'a\\', если article\\n        sents = None\\n        \\n        if item.get(\\'comment\\') is not None:\\n            sents = nltk.sent_tokenize(item[\\'comment\\'])\\n            item_origin = \\'t\\'\\n        elif item.get(\\'article\\') is not None:   \\n            sents = nltk.sent_tokenize(item[\\'article\\'])\\n            item_origin = \\'a\\'\\n            \\n        for sent in sents:\\n            toks = tokenizer.tokenize(sent)\\n            toks = [t.lower() for t in toks if t.isalnum()]\\n            sent = \\' \\'.join(toks)\\n            \\n            try:\\n                lemma_annotate = isp_api.lemmatizationAnnotate(text = sent)\\n                pos_annotate = isp_api.posTaggingAnnotate(text = sent)\\n                \\n            except KeyboardInterrupt:\\n                print(\"Understandable, have a nice day\")\\n                vocab.build_xml()\\n                lemmatized_txt.close()\\n                sys.exit()\\n                \\n            except:\\n                print(\"Sentence\", end = \" \"\")\\n                print(sent, end = \"\" \")\\n                print(\"in commentary #\", end = \"\")\\n                print(counter, end = \" \")\\n                print(\"not processed\")\\n                print(\"\\n\")\\n                continue\\n                \\n            if lemma_annotate.get(\\'annotations\\').get(\\'lemma\\') is not None:\\n                lemmatized = []\\n                for i in range(len(lemma_annotate.get(\\'annotations\\').get(\\'lemma\\'))):\\n                    l = lemma_annotate.get(\\'annotations\\').get(\\'lemma\\')[i].get(\\'value\\')\\n                    p = pos_annotate[\\'annotations\\'][\\'pos-token\\'][i][\\'value\\'].get(\\'tag\\')\\n                    word_id = vocab.add_item(toks[i], l, p, item_origin)\\n                    lemmatized.append(str(word_id))\\n                \\n                lemmatized = \\' \\'.join(lemmatized)\\n                lemmatized_txt.write(lemmatized + \\'\\n\\')\\n        \\n        counter = counter + 1\\n        if(tm.clock() - checkpoint > dt):\\n            print(\"Progress: \", end = \\'\\')\\n            print(counter / num_comments * 100, end = \\'\\')\\n            print(\"%\")\\n            print(\"Comments processed by turn: \", end= \\'\\')\\n            print(counter - prev_counter)\\n            print(\"Comments processed total: \", end= \\'\\')\\n            print(counter)\\n            print(\"Time left: \", end = \\'\\')\\n            speed = (counter - prev_counter) / dt\\n            print(speed * (num_comments - counter))\\n            print(\"Speed: \", end = \\'\\')\\n            print(speed, end = \\' \\')\\n            print(\"comments/sec\")\\n            print(\"\\n\")\\n            prev_counter = counter\\n            vocab.build_xml()\\n            if (counter - anchor_counter) / num_comments * 100 > 25:\\n                print(\"Sleeping for 20 minutes\")\\n                tm.sleep(1200)\\n                anchor_counter = counter\\n            checkpoint = tm.clock()\\n            \\n    vocab.build_xml()\\n    print(\"Finished\")\\n    \\nlemmatized_txt.close()\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Раскоментить, чтобы составить словарь и файл с лемматизированным текстом из скаченных scrapy файлов\n",
    "'''\n",
    "lemmatized_txt = open('lemmatized.txt', 'w', encoding='utf8')\n",
    "\n",
    "with open('learn_scrapy\\pda.jl', 'rb') as f:\n",
    "    \n",
    "    vocab = FullVocab()\n",
    "    num_comments = 100617\n",
    "    counter = 0\n",
    "    prev_counter = 0\n",
    "    anchor_counter = 0\n",
    "    checkpoint = tm.clock()\n",
    "    dt = 900\n",
    "    \n",
    "    for item in jl.reader(f):\n",
    "        item_origin = None    #Сюда пишем 't', если comment, 'a', если article\n",
    "        sents = None\n",
    "        \n",
    "        if item.get('comment') is not None:\n",
    "            sents = nltk.sent_tokenize(item['comment'])\n",
    "            item_origin = 't'\n",
    "        elif item.get('article') is not None:   \n",
    "            sents = nltk.sent_tokenize(item['article'])\n",
    "            item_origin = 'a'\n",
    "            \n",
    "        for sent in sents:\n",
    "            toks = tokenizer.tokenize(sent)\n",
    "            toks = [t.lower() for t in toks if t.isalnum()]\n",
    "            sent = ' '.join(toks)\n",
    "            \n",
    "            try:\n",
    "                lemma_annotate = isp_api.lemmatizationAnnotate(text = sent)\n",
    "                pos_annotate = isp_api.posTaggingAnnotate(text = sent)\n",
    "                \n",
    "            except KeyboardInterrupt:\n",
    "                print(\"Understandable, have a nice day\")\n",
    "                vocab.build_xml()\n",
    "                lemmatized_txt.close()\n",
    "                sys.exit()\n",
    "                \n",
    "            except:\n",
    "                print(\"Sentence\", end = \" \\\"\")\n",
    "                print(sent, end = \"\\\" \")\n",
    "                print(\"in commentary #\", end = \"\")\n",
    "                print(counter, end = \" \")\n",
    "                print(\"not processed\")\n",
    "                print(\"\\n\")\n",
    "                continue\n",
    "                \n",
    "            if lemma_annotate.get('annotations').get('lemma') is not None:\n",
    "                lemmatized = []\n",
    "                for i in range(len(lemma_annotate.get('annotations').get('lemma'))):\n",
    "                    l = lemma_annotate.get('annotations').get('lemma')[i].get('value')\n",
    "                    p = pos_annotate['annotations']['pos-token'][i]['value'].get('tag')\n",
    "                    word_id = vocab.add_item(toks[i], l, p, item_origin)\n",
    "                    lemmatized.append(str(word_id))\n",
    "                \n",
    "                lemmatized = ' '.join(lemmatized)\n",
    "                lemmatized_txt.write(lemmatized + '\\n')\n",
    "        \n",
    "        counter = counter + 1\n",
    "        if(tm.clock() - checkpoint > dt):\n",
    "            print(\"Progress: \", end = '')\n",
    "            print(counter / num_comments * 100, end = '')\n",
    "            print(\"%\")\n",
    "            print(\"Comments processed by turn: \", end= '')\n",
    "            print(counter - prev_counter)\n",
    "            print(\"Comments processed total: \", end= '')\n",
    "            print(counter)\n",
    "            print(\"Time left: \", end = '')\n",
    "            speed = (counter - prev_counter) / dt\n",
    "            print(speed * (num_comments - counter))\n",
    "            print(\"Speed: \", end = '')\n",
    "            print(speed, end = ' ')\n",
    "            print(\"comments/sec\")\n",
    "            print(\"\\n\")\n",
    "            prev_counter = counter\n",
    "            vocab.build_xml()\n",
    "            if (counter - anchor_counter) / num_comments * 100 > 25:\n",
    "                print(\"Sleeping for 20 minutes\")\n",
    "                tm.sleep(1200)\n",
    "                anchor_counter = counter\n",
    "            checkpoint = tm.clock()\n",
    "            \n",
    "    vocab.build_xml()\n",
    "    print(\"Finished\")\n",
    "    \n",
    "lemmatized_txt.close()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2vec модель из gensim\n",
    "Ищу синонимы с помощью метода most_similar() и подставляю в изначальный текст"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab finished\n"
     ]
    }
   ],
   "source": [
    "#Загружаю словарь из файла\n",
    "new_vocab = FullVocab(from_file = True)\n",
    "print('Vocab finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building models completed\n"
     ]
    }
   ],
   "source": [
    "#Строю модель из лемматизированных комментариев и/или статей\n",
    "w2v_model = 0\n",
    "w2v_model = gensim.models.Word2Vec(gensim.models.word2vec.LineSentence('lemmatized.txt'), min_count=10)\n",
    "print('Building models completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Запустить, чтобы записать в файлик slang_vocab.txt сленговый словарь в виде \"слово из статей\" - [\"список сленговых синонимов\"]\n",
    "synonyms_cnt = 30\n",
    "with jl.open('slang_vocab.txt', 'w') as fd:\n",
    "    for item in new_vocab.vocab:\n",
    "        if item['origin'] in ['tax', 'xax'] and item['cnt'] > 10 and item['normal_form'] is not None:\n",
    "            synonyms = w2v_model.most_similar_cosmul(positive = [str(item['id'])], topn = synonyms_cnt)\n",
    "            suitable_syns = [s for s in synonyms if new_vocab.word_by_id(int(s[0]), return_val = 'item')['origin'] in\n",
    "                           ['txx'] and \n",
    "                           new_vocab.word_by_id(int(s[0]), return_val = 'item')['tags'] == item['tags'] and \n",
    "                           not (new_vocab.word_by_id(int(s[0])) == 'не' + item['normal_form'] or\n",
    "                           'не' + new_vocab.word_by_id(int(s[0])) == item['normal_form'] or\n",
    "                           new_vocab.word_by_id(int(s[0])) == 'без' + item['normal_form'] or\n",
    "                           'без' + new_vocab.word_by_id(int(s[0])) == item['normal_form'])]\n",
    "            if(len(suitable_syns) > 0):\n",
    "                line = item['normal_form'] + ' - '\n",
    "                syns = [new_vocab.word_by_id(int(s[0])) for s in suitable_syns]\n",
    "                line += ', '.join(syns)\n",
    "                #print(line)\n",
    "                fd.write(line)\n",
    "\n",
    "#new_vocab.print_slang_vocab(w2v_model)\n",
    "print(\"Finished building slang vocabulary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iPad Mini 4 стал логическим продолжением популярной линейки «яблочных» планшетов, которые нашли применение в самых разных сферах.\n",
      "iPad Mini 4 стал логическим продолжением популярной линейки « яблочных » клав , которые глядывали применение в самых разных сферах .\n",
      "\n",
      "\n",
      "Развлекательное устройство, «читалка», автомобильный навигатор, рабочий инструмент — компактный и лёгкий планшет стал настоящим хитом для тех, кто всюду носит с собой гаджет и нуждается в большем экране, чем у смартфона или смартпэда.\n",
      "Развлекательное устройство , « читалка » , автомобильный навигатор , рабочий инструмент — компактный и лёгкий клава стал настоящим хитом для тех , кто всюду тягает с собой гаджет и нуждается в большем амольде , чем у смарта или смартпэда .\n",
      "\n",
      "\n",
      "Между тем рынок чутко отреагировал на запросы пользователей и наполнился множеством самых разных аксессуаров, среди которых чехлы, которые позволяют ронять iPad Mini 4 без вреда для корпуса, закалённые стекла и целая плеяда полезных подставок на любой случай жизни.\n",
      "Между тем рынок чутко отреагировал на запросы юзеров и наполнился множеством самых разных аксессуаров , среди которых чехлы , которые позволяют ронять iPad Mini 4 без вреда для корпуса , закалённые стекла и целая плеяда полезных подставок на любой случай жизни .\n",
      "\n",
      "\n",
      "Как и в случае со смартфонами, защита дисплея планшета является обязательной задачей, поскольку мобильное устройство часто используется на ходу и рискует оказаться на полу или случайно удариться о другой твёрдый предмет в сумке.\n",
      "Как и в случае со смартами , защита дисплея клавы является обязательной задачей , поскольку мобильное устройство часто используется на телеграму и рискует оказаться на совку или случайно удариться о другой твёрдый предмет в сумке .\n",
      "\n",
      "\n",
      "iPad Mini 4 прикрыт стандартным плоским стеклом, которое сравнительно неплохо противостоит появлению мелких царапин, но не обладает противоударными свойствами.\n",
      "iPad Mini 4 прикрыт стандартным плоским стеклом , которое сравнительно шикарно противостоит появлению мелких царапин , но не обладает противоударными свойствами .\n",
      "\n",
      "\n",
      "Floveme предлагает интересное решение в виде усиленного стекла для защиты от ударов и падений — в результате несчастного случая весь удар и избыточную нагрузку принимает на себя минеральный протектор, а не родное стекло планшета.\n",
      "Floveme предлагает интересное решение в виде усиленного стекла для защиты от ударов и падений — в результате несчастного случая весь удар и избыточную нагрузку принимает на себя минеральный протектор , а не родное стекло клавы .\n",
      "\n",
      "\n",
      "При этом сохраняются все сильные стороны экрана iPad Mini 4, включая высокая яркость и контрастность.\n",
      "При этом сохраняются все сильные стороны амольда iPad Mini 4 , включая высокая яркость и контрастность .\n",
      "\n",
      "\n",
      "Несмотря на толщину 0,4 мм стекло полностью совместимо со всеми защитными чехлами.\n",
      "Несмотря на толщину 0,4 мм стекло полностью совместимо со всеми защитными чехлами .\n",
      "\n",
      "\n",
      "Тонкий и гладкий корпус iPad Mini 4 приятно лежит в руке и холодит руку даже в самую жаркую погоду.\n",
      "Тонкий и гладкий корпус iPad Mini 4 приятно подержит в руке и холодит руку даже в самую жаркую погоду .\n",
      "\n",
      "\n",
      "Защитить заднюю панель от постоянного нахождения на столе или в сумке помогут многочисленные защитные кейсы, среди которых присутствуют тонкие решения и настоящие противоударные монстры.\n",
      "Защитить заднюю панель от постоянного нахождения на виртуалке или в сумке лечат многочисленные защитные кейсы , среди которых присутствуют тонкие решения и настоящие противоударные монстры .\n",
      "\n",
      "\n",
      "Классический и крайне популярный чехол повторяет дизайн официального Smart Cover, но предлагает несколько актуальных модификаций.\n",
      "Классический и крайне популярный чехол хейтеет мейза официального Smart Cover , но предлагает несколько актуальных модификаций .\n",
      "\n",
      "\n",
      "Например, задняя крышка из противоскользящего силикона и плотная передняя крышка, которая складывается в удобную подставку для работы под комфортным для чтения и набора текста углом.\n",
      "\n",
      "Например , задняя крышка из противоскользящего силикона и плотная передняя крышка , которая складывается в удобную подставку для серфинга под комфортным для серфинга и набора желтизны углом .\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2065"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Получаю наилучший вариант из pymorphy2.parse\n",
    "def get_propper_parse(word, word_forms):\n",
    "    word_parse = morph.parse(word)\n",
    "    ranks = [0] * len(word_parse)\n",
    "    for i, p in enumerate(word_parse):\n",
    "        lexemas = [w.word for w in p.lexeme]\n",
    "        for w in word_forms:\n",
    "            if w in lexemas:\n",
    "                ranks[i] += 1\n",
    "\n",
    "    max_index = ranks.index(max(ranks))\n",
    "    return word_parse[max_index]\n",
    "\n",
    "\n",
    "#Спрягаю word по подобию template\n",
    "def inflect_by_template(word, template):\n",
    "    grammemes = sorted(template.tag.grammemes)\n",
    "    for g in grammemes:\n",
    "        w = word.inflect({g})\n",
    "        if w is not None:\n",
    "            word = w\n",
    "\n",
    "    return word\n",
    "\n",
    "\n",
    "#Принимаем решение, пропускаем ли синоним\n",
    "def make_decision(suitable_syns, word_item, cnt):\n",
    "    decision = False\n",
    "    freq = int(word_item['cnt']) / cnt\n",
    "    first_proximity = 0.79\n",
    "    second_proximity = 0.75\n",
    "    allowed_freq_min = 100 / cnt\n",
    "    allowed_freq_max = 20000 / cnt\n",
    "    if freq < allowed_freq_max and freq > allowed_freq_min and suitable_syns[0][1] > first_proximity:\n",
    "        decision = True\n",
    "\n",
    "    return decision\n",
    "\n",
    "\n",
    "#Основная функия, принимаем на вход файл со статьёй, разбиваем на предложения и подбираем сленговые синонимы\n",
    "def chiki_briki(filename):\n",
    "    synonyms_cnt = 30\n",
    "    \n",
    "    target_article_txt = open(filename, 'r', encoding='utf8')\n",
    "    target_article = target_article_txt.read()\n",
    "    ta_sents = nltk.sent_tokenize(target_article)\n",
    "    new_article = ''\n",
    "    for sent in ta_sents:\n",
    "        la = isp_api.lemmatizationAnnotate(text = sent)\n",
    "        lp = isp_api.posTaggingAnnotate(text = sent)\n",
    "        \n",
    "        comparer = []\n",
    "        new_sent = []       \n",
    "        for i in range(len(la['annotations']['lemma'])):\n",
    "            l = la['annotations']['lemma'][i]\n",
    "            p = lp['annotations']['pos-token'][i]\n",
    "            word_id = str(new_vocab.id_by_word(l['value'], tag = p['value']['tag']))\n",
    "            comparer.append({'text': l['text'], 'norma': l['value'], 'tag': p['value']['tag'], 'word_id': word_id})\n",
    "        \n",
    "        for i, c in enumerate(comparer):\n",
    "            if c['tag'] in ['PR', 'PART', 'CONJ'] or not c['text'].isalnum():\n",
    "                new_sent.append(c['text'])\n",
    "                continue\n",
    "                \n",
    "            word_id = new_vocab.id_by_word(c['norma'], tag = c['tag'])\n",
    "            word_item = new_vocab.word_by_id(word_id, return_val = 'item')\n",
    "            word_norm = new_vocab.word_by_id(word_id)\n",
    "            if word_id is None or int(word_item['cnt']) < 10:\n",
    "                new_sent.append(c['text'])\n",
    "                continue\n",
    "                \n",
    "            #print(word_item['normal_form'] + ' - ' + word_item['origin'])\n",
    "            ''' \n",
    "            if word_item['origin'] is not 'xax':\n",
    "                new_sent.append(c['text'])\n",
    "                continue\n",
    "            '''\n",
    "            #new_sent.append(c['norma'])\n",
    "            synonyms = w2v_model.most_similar_cosmul(positive = [str(word_id)], topn = synonyms_cnt)\n",
    "            '''\n",
    "            print(word_item['normal_form'])\n",
    "            for s in synonyms:\n",
    "                print(new_vocab.word_by_id(int(s[0])), end = \" - \")\n",
    "                print(s[1])\n",
    "            '''\n",
    "            \n",
    "            suitable_syns = [s for s in synonyms if new_vocab.word_by_id(int(s[0]), return_val = 'item')['origin'] in\n",
    "                           ['txx'] and \n",
    "                           new_vocab.word_by_id(int(s[0]), return_val = 'item')['tags'] == c['tag'] and \n",
    "                           not (new_vocab.word_by_id(int(s[0])) == 'не' + word_norm or\n",
    "                           'не' + new_vocab.word_by_id(int(s[0])) == word_norm or\n",
    "                           new_vocab.word_by_id(int(s[0])) == 'без' + word_norm or\n",
    "                           'без' + new_vocab.word_by_id(int(s[0])) == word_norm)]\n",
    "            lemma = [w['word_id'] for w in comparer]\n",
    "            '''\n",
    "            ranked_syns = []\n",
    "            for s in suitable_syns:\n",
    "                lemma[i] = s[0]\n",
    "                rank = (w2v_model.score([lemma])) / 100 \n",
    "                ranked_syns.append({'syn': s[0], 'rank': rank[0]})\n",
    "            \n",
    "            ranked_syns = sorted(ranked_syns, key=lambda s: s['rank'], reverse=True)\n",
    "            '''\n",
    "            \n",
    "            #print(c['norma'])\n",
    "            #print(suitable_syns)\n",
    "            #print(ranked_syns)\n",
    "            \n",
    "            if len(suitable_syns) == 0:  \n",
    "                new_sent.append(c['text'])\n",
    "                continue\n",
    "            \n",
    "            if  make_decision(suitable_syns, word_item, new_vocab.id_counter):\n",
    "                synonym = suitable_syns[0][0]\n",
    "                synonym_item = new_vocab.word_by_id(int(suitable_syns[0][0]), return_val = 'item')\n",
    "                \n",
    "                synonym_parse = get_propper_parse(synonym_item['normal_form'], synonym_item['word_forms'])\n",
    "                word_parse = get_propper_parse(c['text'], word_item['word_forms'])\n",
    "                synonym_parse = inflect_by_template(synonym_parse, word_parse)\n",
    "                \n",
    "                word_to_substitute = synonym_parse.word\n",
    "                #Первое слово в предожении с заглавной буквы\n",
    "                if i == 0:\n",
    "                    word_to_substitute = word_to_substitute.title()\n",
    "                    \n",
    "                new_sent.append(word_to_substitute)\n",
    "            #elif suitable_syns[0][1] > second_proximity: \n",
    "                #optionnal_syn = '(' + new_vocab.word_by_id(int(ranked_syns[0]['syn'])) + ')'\n",
    "                #new_sent.append(optionnal_syn)\n",
    "            else:  \n",
    "                new_sent.append(c['text'])\n",
    "                continue\n",
    "                \n",
    "        \n",
    "        print(sent)\n",
    "        new_sent = ' '.join(new_sent)\n",
    "        print(new_sent)\n",
    "        new_article += new_sent + ' '\n",
    "        print('\\n')\n",
    "            \n",
    "        '''\n",
    "            t_id = new_vocab.id_by_word(t)\n",
    "            \n",
    "            \n",
    "            if t_id is not None and int(new_vocab.word_by_id(t_id, return_val = 'item')['cnt']) > 10:\n",
    "                lst = w2v_model.most_similar(positive = [str(t_id)])\n",
    "                new_t = new_vocab.word_by_id(int(lst[0][0]), return_val = 'item')\n",
    "                if new_t['origin'] == 'tax' or new_t['origin'] == 'txx':\n",
    "                    print(new_t['normal_form'], end = ' ')\n",
    "                else:\n",
    "                    print(t, end = ' ')\n",
    "        \n",
    "            else:\n",
    "                print(t, end = ' ')\n",
    "        '''\n",
    "    return new_article\n",
    "    \n",
    "\n",
    "new_article = chiki_briki('target_article2.txt')\n",
    "new_article_txt = open('new_article.txt', 'w', encoding='utf8')\n",
    "new_article_txt.write(new_article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# НИЖЕ ЧЕРНОВИК, СМОТРЕТЬ ТУДА НЕ НУЖНО"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s8 - 0.7711077332496643\n",
      "active - 0.7300979495048523\n",
      "s6 - 0.7240570187568665\n",
      "s3 - 0.7103427052497864\n",
      "s9 - 0.7008739709854126\n",
      "s7 - 0.6820350289344788\n",
      "samsung - 0.677720844745636\n",
      "galaxy - 0.6628284454345703\n",
      "c - 0.6439862251281738\n",
      "флагманский - 0.6419556736946106\n"
     ]
    }
   ],
   "source": [
    "lst = w2v_model.most_similar(positive = [str(new_vocab.id_by_word('samsung'))])\n",
    "for l in lst:\n",
    "    print(new_vocab.word_by_id(int(l[0])), end = \" - \")\n",
    "    print(l[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2034\n",
      "2365\n",
      "0.915373687744\n",
      "[('2151', 0.8232387900352478), ('2367', 0.7768270969390869), ('2380', 0.7505201101303101), ('1583', 0.7477496862411499), ('1344', 0.7444450855255127), ('26640', 0.7400096654891968), ('453', 0.7385299801826477), ('2021', 0.7321900129318237), ('3844', 0.7259445190429688), ('7477', 0.724348783493042)]\n",
      "87557\n",
      "0.011421131377274232\n",
      "0.11421131377274232\n"
     ]
    }
   ],
   "source": [
    "print(new_vocab.id_by_word('samsung'))\n",
    "print(new_vocab.id_by_word('galaxy'))\n",
    "print(1 + w2v_model.score([[str(new_vocab.id_by_word('samsung')), str(new_vocab.id_by_word('mass'))]])[0] / 100)\n",
    "print(w2v_model.most_similar_cosmul(positive = ['256']))\n",
    "\n",
    "print(new_vocab.id_counter)\n",
    "allowed_freq_min = 1000 / new_vocab.id_counter\n",
    "allowed_freq_max = 10000 / new_vocab.id_counter\n",
    "print(allowed_freq_min)\n",
    "print(allowed_freq_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "comments = ''\n",
    "with open('learn_scrapy\\pda.jl', 'rb') as f:\n",
    "    for item in jl.reader(f):\n",
    "        comments = comments + item['comment']       #создаём один string на все комменты\n",
    "                                                    #скорее всего, будет логичнее для N-граммов каждое предложение\n",
    "                                                    #запоминать в один string и хранить как list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kek1\n",
      "Kek2\n",
      "Kek3\n",
      "Kek4\n"
     ]
    }
   ],
   "source": [
    "#тут токенизируем комментарии\n",
    "#есть подозрение, что я создаю слишком много сущностей \n",
    "\n",
    "tknzr = nltk.TweetTokenizer()\n",
    "print('Kek1')\n",
    "tokens = tknzr.tokenize(comments)\n",
    "print('Kek2')\n",
    "text = nltk.Text(tokens)\n",
    "print('Kek3')\n",
    "sents = nltk.sent_tokenize(comments)                         #строить n-граммную модель, возможно, буду \n",
    "print('Kek4')                                                             #анализируя предложения по-отдельности\n",
    "words = sorted([w.lower() for w in tokens if w.isalpha()])   #впоследствии буду отсекать наименее наиболее частотные слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10600020178435018\n"
     ]
    }
   ],
   "source": [
    "def lexical_diversity(text):\n",
    "    return len(set(text)) / len(text)\n",
    "\n",
    "print(lexical_diversity(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sent_tokens = list()                            #здесь храним предложения\n",
    "for i in range(len(sents)):\n",
    "    sent_tokens.append(tknzr.tokenize(sents[i]))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#сох нормальные формы слов\n",
    "morph = pm2.MorphAnalyzer()     \n",
    "vocab_normalized = sorted(set([morph.parse(w)[0].normalized for w in words]), key = lambda w: w.normal_form)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def progress(full, curr, checkpoint):\n",
    "    if curr / full * 100 > checkpoint:\n",
    "        print(curr / full * 100, end = '')\n",
    "        print('% complited')\n",
    "        return True\n",
    "    \n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "i = 0\n",
    "h = 0.1\n",
    "checkpoint = h\n",
    "time = tm.clock()\n",
    "for sent in sents:\n",
    "    if progress(len(sents), i, checkpoint):\n",
    "        time_per_checkpoint = tm.clock() - time\n",
    "        print('Time per checkpoint:', end = ' ')\n",
    "        print(time_per_checkpoint)\n",
    "        print('Time per sentence:', end = ' ')\n",
    "        time_per_sent = time_per_checkpoint / (len(sents) * h)\n",
    "        print(time_per_sent)\n",
    "        print('Estimated time:', end = ' ')\n",
    "        print(time_per_checkpoint * (100 - checkpoint))\n",
    "        time = tm.clock()\n",
    "        checkpoint = checkpoint + h\n",
    "        \n",
    "    toks = tknzr.tokenize(sent)\n",
    "    toks = [t.lower() for t in toks if t.isalnum()]\n",
    "    sentence = ''\n",
    "    for t in toks:\n",
    "        sentence = sentence + t + ' '\n",
    "    \n",
    "    lemma_raw = isp_api.lemmatizationAnnotate(text = sentence)\n",
    "    lemmatized = ''\n",
    "    if lemma_raw.get('annotations').get('lemma') is not None:\n",
    "        for l in lemma_raw.get('annotations').get('lemma'):\n",
    "            lemmatized = lemmatized + l.get('value') + ' '\n",
    "        \n",
    "        sents[i] = lemmatized\n",
    "    \n",
    "    else:\n",
    "        sents[i] = ''\n",
    "        \n",
    "    i = i + 1\n",
    "        \n",
    "    \n",
    "sents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sents' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-e8335ea762b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'lemmatized.txt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf8'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msents\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m5000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sents' is not defined"
     ]
    }
   ],
   "source": [
    "with open('lemmatized.txt', 'w', encoding='utf8') as f:\n",
    "    for s in sents[0:5000]:\n",
    "        f.write(s + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec(gensim.models.word2vec.LineSentence('lemmatized.txt'), min_count=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('а', 0.9998720288276672),\n",
       " ('они', 0.9998717904090881),\n",
       " ('где', 0.9998696446418762),\n",
       " ('этот', 0.9998689293861389),\n",
       " ('мочь', 0.9998672604560852),\n",
       " ('когда', 0.999864935874939),\n",
       " ('это', 0.9998648166656494),\n",
       " ('там', 0.9998632669448853),\n",
       " ('с', 0.9998587965965271),\n",
       " ('ничто', 0.9998581409454346)]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive = ['айфон'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "сейчас\n",
      "{'value': {'characters': [], 'tag': 'ADV', 'type': 'syn-tag-rus'}, 'start': 0, 'text': 'сейчас', 'end': 6, 'annotated-text': 'сейчас я говорю о том человеке который спит отдыхая на работе'}\n",
      "[Parse(word='сейчас', tag=OpencorporaTag('ADVB'), normal_form='сейчас', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'сейчас', 3, 0),))]\n",
      "\n",
      "\n",
      "я\n",
      "{'value': {'characters': [{'tag': 'Masculine', 'type': 'gender'}, {'tag': 'DEICTIC', 'type': 'pronoun'}, {'tag': 'Singular', 'type': 'number'}, {'tag': 'Nominative', 'type': 'case'}, {'tag': 'Animated', 'type': 'animacy'}], 'tag': 'S', 'type': 'syn-tag-rus'}, 'start': 7, 'text': 'я', 'end': 8, 'annotated-text': 'сейчас я говорю о том человеке который спит отдыхая на работе'}\n",
      "[Parse(word='я', tag=OpencorporaTag('NPRO,1per sing,nomn'), normal_form='я', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'я', 3100, 0),))]\n",
      "\n",
      "\n",
      "говорить\n",
      "{'value': {'characters': [{'tag': 'First', 'type': 'person'}, {'tag': 'Singular', 'type': 'number'}, {'tag': 'NotPast', 'type': 'tense'}, {'tag': 'Indicative', 'type': 'mode'}], 'tag': 'V', 'type': 'syn-tag-rus'}, 'start': 9, 'text': 'говорю', 'end': 15, 'annotated-text': 'сейчас я говорю о том человеке который спит отдыхая на работе'}\n",
      "[Parse(word='говорить', tag=OpencorporaTag('INFN,impf,tran'), normal_form='говорить', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'говорить', 395, 0),))]\n",
      "\n",
      "\n",
      "о\n",
      "{'value': {'characters': [], 'tag': 'PR', 'type': 'syn-tag-rus'}, 'start': 16, 'text': 'о', 'end': 17, 'annotated-text': 'сейчас я говорю о том человеке который спит отдыхая на работе'}\n",
      "[Parse(word='о', tag=OpencorporaTag('PREP'), normal_form='о', score=0.990985, methods_stack=((<DictionaryAnalyzer>, 'о', 2133, 0),)), Parse(word='о', tag=OpencorporaTag('INTJ'), normal_form='о', score=0.009014, methods_stack=((<DictionaryAnalyzer>, 'о', 21, 0),))]\n",
      "\n",
      "\n",
      "то\n",
      "{'value': {'characters': [{'tag': 'Neuter', 'type': 'gender'}, {'tag': 'Prepositional', 'type': 'case'}, {'tag': 'Singular', 'type': 'number'}, {'tag': 'Inanimated', 'type': 'animacy'}], 'tag': 'S', 'type': 'syn-tag-rus'}, 'start': 18, 'text': 'том', 'end': 21, 'annotated-text': 'сейчас я говорю о том человеке который спит отдыхая на работе'}\n",
      "[Parse(word='то', tag=OpencorporaTag('CONJ'), normal_form='то', score=0.5, methods_stack=((<DictionaryAnalyzer>, 'то', 20, 0),)), Parse(word='то', tag=OpencorporaTag('PRCL'), normal_form='то', score=0.1, methods_stack=((<DictionaryAnalyzer>, 'то', 22, 0),))]\n",
      "\n",
      "\n",
      "человек\n",
      "{'value': {'characters': [{'tag': 'Masculine', 'type': 'gender'}, {'tag': 'Prepositional', 'type': 'case'}, {'tag': 'Singular', 'type': 'number'}, {'tag': 'Animated', 'type': 'animacy'}], 'tag': 'S', 'type': 'syn-tag-rus'}, 'start': 22, 'text': 'человеке', 'end': 30, 'annotated-text': 'сейчас я говорю о том человеке который спит отдыхая на работе'}\n",
      "[Parse(word='человек', tag=OpencorporaTag('NOUN,anim,masc sing,nomn'), normal_form='человек', score=0.549382, methods_stack=((<DictionaryAnalyzer>, 'человек', 488, 0),)), Parse(word='человек', tag=OpencorporaTag('NOUN,anim,masc sing,nomn'), normal_form='человек', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'человек', 488, 0),))]\n",
      "\n",
      "\n",
      "который\n",
      "{'value': {'characters': [{'tag': 'Masculine', 'type': 'gender'}, {'tag': 'Nominative', 'type': 'case'}, {'tag': 'Singular', 'type': 'number'}], 'tag': 'S', 'type': 'syn-tag-rus'}, 'start': 31, 'text': 'который', 'end': 38, 'annotated-text': 'сейчас я говорю о том человеке который спит отдыхая на работе'}\n",
      "[Parse(word='который', tag=OpencorporaTag('ADJF,Apro,Subx,Anph masc,sing,nomn'), normal_form='который', score=0.823529, methods_stack=((<DictionaryAnalyzer>, 'который', 1788, 0),)), Parse(word='который', tag=OpencorporaTag('ADJF,Apro,Subx,Anph masc,sing,nomn'), normal_form='который', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'который', 1788, 0),))]\n",
      "\n",
      "\n",
      "спать\n",
      "{'value': {'characters': [{'tag': 'Third', 'type': 'person'}, {'tag': 'Singular', 'type': 'number'}, {'tag': 'NotPast', 'type': 'tense'}, {'tag': 'Indicative', 'type': 'mode'}], 'tag': 'V', 'type': 'syn-tag-rus'}, 'start': 39, 'text': 'спит', 'end': 43, 'annotated-text': 'сейчас я говорю о том человеке который спит отдыхая на работе'}\n",
      "[Parse(word='спать', tag=OpencorporaTag('INFN,impf,intr'), normal_form='спать', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'спать', 2808, 0),))]\n",
      "\n",
      "\n",
      "отдыхать\n",
      "{'value': {'characters': [{'tag': 'Gerund', 'type': 'representation'}, {'tag': 'NotPast', 'type': 'tense'}], 'tag': 'V', 'type': 'syn-tag-rus'}, 'start': 44, 'text': 'отдыхая', 'end': 51, 'annotated-text': 'сейчас я говорю о том человеке который спит отдыхая на работе'}\n",
      "[Parse(word='отдыхать', tag=OpencorporaTag('INFN,impf,intr'), normal_form='отдыхать', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'отдыхать', 2247, 0),))]\n",
      "\n",
      "\n",
      "на\n",
      "{'value': {'characters': [], 'tag': 'PR', 'type': 'syn-tag-rus'}, 'start': 52, 'text': 'на', 'end': 54, 'annotated-text': 'сейчас я говорю о том человеке который спит отдыхая на работе'}\n",
      "[Parse(word='на', tag=OpencorporaTag('PREP'), normal_form='на', score=0.99931, methods_stack=((<DictionaryAnalyzer>, 'на', 24, 0),)), Parse(word='на', tag=OpencorporaTag('PRCL'), normal_form='на', score=0.000477, methods_stack=((<DictionaryAnalyzer>, 'на', 22, 0),)), Parse(word='на', tag=OpencorporaTag('INTJ'), normal_form='на', score=0.000212, methods_stack=((<DictionaryAnalyzer>, 'на', 21, 0),))]\n",
      "\n",
      "\n",
      "работа\n",
      "{'value': {'characters': [{'tag': 'Feminine', 'type': 'gender'}, {'tag': 'Prepositional', 'type': 'case'}, {'tag': 'Singular', 'type': 'number'}, {'tag': 'Inanimated', 'type': 'animacy'}], 'tag': 'S', 'type': 'syn-tag-rus'}, 'start': 55, 'text': 'работе', 'end': 61, 'annotated-text': 'сейчас я говорю о том человеке который спит отдыхая на работе'}\n",
      "[Parse(word='работа', tag=OpencorporaTag('NOUN,inan,femn sing,nomn'), normal_form='работа', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'работа', 55, 0),))]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i = 5\n",
    "\n",
    "message = 'сейчас я говорю о том человеке который спит отдыхая на работе'\n",
    "message_tokens = tokenizer.tokenize(message)\n",
    "parse_tokens = [morph.parse(t) for t in message_tokens]\n",
    "#print(parse_tokens[i])\n",
    "isp_POS = isp_api.posTaggingAnnotate(text = message)\n",
    "isp_lemma = isp_api.lemmatizationAnnotate(text = message)\n",
    "\n",
    "#print(isp_POS['annotations']['pos-token'][i])\n",
    "\n",
    "#print(isp_lemma.get('annotations').get('lemma')[i].get('value'))\n",
    "\n",
    "#print(' '.join([l.get('value') for l in isp_lemma.get('annotations').get('lemma')]))\n",
    "\n",
    "for j in range(len(message_tokens)):\n",
    "    isp_lemma_token = isp_lemma.get('annotations').get('lemma')[j].get('value')\n",
    "    morph_parse_token = [w.normalized for w in morph.parse(isp_lemma_token) if w.normal_form == w.word]\n",
    "    print(isp_lemma_token)\n",
    "    print(isp_POS['annotations']['pos-token'][j])\n",
    "    print(morph_parse_token)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "встать\n",
      "{'text': 'встать', 'annotations': {'lemma': [{'text': 'встать', 'value': 'вставать', 'annotated-text': 'встать', 'start': 0, 'end': 6}]}}\n",
      "[Parse(word='встать', tag=OpencorporaTag('INFN,perf,intr'), normal_form='встать', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'встать', 904, 0),))]\n",
      "\n",
      "\n",
      "вставать\n",
      "{'text': 'вставать', 'annotations': {'lemma': [{'text': 'вставать', 'value': 'вставать', 'annotated-text': 'вставать', 'start': 0, 'end': 8}]}}\n",
      "[Parse(word='вставать', tag=OpencorporaTag('INFN,impf,intr'), normal_form='вставать', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'вставать', 903, 0),))]\n",
      "\n",
      "\n",
      "встали\n",
      "{'text': 'встали', 'annotations': {'lemma': [{'text': 'встали', 'value': 'вставать', 'annotated-text': 'встали', 'start': 0, 'end': 6}]}}\n",
      "[Parse(word='встали', tag=OpencorporaTag('VERB,perf,intr plur,past,indc'), normal_form='встать', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'встали', 904, 4),))]\n",
      "\n",
      "\n",
      "вставали\n",
      "{'text': 'вставали', 'annotations': {'lemma': [{'text': 'вставали', 'value': 'вставать', 'annotated-text': 'вставали', 'start': 0, 'end': 8}]}}\n",
      "[Parse(word='вставали', tag=OpencorporaTag('VERB,impf,intr plur,past,indc'), normal_form='вставать', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'вставали', 903, 10),))]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ws = ['встать', 'вставать', 'встали', 'вставали']\n",
    "for w in ws:\n",
    "    print(w)\n",
    "    ns = morph.parse(w)\n",
    "    ts = isp_api.lemmatizationAnnotate(text = w)\n",
    "    print(ts)\n",
    "    print(ns)\n",
    "        \n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

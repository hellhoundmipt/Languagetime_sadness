{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Maxim\\Anaconda3\\lib\\site-packages\\gensim-2.2.0-py3.5-win-amd64.egg\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import json_lines as jl\n",
    "import nltk\n",
    "import xmltodict\n",
    "from ispras import texterra as isp\n",
    "import pymorphy2 as pm2\n",
    "import gensim\n",
    "import time as tm\n",
    "\n",
    "isp_api = isp.API('ba74236a7212a71054ae1408b30b1bdef771d35b')\n",
    "#nltk.download()\n",
    "\n",
    "tokenizer = nltk.TweetTokenizer()\n",
    "morph = pm2.MorphAnalyzer()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лемматизация с помощью Texterra\n",
    "\n",
    "Читаю комментарии из файла по очереди, разбиваю каждый на предложения, лемматизирую с помощью Texterra, составляя при этом словарь FullVocab. В словаре хранятся связи <u>\"нормальная форма\"</u> - <u>\"id нормальной формы\"</u> - <u>\"часть речи\"</u> - <u>\"встретившиеся варианты слова\"</u>. В lemmatized.txt записываются построчно лемматизированные предложения, но вместо слов в нём <u>\"id нормальной формы\"</u>, чтобы для word2vec слова \"стать\" (<i>глагол</i>) и \"стать\" (<i>сущ</i>) не были одним и тем же словом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FullVocab:\n",
    "    \n",
    "    #Инициализация, либо пустой класс, либо из файла vocab.xml\n",
    "    def __init__(self, from_file = \"no\"):\n",
    "        self.id_counter = 0\n",
    "        self.vocab = []\n",
    "        if from_file == 'yes':\n",
    "            f = open('vocab.xml', 'r')\n",
    "            root = etree.XML(f.read())\n",
    "            for item in root:\n",
    "                self.add_item(item[3].text.split(' '), item[1].text, item[2].text)\n",
    "            \n",
    "        \n",
    "    #Добавляем новое слово в словарь\n",
    "    def add_item(self, words, normal_form, tags_normal):\n",
    "        \n",
    "        #Дабы было можно и str и list в аргументы добавлять\n",
    "        if type(words) is not list:\n",
    "            words = [words]\n",
    "            \n",
    "        #Проверяем, есть ли нормальная форма слова в словаре. Если есть, то обновляем словоформы, если нет, то \n",
    "        #добавляем новую запись в словарь, присвивая уникальный id\n",
    "        for item in self.vocab:\n",
    "            if item.get('normal_form') == normal_form and item.get('tags') == tags_normal:\n",
    "                item['word_forms'].extend(words)\n",
    "                item['word_forms'] = sorted(set(item['word_forms']))\n",
    "                return item['id']\n",
    "                \n",
    "        new_item = {'id': self.id_counter, 'normal_form': normal_form, 'tags': tags_normal, 'word_forms': words}\n",
    "        self.vocab.append(new_item)\n",
    "        self.id_counter = self.id_counter + 1\n",
    "        return self.id_counter - 1\n",
    "            \n",
    "    def show_vocab(self):\n",
    "        print(self.vocab)\n",
    "        \n",
    "    #Сохраняем копию данные класса в vocab.xml\n",
    "    def build_xml(self):\n",
    "        f = open('vocab.xml', 'w')\n",
    "        root = etree.Element('root')\n",
    "        for item in self.vocab:\n",
    "            child = etree.SubElement(root, 'w' + str(item['id']))\n",
    "            etree.SubElement(child, 'id').text = str(item['id'])\n",
    "            etree.SubElement(child, 'normal_form').text = item['normal_form']\n",
    "            etree.SubElement(child, 'tags').text = item['tags']\n",
    "            etree.SubElement(child, 'word_forms'). text = ' '.join(item['word_forms'])\n",
    "            \n",
    "        f.write(etree.tostring(root, encoding='unicode', method='xml', pretty_print=True))\n",
    "        \n",
    "    #Ищем нормальную форму или запись в словаре по id\n",
    "    def word_by_id(self, word_id, return_val = \"word\"):\n",
    "        item = next((item for item in self.vocab if item[\"id\"] == word_id))\n",
    "        if return_val == \"word\":\n",
    "            return item[\"normal_form\"]\n",
    "        elif return_val == \"item\":\n",
    "            return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 0.33397262085840884%\n",
      "Comments processed: 101\n",
      "Speed: 1.3\n",
      " comments/sec\n",
      "Time left: 30442.41\n",
      "\n",
      "\n",
      "Progress: 0.7241584551286291%\n",
      "Comments processed: 118\n",
      "Speed: 1.01\n",
      " comments/sec\n",
      "Time left: 35427.14\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-e5d5711345e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0msent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0mlemma_annotate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0misp_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlemmatizationAnnotate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m             \u001b[0mpos_annotate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0misp_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mposTaggingAnnotate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlemma_annotate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'annotations'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'lemma'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Maxim\\Anaconda3\\lib\\site-packages\\ispras\\texterra.py\u001b[0m in \u001b[0;36mlemmatizationAnnotate\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    243\u001b[0m     \"\"\"Detects lemma of each word of a given text.\n\u001b[1;32m    244\u001b[0m       Note: this method returns Texterra document\"\"\"\n\u001b[0;32m--> 245\u001b[0;31m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__presetNLP\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'lemmatization'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mposTaggingAnnotate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Maxim\\Anaconda3\\lib\\site-packages\\ispras\\texterra.py\u001b[0m in \u001b[0;36m__presetNLP\u001b[0;34m(self, methodName, text)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m     \u001b[0mspecs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAPI\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNLPSpecs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmethodName\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPOST\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspecs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'path'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspecs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'params'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'json'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    512\u001b[0m     \u001b[0mannotations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'annotations'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m     \u001b[0mit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mannotations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m3\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mannotations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miteritems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Maxim\\Anaconda3\\lib\\site-packages\\ispras\\ispras.py\u001b[0m in \u001b[0;36mPOST\u001b[0;34m(self, path, request_params, form_params, format)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0murl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murl\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapikey\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mrequest_params\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'apikey'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapikey\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mpage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__headers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mform_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m60\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mpage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__parse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Maxim\\Anaconda3\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    110\u001b[0m     \"\"\"\n\u001b[1;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m     \u001b[1;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'post'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Maxim\\Anaconda3\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[1;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[1;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Maxim\\Anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    500\u001b[0m         }\n\u001b[1;32m    501\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 502\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    503\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Maxim\\Anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m         \u001b[1;31m# Send the request\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 612\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    613\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m         \u001b[1;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Maxim\\Anaconda3\\lib\\site-packages\\requests\\adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    438\u001b[0m                     \u001b[0mdecode_content\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m                     \u001b[0mretries\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m                     \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m                 )\n\u001b[1;32m    442\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Maxim\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    598\u001b[0m                                                   \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout_obj\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m                                                   \u001b[0mbody\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m                                                   chunked=chunked)\n\u001b[0m\u001b[1;32m    601\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m             \u001b[1;31m# If we're going to release the connection in ``finally:``, then\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Maxim\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    354\u001b[0m             \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest_chunked\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mhttplib_request_kw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m             \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mhttplib_request_kw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[1;31m# Reset the timeout for the recv() on the socket\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Maxim\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, body, headers)\u001b[0m\n\u001b[1;32m   1104\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1105\u001b[0m         \u001b[1;34m\"\"\"Send a complete request to the server.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1106\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_send_request\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_set_content_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Maxim\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_send_request\u001b[0;34m(self, method, url, body, headers)\u001b[0m\n\u001b[1;32m   1149\u001b[0m             \u001b[1;31m# default charset of iso-8859-1.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m             \u001b[0mbody\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_encode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'body'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1151\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendheaders\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1152\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mgetresponse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Maxim\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mendheaders\u001b[0;34m(self, message_body)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1101\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mCannotSendHeader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1102\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_send_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage_body\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Maxim\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_send_output\u001b[0;34m(self, message_body)\u001b[0m\n\u001b[1;32m    932\u001b[0m         \u001b[1;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 934\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    935\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmessage_body\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    936\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage_body\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Maxim\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    875\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msock\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_open\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 877\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    878\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mNotConnected\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Maxim\\Anaconda3\\lib\\site-packages\\urllib3\\connection.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mconn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_new_conn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_prepare_conn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Maxim\\Anaconda3\\lib\\site-packages\\urllib3\\connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m             conn = connection.create_connection(\n\u001b[0;32m--> 141\u001b[0;31m                 (self.host, self.port), self.timeout, **extra_kw)\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mSocketTimeout\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Maxim\\Anaconda3\\lib\\site-packages\\urllib3\\util\\connection.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0msource_address\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m                 \u001b[0msock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource_address\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m             \u001b[0msock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msa\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msock\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lemmatized_txt = open('lemmatized.txt', 'w', encoding='utf8')\n",
    "\n",
    "with open('learn_scrapy\\pda.jl', 'rb') as f:\n",
    "    \n",
    "    vocab = FullVocab()\n",
    "    num_comments = 30242\n",
    "    counter = 0\n",
    "    prev_counter = 0\n",
    "    checkpoint = tm.clock()\n",
    "    dt = 100\n",
    "    \n",
    "    for item in jl.reader(f):\n",
    "        sents = nltk.sent_tokenize(item['comment'])\n",
    "        for sent in sents:\n",
    "            toks = tokenizer.tokenize(sent)\n",
    "            toks = [t.lower() for t in toks if t.isalnum()]\n",
    "            sent = ' '.join(toks)\n",
    "            \n",
    "            lemma_annotate = isp_api.lemmatizationAnnotate(text = sent)\n",
    "            pos_annotate = isp_api.posTaggingAnnotate(text = sent)\n",
    "            if lemma_annotate.get('annotations').get('lemma') is not None:\n",
    "                lemmatized = []\n",
    "                for i in range(len(lemma_annotate.get('annotations').get('lemma'))):\n",
    "                    l = lemma_annotate.get('annotations').get('lemma')[i].get('value')\n",
    "                    p = pos_annotate['annotations']['pos-token'][i]['value'].get('tag')\n",
    "                    word_id = vocab.add_item(toks[i], l, p)\n",
    "                    lemmatized.append(str(word_id))\n",
    "                \n",
    "                lemmatized = ' '.join(lemmatized)\n",
    "                lemmatized_txt.write(lemmatized + '\\n')\n",
    "        \n",
    "        counter = counter + 1\n",
    "        if(tm.clock() - checkpoint > dt):\n",
    "            print(\"Progress: \", end = '')\n",
    "            print(counter / num_comments * 100, end = '')\n",
    "            print(\"%\")\n",
    "            print(\"Comments processed by turn: \", end= '')\n",
    "            print(counter - prev_counter)\n",
    "            print(\"Comments processed total: \", end= '')\n",
    "            print(counter)\n",
    "            print(\"Speed: \", end = '')\n",
    "            print(speed, end = ' ')\n",
    "            print(\"comments/sec\")\n",
    "            print(\"Time left: \", end = '')\n",
    "            speed = (counter - prev_counter) / dt\n",
    "            print(speed * (num_comments - counter))\n",
    "            print(\"\\n\")\n",
    "            prev_counter = counter\n",
    "            tm.sleep(20)\n",
    "            checkpoint = tm.clock()\n",
    "            \n",
    "    vocab.buid_xml()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "comments = ''\n",
    "with open('learn_scrapy\\pda.jl', 'rb') as f:\n",
    "    for item in jl.reader(f):\n",
    "        comments = comments + item['comment']       #создаём один string на все комменты\n",
    "                                                    #скорее всего, будет логичнее для N-граммов каждое предложение\n",
    "                                                    #запоминать в один string и хранить как list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kek1\n",
      "Kek2\n",
      "Kek3\n",
      "Kek4\n"
     ]
    }
   ],
   "source": [
    "#тут токенизируем комментарии\n",
    "#есть подозрение, что я создаю слишком много сущностей \n",
    "\n",
    "tknzr = nltk.TweetTokenizer()\n",
    "print('Kek1')\n",
    "tokens = tknzr.tokenize(comments)\n",
    "print('Kek2')\n",
    "text = nltk.Text(tokens)\n",
    "print('Kek3')\n",
    "sents = nltk.sent_tokenize(comments)                         #строить n-граммную модель, возможно, буду \n",
    "print('Kek4')                                                             #анализируя предложения по-отдельности\n",
    "words = sorted([w.lower() for w in tokens if w.isalpha()])   #впоследствии буду отсекать наименее наиболее частотные слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10600020178435018\n"
     ]
    }
   ],
   "source": [
    "def lexical_diversity(text):\n",
    "    return len(set(text)) / len(text)\n",
    "\n",
    "print(lexical_diversity(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sent_tokens = list()                            #здесь храним предложения\n",
    "for i in range(len(sents)):\n",
    "    sent_tokens.append(tknzr.tokenize(sents[i]))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#сох нормальные формы слов\n",
    "morph = pm2.MorphAnalyzer()     \n",
    "vocab_normalized = sorted(set([morph.parse(w)[0].normalized for w in words]), key = lambda w: w.normal_form)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def progress(full, curr, checkpoint):\n",
    "    if curr / full * 100 > checkpoint:\n",
    "        print(curr / full * 100, end = '')\n",
    "        print('% complited')\n",
    "        return True\n",
    "    \n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "i = 0\n",
    "h = 0.1\n",
    "checkpoint = h\n",
    "time = tm.clock()\n",
    "for sent in sents:\n",
    "    if progress(len(sents), i, checkpoint):\n",
    "        time_per_checkpoint = tm.clock() - time\n",
    "        print('Time per checkpoint:', end = ' ')\n",
    "        print(time_per_checkpoint)\n",
    "        print('Time per sentence:', end = ' ')\n",
    "        time_per_sent = time_per_checkpoint / (len(sents) * h)\n",
    "        print(time_per_sent)\n",
    "        print('Estimated time:', end = ' ')\n",
    "        print(time_per_checkpoint * (100 - checkpoint))\n",
    "        time = tm.clock()\n",
    "        checkpoint = checkpoint + h\n",
    "        \n",
    "    toks = tknzr.tokenize(sent)\n",
    "    toks = [t.lower() for t in toks if t.isalnum()]\n",
    "    sentence = ''\n",
    "    for t in toks:\n",
    "        sentence = sentence + t + ' '\n",
    "    \n",
    "    lemma_raw = isp_api.lemmatizationAnnotate(text = sentence)\n",
    "    lemmatized = ''\n",
    "    if lemma_raw.get('annotations').get('lemma') is not None:\n",
    "        for l in lemma_raw.get('annotations').get('lemma'):\n",
    "            lemmatized = lemmatized + l.get('value') + ' '\n",
    "        \n",
    "        sents[i] = lemmatized\n",
    "    \n",
    "    else:\n",
    "        sents[i] = ''\n",
    "        \n",
    "    i = i + 1\n",
    "        \n",
    "    \n",
    "sents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sents' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-e8335ea762b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'lemmatized.txt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf8'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msents\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m5000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sents' is not defined"
     ]
    }
   ],
   "source": [
    "with open('lemmatized.txt', 'w', encoding='utf8') as f:\n",
    "    for s in sents[0:5000]:\n",
    "        f.write(s + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec(gensim.models.word2vec.LineSentence('lemmatized.txt'), min_count=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('а', 0.9998720288276672),\n",
       " ('они', 0.9998717904090881),\n",
       " ('где', 0.9998696446418762),\n",
       " ('этот', 0.9998689293861389),\n",
       " ('мочь', 0.9998672604560852),\n",
       " ('когда', 0.999864935874939),\n",
       " ('это', 0.9998648166656494),\n",
       " ('там', 0.9998632669448853),\n",
       " ('с', 0.9998587965965271),\n",
       " ('ничто', 0.9998581409454346)]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive = ['айфон'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "сейчас\n",
      "{'value': {'characters': [], 'tag': 'ADV', 'type': 'syn-tag-rus'}, 'start': 0, 'text': 'сейчас', 'end': 6, 'annotated-text': 'сейчас я говорю о том человеке который спит отдыхая на работе'}\n",
      "[Parse(word='сейчас', tag=OpencorporaTag('ADVB'), normal_form='сейчас', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'сейчас', 3, 0),))]\n",
      "\n",
      "\n",
      "я\n",
      "{'value': {'characters': [{'tag': 'Masculine', 'type': 'gender'}, {'tag': 'DEICTIC', 'type': 'pronoun'}, {'tag': 'Singular', 'type': 'number'}, {'tag': 'Nominative', 'type': 'case'}, {'tag': 'Animated', 'type': 'animacy'}], 'tag': 'S', 'type': 'syn-tag-rus'}, 'start': 7, 'text': 'я', 'end': 8, 'annotated-text': 'сейчас я говорю о том человеке который спит отдыхая на работе'}\n",
      "[Parse(word='я', tag=OpencorporaTag('NPRO,1per sing,nomn'), normal_form='я', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'я', 3100, 0),))]\n",
      "\n",
      "\n",
      "говорить\n",
      "{'value': {'characters': [{'tag': 'First', 'type': 'person'}, {'tag': 'Singular', 'type': 'number'}, {'tag': 'NotPast', 'type': 'tense'}, {'tag': 'Indicative', 'type': 'mode'}], 'tag': 'V', 'type': 'syn-tag-rus'}, 'start': 9, 'text': 'говорю', 'end': 15, 'annotated-text': 'сейчас я говорю о том человеке который спит отдыхая на работе'}\n",
      "[Parse(word='говорить', tag=OpencorporaTag('INFN,impf,tran'), normal_form='говорить', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'говорить', 395, 0),))]\n",
      "\n",
      "\n",
      "о\n",
      "{'value': {'characters': [], 'tag': 'PR', 'type': 'syn-tag-rus'}, 'start': 16, 'text': 'о', 'end': 17, 'annotated-text': 'сейчас я говорю о том человеке который спит отдыхая на работе'}\n",
      "[Parse(word='о', tag=OpencorporaTag('PREP'), normal_form='о', score=0.990985, methods_stack=((<DictionaryAnalyzer>, 'о', 2133, 0),)), Parse(word='о', tag=OpencorporaTag('INTJ'), normal_form='о', score=0.009014, methods_stack=((<DictionaryAnalyzer>, 'о', 21, 0),))]\n",
      "\n",
      "\n",
      "то\n",
      "{'value': {'characters': [{'tag': 'Neuter', 'type': 'gender'}, {'tag': 'Prepositional', 'type': 'case'}, {'tag': 'Singular', 'type': 'number'}, {'tag': 'Inanimated', 'type': 'animacy'}], 'tag': 'S', 'type': 'syn-tag-rus'}, 'start': 18, 'text': 'том', 'end': 21, 'annotated-text': 'сейчас я говорю о том человеке который спит отдыхая на работе'}\n",
      "[Parse(word='то', tag=OpencorporaTag('CONJ'), normal_form='то', score=0.5, methods_stack=((<DictionaryAnalyzer>, 'то', 20, 0),)), Parse(word='то', tag=OpencorporaTag('PRCL'), normal_form='то', score=0.1, methods_stack=((<DictionaryAnalyzer>, 'то', 22, 0),))]\n",
      "\n",
      "\n",
      "человек\n",
      "{'value': {'characters': [{'tag': 'Masculine', 'type': 'gender'}, {'tag': 'Prepositional', 'type': 'case'}, {'tag': 'Singular', 'type': 'number'}, {'tag': 'Animated', 'type': 'animacy'}], 'tag': 'S', 'type': 'syn-tag-rus'}, 'start': 22, 'text': 'человеке', 'end': 30, 'annotated-text': 'сейчас я говорю о том человеке который спит отдыхая на работе'}\n",
      "[Parse(word='человек', tag=OpencorporaTag('NOUN,anim,masc sing,nomn'), normal_form='человек', score=0.549382, methods_stack=((<DictionaryAnalyzer>, 'человек', 488, 0),)), Parse(word='человек', tag=OpencorporaTag('NOUN,anim,masc sing,nomn'), normal_form='человек', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'человек', 488, 0),))]\n",
      "\n",
      "\n",
      "который\n",
      "{'value': {'characters': [{'tag': 'Masculine', 'type': 'gender'}, {'tag': 'Nominative', 'type': 'case'}, {'tag': 'Singular', 'type': 'number'}], 'tag': 'S', 'type': 'syn-tag-rus'}, 'start': 31, 'text': 'который', 'end': 38, 'annotated-text': 'сейчас я говорю о том человеке который спит отдыхая на работе'}\n",
      "[Parse(word='который', tag=OpencorporaTag('ADJF,Apro,Subx,Anph masc,sing,nomn'), normal_form='который', score=0.823529, methods_stack=((<DictionaryAnalyzer>, 'который', 1788, 0),)), Parse(word='который', tag=OpencorporaTag('ADJF,Apro,Subx,Anph masc,sing,nomn'), normal_form='который', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'который', 1788, 0),))]\n",
      "\n",
      "\n",
      "спать\n",
      "{'value': {'characters': [{'tag': 'Third', 'type': 'person'}, {'tag': 'Singular', 'type': 'number'}, {'tag': 'NotPast', 'type': 'tense'}, {'tag': 'Indicative', 'type': 'mode'}], 'tag': 'V', 'type': 'syn-tag-rus'}, 'start': 39, 'text': 'спит', 'end': 43, 'annotated-text': 'сейчас я говорю о том человеке который спит отдыхая на работе'}\n",
      "[Parse(word='спать', tag=OpencorporaTag('INFN,impf,intr'), normal_form='спать', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'спать', 2808, 0),))]\n",
      "\n",
      "\n",
      "отдыхать\n",
      "{'value': {'characters': [{'tag': 'Gerund', 'type': 'representation'}, {'tag': 'NotPast', 'type': 'tense'}], 'tag': 'V', 'type': 'syn-tag-rus'}, 'start': 44, 'text': 'отдыхая', 'end': 51, 'annotated-text': 'сейчас я говорю о том человеке который спит отдыхая на работе'}\n",
      "[Parse(word='отдыхать', tag=OpencorporaTag('INFN,impf,intr'), normal_form='отдыхать', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'отдыхать', 2247, 0),))]\n",
      "\n",
      "\n",
      "на\n",
      "{'value': {'characters': [], 'tag': 'PR', 'type': 'syn-tag-rus'}, 'start': 52, 'text': 'на', 'end': 54, 'annotated-text': 'сейчас я говорю о том человеке который спит отдыхая на работе'}\n",
      "[Parse(word='на', tag=OpencorporaTag('PREP'), normal_form='на', score=0.99931, methods_stack=((<DictionaryAnalyzer>, 'на', 24, 0),)), Parse(word='на', tag=OpencorporaTag('PRCL'), normal_form='на', score=0.000477, methods_stack=((<DictionaryAnalyzer>, 'на', 22, 0),)), Parse(word='на', tag=OpencorporaTag('INTJ'), normal_form='на', score=0.000212, methods_stack=((<DictionaryAnalyzer>, 'на', 21, 0),))]\n",
      "\n",
      "\n",
      "работа\n",
      "{'value': {'characters': [{'tag': 'Feminine', 'type': 'gender'}, {'tag': 'Prepositional', 'type': 'case'}, {'tag': 'Singular', 'type': 'number'}, {'tag': 'Inanimated', 'type': 'animacy'}], 'tag': 'S', 'type': 'syn-tag-rus'}, 'start': 55, 'text': 'работе', 'end': 61, 'annotated-text': 'сейчас я говорю о том человеке который спит отдыхая на работе'}\n",
      "[Parse(word='работа', tag=OpencorporaTag('NOUN,inan,femn sing,nomn'), normal_form='работа', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'работа', 55, 0),))]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i = 5\n",
    "\n",
    "message = 'сейчас я говорю о том человеке который спит отдыхая на работе'\n",
    "message_tokens = tokenizer.tokenize(message)\n",
    "parse_tokens = [morph.parse(t) for t in message_tokens]\n",
    "#print(parse_tokens[i])\n",
    "isp_POS = isp_api.posTaggingAnnotate(text = message)\n",
    "isp_lemma = isp_api.lemmatizationAnnotate(text = message)\n",
    "\n",
    "#print(isp_POS['annotations']['pos-token'][i])\n",
    "\n",
    "#print(isp_lemma.get('annotations').get('lemma')[i].get('value'))\n",
    "\n",
    "#print(' '.join([l.get('value') for l in isp_lemma.get('annotations').get('lemma')]))\n",
    "\n",
    "for j in range(len(message_tokens)):\n",
    "    isp_lemma_token = isp_lemma.get('annotations').get('lemma')[j].get('value')\n",
    "    morph_parse_token = [w.normalized for w in morph.parse(isp_lemma_token) if w.normal_form == w.word]\n",
    "    print(isp_lemma_token)\n",
    "    print(isp_POS['annotations']['pos-token'][j])\n",
    "    print(morph_parse_token)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "встать\n",
      "{'text': 'встать', 'annotations': {'lemma': [{'text': 'встать', 'value': 'вставать', 'annotated-text': 'встать', 'start': 0, 'end': 6}]}}\n",
      "[Parse(word='встать', tag=OpencorporaTag('INFN,perf,intr'), normal_form='встать', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'встать', 904, 0),))]\n",
      "\n",
      "\n",
      "вставать\n",
      "{'text': 'вставать', 'annotations': {'lemma': [{'text': 'вставать', 'value': 'вставать', 'annotated-text': 'вставать', 'start': 0, 'end': 8}]}}\n",
      "[Parse(word='вставать', tag=OpencorporaTag('INFN,impf,intr'), normal_form='вставать', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'вставать', 903, 0),))]\n",
      "\n",
      "\n",
      "встали\n",
      "{'text': 'встали', 'annotations': {'lemma': [{'text': 'встали', 'value': 'вставать', 'annotated-text': 'встали', 'start': 0, 'end': 6}]}}\n",
      "[Parse(word='встали', tag=OpencorporaTag('VERB,perf,intr plur,past,indc'), normal_form='встать', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'встали', 904, 4),))]\n",
      "\n",
      "\n",
      "вставали\n",
      "{'text': 'вставали', 'annotations': {'lemma': [{'text': 'вставали', 'value': 'вставать', 'annotated-text': 'вставали', 'start': 0, 'end': 8}]}}\n",
      "[Parse(word='вставали', tag=OpencorporaTag('VERB,impf,intr plur,past,indc'), normal_form='вставать', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'вставали', 903, 10),))]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ws = ['встать', 'вставать', 'встали', 'вставали']\n",
    "for w in ws:\n",
    "    print(w)\n",
    "    ns = morph.parse(w)\n",
    "    ts = isp_api.lemmatizationAnnotate(text = w)\n",
    "    print(ts)\n",
    "    print(ns)\n",
    "        \n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
